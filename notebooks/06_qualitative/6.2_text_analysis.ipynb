{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {
    "tags": []
   },
   "source": [
    "# üìù 6.2 Text Analysis for Qualitative Research\n",
    "\n",
    "We‚Äôll **prepare** text for human-led coding (cleaning, tokenisation, light structure) and add **small helper summaries** (frequencies, n-grams) that support‚Äînot replace‚Äîinterpretation.\n",
    "\n",
    "This notebook keeps the qualitative lens front-and-centre while giving you just enough NLP to work efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objectives",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üéØ Objectives\n",
    "- Clean and tokenise open-ended responses with NLTK.\n",
    "- Lemmatise, remove stopwords/punctuation, handle case.\n",
    "- Build **n-grams** (bigrams, trigrams) to surface phrases.\n",
    "- Optional: POS tags and (careful) sentiment as exploratory aides.\n",
    "- Export a tidy table ready for **manual coding** or 6.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd563cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "MODULE = '06_qualitative'\n",
    "DATASET = 'food_preferences.txt'\n",
    "BASE_PATH = '/content/data-analysis-projects'\n",
    "MODULE_PATH = os.path.join(BASE_PATH, 'notebooks', MODULE)\n",
    "DATASET_PATH = os.path.join(MODULE_PATH, 'data', DATASET)\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(BASE_PATH):\n",
    "        print('Cloning repository...')\n",
    "        !git clone https://github.com/ggkuhnle/data-analysis-projects.git\n",
    "    os.chdir(MODULE_PATH)\n",
    "    if not os.path.exists(DATASET_PATH):\n",
    "        raise FileNotFoundError('Dataset missing after clone.')\n",
    "    print('Dataset ready ‚úÖ')\n",
    "except Exception as e:\n",
    "    print('Setup fallback: upload file...')\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    uploaded = files.upload()\n",
    "    if DATASET in uploaded:\n",
    "        with open(os.path.join('data', DATASET), 'wb') as f:\n",
    "            f.write(uploaded[DATASET])\n",
    "        print('Uploaded dataset ‚úÖ')\n",
    "    else:\n",
    "        raise FileNotFoundError('Upload food_preferences.txt to continue.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -q pandas nltk matplotlib seaborn scikit-learn wordcloud\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "# Handle tokenizers/taggers across NLTK versions\n",
    "for pkg in [\n",
    "    \"punkt\", \"punkt_tab\",\n",
    "    \"stopwords\", \"wordnet\",\n",
    "    \"averaged_perceptron_tagger_eng\", \"averaged_perceptron_tagger\",\n",
    "    \"vader_lexicon\"\n",
    "]:\n",
    "    try:\n",
    "        nltk.download(pkg, quiet=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter \n",
    "from wordcloud import WordCloud \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd6fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "txt = Path('data')/'food_preferences.txt'\n",
    "responses = [r.strip() for r in txt.read_text(encoding='utf-8').splitlines() if r.strip()]\n",
    "df = pd.DataFrame({'response_id': range(1, len(responses)+1), 'text': responses})\n",
    "print('N responses:', len(df))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0faed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then define stoplist + lemmatiser\n",
    "stop = set(stopwords.words('english')).union({'hippo', 'h1','h2','h3'})\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def clean_tokens(text: str):\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [w for w in words if w.isalpha()]  # drop punctuation/numbers\n",
    "    words = [w for w in words if w not in stop]\n",
    "    words = [lem.lemmatize(w) for w in words]\n",
    "    return words\n",
    "\n",
    "df['tokens'] = df['text'].apply(clean_tokens)\n",
    "df[['response_id','text','tokens']].head(6)\n",
    "\n",
    "print(\"Text/NLP environment ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preproc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üßº Preprocessing pipeline\n",
    "We‚Äôll lowercase, tokenise, remove stopwords/punctuation, and **lemmatise** (carrots‚Üícarrot). This **supports** coding by removing noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english')).union({'hippo', 'h1','h2','h3'})\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def clean_tokens(text: str):\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [w for w in words if w.isalpha()]  # drop punctuation/numbers\n",
    "    words = [w for w in words if w not in stop]\n",
    "    words = [lem.lemmatize(w) for w in words]\n",
    "    return words\n",
    "\n",
    "df['tokens'] = df['text'].apply(clean_tokens)\n",
    "df[['response_id','text','tokens']].head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freq",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üìä Frequencies & word cloud (orientation only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freq_code",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_tokens = [t for row in df['tokens'] for t in row]\n",
    "freq = Counter(all_tokens).most_common(15)\n",
    "pd.DataFrame(freq, columns=['word','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wordcloud",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wc = WordCloud(width=800, height=400, background_color='white').generate(' '.join(all_tokens))\n",
    "plt.figure(figsize=(10,4)); plt.imshow(wc); plt.axis('off'); plt.title('Word Cloud'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ngrams",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üîó N-grams (bigrams & trigrams)\n",
    "Short phrases can reveal food pairings (e.g., *fresh fruit*, *crunchy carrot*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ngram_code",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def ngram_counts(tokens_list, n=2, top=15):\n",
    "    ng = Counter()\n",
    "    for toks in tokens_list:\n",
    "        ng.update(ngrams(toks, n))\n",
    "    return pd.DataFrame(ng.most_common(top), columns=[f'{n}-gram','count'])\n",
    "\n",
    "bigrams = ngram_counts(df['tokens'], n=2, top=15)\n",
    "trigrams = ngram_counts(df['tokens'], n=3, top=10)\n",
    "display(bigrams); display(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pos_sent",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üß™ Optional aides: POS tags & VADER sentiment\n",
    "Use sparingly; these are **exploratory hints**, not findings. Sentiment can be noisy in domain language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pos",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ok = False\n",
    "for res in (\"taggers/averaged_perceptron_tagger_eng\",\n",
    "            \"taggers/averaged_perceptron_tagger\"):\n",
    "    try:\n",
    "        nltk.data.find(res)\n",
    "        ok = True\n",
    "        break\n",
    "    except LookupError:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    from nltk import pos_tag\n",
    "    df['pos'] = df['tokens'].apply(pos_tag)  # expects list[str]\n",
    "    out = df[['response_id','pos']].head(4)\n",
    "    display(out)  # <-- force rendering\n",
    "except Exception as e:\n",
    "    print(\"POS tagging unavailable:\", repr(e), \"| tagger_found:\", ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vader",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    df['sentiment'] = df['text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "    sns.histplot(df['sentiment']); plt.title('Sentiment (VADER) ‚Äî exploratory only'); plt.show()\n",
    "except Exception as e:\n",
    "    print('VADER unavailable:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üì§ Export a coding-ready table\n",
    "We create a simple structure that supports **manual coding** (e.g., in Excel/Sheets or in 6.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_code",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = df[['response_id','text','tokens']].copy()\n",
    "out['initial_code'] = ''  # analyst will fill codes\n",
    "out['notes'] = ''         # memo/comments\n",
    "out_path = 'qual_coding_sheet.csv'\n",
    "out.to_csv(out_path, index=False)\n",
    "print('Wrote:', out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üß© Exercises\n",
    "1) **Stoplist tuning**: add domain-specific stopwords (e.g., *like, really, very*)‚Äîhow do top words change?\n",
    "2) **Phrase mining**: examine bigrams containing *fruit* or *carrot*; collect example quotes.\n",
    "3) **Coding sheet**: add 2‚Äì4 provisional **initial codes** per 10 responses (keep them short & action-oriented)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrap",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ‚úÖ Conclusion\n",
    "You prepared text for analysis and produced a coding-ready table. Next: formal **coding & thematic analysis** with reliability checks (6.3).\n",
    "\n",
    "<details><summary>More</summary>\n",
    "- NLTK docs (tokenisation, POS, stopwords)\n",
    "- Practical theming workflows\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
