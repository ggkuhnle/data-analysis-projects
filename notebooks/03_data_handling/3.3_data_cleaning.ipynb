{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# ðŸ§¹ 3.3 Data Cleaning â€” Principles and Practice\n",
        "\n",
        "Clean data is the foundation of valid inference. In nutrition research (NDNS-style surveys, trials, food logs), data often arrives **messy**: missing values, inconsistent formats, duplicates, and subtle logic errors. This notebook gives you a practical, principled workflow for cleaning data in pandas.\n",
        "\n",
        "---\n",
        "## ðŸŽ¯ Objectives\n",
        "By the end of this notebook you can:\n",
        "- Apply **principles of data cleaning**: make data *correct, consistent, complete, and documented*.\n",
        "- Handle **missing values** sensibly (inspect, decide, impute, or exclude).\n",
        "- Fix **poorly formatted values** (strings instead of numbers, units like `%`, decimal commas, stray symbols).\n",
        "- Detect and manage **duplicates** (exact, key-based, and near-duplicates).\n",
        "- Validate cleaned data against simple **rules**.\n",
        "\n",
        "---\n",
        "## ðŸ§­ Principles of Data Cleaning (Field-tested)\n",
        "1. **Profile first**: *Look* before you leap. Inspect shapes, column types, unique values, and obvious anomalies.\n",
        "2. **Make minimal, reversible changes**: Keep raw data intact; work on a copy. Store transformations in code, not by hand.\n",
        "3. **Be explicit about rules**: Write *why* you drop/impute/convert. Use comments and validation checks.\n",
        "4. **Prefer tidy structure**: One variable per column, one observation per row, one unit per table.\n",
        "5. **Treat missingness as information**: The pattern of missing values is a signal. Donâ€™t blindly fill everything.\n",
        "6. **Standardise representations**: Dates, decimals, units, categories (e.g., sex codes) should be uniform.\n",
        "7. **Idempotence**: Running the cleaning script twice should not change results further.\n",
        "8. **Document decisions**: Note assumptions (e.g., iron in mg/day), thresholds (plausibility bounds), and imputations used.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70fde1d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup for Google Colab: Fetch datasets automatically or manually\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "MODULE = '03_data_handling'\n",
        "DATASET = 'hippo_nutrients.csv'\n",
        "BASE_PATH = '/content/data-analysis-projects'\n",
        "MODULE_PATH = os.path.join(BASE_PATH, 'notebooks', MODULE)\n",
        "DATASET_PATH = os.path.join('data', DATASET)\n",
        "\n",
        "try:\n",
        "    if not os.path.exists(BASE_PATH):\n",
        "        !git clone https://github.com/ggkuhnle/data-analysis-projects.git\n",
        "    os.chdir(MODULE_PATH)\n",
        "    assert os.path.exists(DATASET_PATH)\n",
        "    print(f'Dataset found: {DATASET_PATH} âœ…')\n",
        "except Exception as e:\n",
        "    print(f'Automatic clone failed: {e}')\n",
        "    os.makedirs('data', exist_ok=True)\n",
        "    uploaded = files.upload()\n",
        "    if DATASET in uploaded:\n",
        "        with open(DATASET_PATH, 'wb') as f:\n",
        "            f.write(uploaded[DATASET])\n",
        "        print(f'Successfully uploaded {DATASET} âœ…')\n",
        "    else:\n",
        "        raise FileNotFoundError(f'Upload failed. Please ensure you uploaded {DATASET}.')\n",
        "\n",
        "%pip install pandas openpyxl sqlalchemy -q\n",
        "import pandas as pd, numpy as np\n",
        "print('Environment ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load_data",
      "metadata": {},
      "source": [
        "## 1) Load and Profile\n",
        "We start by loading the data and building a quick profile: shapes, dtypes, head/tail, and simple summaries. Profiling guides all subsequent cleaning steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "profile",
      "metadata": {},
      "outputs": [],
      "source": [
        "raw = pd.read_csv('data/hippo_nutrients.csv')\n",
        "df = raw.copy()  # work on a copy; keep raw intact\n",
        "\n",
        "print('Shape:', df.shape)\n",
        "print('\\nDtypes:')\n",
        "print(df.dtypes)\n",
        "\n",
        "print('\\nHead:')\n",
        "display(df.head(5))\n",
        "\n",
        "print('\\nBasic summary:')\n",
        "display(df.describe(include='all'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "missing_overview",
      "metadata": {},
      "source": [
        "## 2) Missing Values â€” Inspect before you impute\n",
        "Missing data can be **MCAR** (completely at random), **MAR** (at random given observed variables), or **MNAR** (not at random). Your strategy depends on the context.\n",
        "\n",
        "Typical steps:\n",
        "- Identify missingness overall and by variable.\n",
        "- Check missingness patterns by groups (e.g., by Nutrient, Year, Sex).\n",
        "- Decide: drop, impute (mean/median/group-wise), or model-based methods (later modules).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "missing_counts",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Missing counts per column:')\n",
        "print(df.isna().sum().sort_values(ascending=False))\n",
        "\n",
        "# Missingness by nutrient (for Value only)\n",
        "if 'Value' in df.columns and 'Nutrient' in df.columns:\n",
        "    miss_by_nutrient = (\n",
        "        df.groupby('Nutrient')['Value']\n",
        "          .apply(lambda s: s.isna().mean())\n",
        "          .sort_values(ascending=False)\n",
        "    )\n",
        "    print('\\nProportion missing in Value by Nutrient:')\n",
        "    display(miss_by_nutrient.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "missing_strategies",
      "metadata": {},
      "source": [
        "### Sensible Imputation Patterns (for teaching datasets)\n",
        "- **Numeric measurements (e.g., `Value`)**: often impute by **group** (by `Nutrient`, sometimes by `Nutrient`Ã—`Sex`), using **median** to reduce outlier influence.\n",
        "- **Categoricals (e.g., `Sex`)**: leave missing or impute with a new category like `'Unknown'` (document this!).\n",
        "- **IDs/keys**: missing IDs are usually fatal â€” investigate or drop those rows with a note.\n",
        "\n",
        "Below we demonstrate **group-wise median imputation** for `Value` and keep everything else untouched (for transparency)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "impute_value",
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'Value' in df.columns and 'Nutrient' in df.columns:\n",
        "    df['Value_imputed'] = df['Value']  # keep original and add an imputed version\n",
        "    df['Value_imputed_flag'] = df['Value'].isna()  # track which were imputed\n",
        "    df['Value_imputed'] = (\n",
        "        df.groupby('Nutrient')['Value_imputed']\n",
        "          .transform(lambda s: s.fillna(s.median()))\n",
        "    )\n",
        "\n",
        "    print('Imputed Value (group-median by Nutrient). Rows imputed:', int(df['Value_imputed_flag'].sum()))\n",
        "    display(df.loc[df['Value_imputed_flag']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "poor_formats_intro",
      "metadata": {},
      "source": [
        "## 3) Poorly Formatted Values â€” Standardise representations\n",
        "Common issues in nutrition datasets:\n",
        "- **Numbers stored as strings**: e.g., `'8.5'`, `' 8.5 '`, `'8,5'` (decimal comma), `'8.5 mg'`.\n",
        "- **Percentages**: e.g., `'12%'` instead of `0.12` or `12`.\n",
        "- **Thousands separators**: `'1,234.5'` or in some locales `'1.234,5'`.\n",
        "- **Category typos/casing**: `'female'`, `'FEMALE'`, `'F'` representing the same concept.\n",
        "\n",
        "Goal: convert to **consistent numeric types**, **consistent units**, and **canonical categories**. Always document unit assumptions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "decimal_commas",
      "metadata": {},
      "source": [
        "### Example: Decimal commas (German-style) â†’ decimal points\n",
        "If values arrive as strings with commas (e.g., `'8,5'`), convert them safely using `str.replace` + `pd.to_numeric`.\n",
        "\n",
        "**Note**: Do not blanket-replace commas if the column also uses commas as thousands separators. Decide the rule from the data context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decimal_commas_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo column with mixed formats (create if not present)\n",
        "if 'Value_raw' not in df.columns:\n",
        "    df['Value_raw'] = df['Value'].astype(str)\n",
        "    # inject a few messy examples for teaching (won't affect original Value)\n",
        "    ex_idx = df.sample(min(3, len(df)), random_state=1).index\n",
        "    df.loc[ex_idx, 'Value_raw'] = ['8,5', ' 12,0 ', '7,9 mg'][:len(ex_idx)]\n",
        "\n",
        "def to_numeric_decomma(s: pd.Series) -> pd.Series:\n",
        "    s = s.astype(str).str.strip()\n",
        "    # remove unit strings like 'mg' or 'kcal' (simple demo)\n",
        "    s = s.str.replace(r'[^0-9,.-]', '', regex=True)\n",
        "    # convert decimal comma to point (assumes comma == decimal separator here)\n",
        "    s = s.str.replace(',', '.', regex=False)\n",
        "    return pd.to_numeric(s, errors='coerce')\n",
        "\n",
        "df['Value_clean_decimal'] = to_numeric_decomma(df['Value_raw'])\n",
        "display(df[['Value_raw','Value_clean_decimal']].head(6))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "percentages",
      "metadata": {},
      "source": [
        "### Example: Percent strings â†’ numeric fractions or percents\n",
        "Decide on a convention:\n",
        "- Store **fractions** (0â€“1), or\n",
        "- Store **percentage points** (0â€“100).\n",
        "\n",
        "Below, we convert `'12%'` â†’ `12.0` (percentage points). Adjust as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "percentages_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "demo = pd.Series(['12%', ' 7.5 %', '0.5%', None, 'n/a'])\n",
        "pct = (demo.astype(str).str.strip()\n",
        "              .str.replace('%','', regex=False)\n",
        "              .str.replace(',','.', regex=False))\n",
        "pct_num = pd.to_numeric(pct, errors='coerce')  # percentage points\n",
        "display(pd.DataFrame({'raw': demo, 'pct_points': pct_num}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "thousands",
      "metadata": {},
      "source": [
        "### Example: Thousands separators\n",
        "Remove separators explicitly. Beware of locale mixes.\n",
        "\n",
        "- UK/US style: `1,234.5` â†’ remove commas â†’ `1234.5`.\n",
        "- DE style: `1.234,5` â†’ remove dots, replace comma with dot â†’ `1234.5`.\n",
        "\n",
        "Whenever possible, confirm the **intended locale** and **units** from metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thousands_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "us = pd.Series(['1,234.5', '2,000', '10,050.75'])\n",
        "us_clean = pd.to_numeric(us.str.replace(',', '', regex=False), errors='coerce')\n",
        "\n",
        "de = pd.Series(['1.234,5', '2.000', '10.050,75'])\n",
        "de_clean = pd.to_numeric(de.str.replace('.', '', regex=False).str.replace(',', '.', regex=False), errors='coerce')\n",
        "\n",
        "display(pd.DataFrame({'us_raw': us, 'us_clean': us_clean}))\n",
        "display(pd.DataFrame({'de_raw': de, 'de_clean': de_clean}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "categoricals",
      "metadata": {},
      "source": [
        "### Example: Canonical categories (e.g., Sex)\n",
        "Standardise category labels and document the mapping. Keep original in case you need to audit later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "categoricals_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'Sex' in df.columns:\n",
        "    df['Sex_raw'] = df['Sex'].astype(str)\n",
        "    sex_map = {\n",
        "        'f':'F', 'female':'F', 'FEMALE':'F', 'F':'F',\n",
        "        'm':'M', 'male':'M', 'MALE':'M', 'M':'M'\n",
        "    }\n",
        "    df['Sex_std'] = df['Sex_raw'].str.strip().str.lower().map(sex_map).fillna('Unknown')\n",
        "    display(df[['Sex_raw','Sex_std']].drop_duplicates().head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "duplicates_intro",
      "metadata": {},
      "source": [
        "## 4) Duplicates â€” Exact, key-based, and near-duplicates\n",
        "Duplicates can arise from repeated data entry, merges, or concatenations.\n",
        "\n",
        "### Types\n",
        "- **Exact duplicates**: Entire rows identical.\n",
        "- **Key-based duplicates**: Same (`ID`,`Nutrient`,`Year`) but different values.\n",
        "- **Near-duplicates**: Slightly different strings or values (e.g., whitespace, casing, rounding). Often require domain judgment.\n",
        "\n",
        "Strategy:\n",
        "1) Count exact duplicates.\n",
        "2) Define a **key** and check for multiple rows per key.\n",
        "3) Decide how to resolve (e.g., prefer non-missing, average within key, keep latest record, or flag for manual review)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duplicates_exact",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.1 Exact duplicates (full-row)\n",
        "exact_dupes = df.duplicated(keep=False)\n",
        "print('Exact duplicate rows (including originals):', int(exact_dupes.sum()))\n",
        "display(df[exact_dupes].head(6))\n",
        "\n",
        "# If you decide to drop exact duplicates:\n",
        "df_no_exact_dupes = df.drop_duplicates().copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duplicates_key",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.2 Key-based duplicates\n",
        "key = ['ID','Nutrient','Year']\n",
        "if all(k in df.columns for k in key):\n",
        "    counts = df.groupby(key).size().reset_index(name='n')\n",
        "    multi = counts[counts['n'] > 1]\n",
        "    print('Keys with >1 row:', len(multi))\n",
        "    display(multi.head(10))\n",
        "\n",
        "    # Inspect conflicts for a sample key (if present)\n",
        "    if not multi.empty:\n",
        "        sample_key = multi.iloc[0][key].to_dict()\n",
        "        print('Example conflicting key:', sample_key)\n",
        "        q = (df['ID'].eq(sample_key['ID']) &\n",
        "             df['Nutrient'].eq(sample_key['Nutrient']) &\n",
        "             df['Year'].eq(sample_key['Year']))\n",
        "        display(df.loc[q])\n",
        "\n",
        "    # Example resolution: within each key, keep the row with the least missingness in Value/Value_imputed\n",
        "    # (Alternatively: take mean, prefer non-missing, or most recent record if you have timestamps.)\n",
        "    def resolve_group(g):\n",
        "        # prefer non-missing Value; fall back to Value_imputed\n",
        "        g = g.copy()\n",
        "        score = g['Value'].notna().astype(int) + g.get('Value_imputed_flag', False).map({True:0, False:0})\n",
        "        # keep the first row with highest score\n",
        "        return g.loc[score.sort_values(ascending=False).index[0:1]]\n",
        "\n",
        "    df_resolved = (\n",
        "        df.groupby(key, as_index=False, group_keys=False)\n",
        "          .apply(resolve_group)\n",
        "    )\n",
        "    print('After key-based resolution, shape:', df_resolved.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "validation_rules",
      "metadata": {},
      "source": [
        "## 5) Validate with Simple Rules\n",
        "Codify assumptions as checks. Fail **loudly** when rules are broken:\n",
        "- **Type checks**: numeric columns should be numeric.\n",
        "- **Range checks**: plausible ranges (e.g., iron intake in mg/day is rarely negative or 10,000).\n",
        "- **Membership checks**: categories in known sets.\n",
        "\n",
        "These checks turn silent data issues into visible test failures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "run_checks",
      "metadata": {},
      "outputs": [],
      "source": [
        "problems = []\n",
        "\n",
        "# Type checks\n",
        "if 'Value_imputed' in df.columns:\n",
        "    if not np.issubdtype(df['Value_imputed'].dtype, np.number):\n",
        "        problems.append('Value_imputed is not numeric.')\n",
        "\n",
        "# Range checks (example plausible bounds, adjust to your domain)\n",
        "if 'Value_imputed' in df.columns:\n",
        "    out_of_range = ~df['Value_imputed'].between(0, 10000)  # generous upper bound\n",
        "    if out_of_range.any():\n",
        "        problems.append(f\"{int(out_of_range.sum())} rows have Value_imputed outside [0, 10000].\")\n",
        "\n",
        "# Membership checks\n",
        "if 'Sex_std' in df.columns:\n",
        "    allowed = {'M','F','Unknown'}\n",
        "    bad = ~df['Sex_std'].isin(allowed)\n",
        "    if bad.any():\n",
        "        problems.append(f\"{int(bad.sum())} rows have unexpected Sex_std labels.\")\n",
        "\n",
        "print('Validation summary:')\n",
        "if problems:\n",
        "    for p in problems:\n",
        "        print(' -', p)\n",
        "else:\n",
        "    print('All checks passed âœ…')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pipeline_export",
      "metadata": {},
      "source": [
        "## 6) Save the Cleaned Dataset (Optional)\n",
        "Export a clean, analysis-ready table. Keep both **raw** and **clean** versions under version control, and record cleaning decisions in `README` or the notebook header."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "save_clean",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose the resolved DataFrame if you performed key-based resolution; otherwise use df\n",
        "clean = df.copy()\n",
        "clean_path = 'data/hippo_nutrients_clean.csv'\n",
        "clean.to_csv(clean_path, index=False)\n",
        "print('Saved:', clean_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exercises",
      "metadata": {},
      "source": [
        "## ðŸ§ª Exercises (Guided)\n",
        "1) **Missingness by subgroup**  \n",
        "   - Compute the proportion of missing `Value` by `(Nutrient, Sex)`.  \n",
        "   - Impute `Value` using **median by NutrientÃ—Sex** and track imputed rows with a boolean flag.\n",
        "\n",
        "2) **Locale & units clean-up**  \n",
        "   - Create a column `Value_text` by copying `Value` as strings and inject a few messy entries (with `'mg'`, spaces, decimal commas).  \n",
        "   - Write a `clean_numeric()` function that removes units, trims, converts decimal commas, and returns numeric.  \n",
        "   - Compare distributions before/after cleaning.\n",
        "\n",
        "3) **Duplicates resolution policy**  \n",
        "   - Identify keys with multiple rows.  \n",
        "   - Implement *two* resolution policies: (a) prefer non-missing `Value`, (b) take the mean within key.  \n",
        "   - Compare results â€” do any policies change downstream summaries?\n",
        "\n",
        "4) **Validation rules**  \n",
        "   - Add a rule: `Year` must be between 1990 and 2035.  \n",
        "   - Add a rule: `Nutrient` must belong to a specific allowed set found in `df['Nutrient'].unique()` (or a curated list).  \n",
        "   - Make the notebook *fail loudly* (raise an Exception) if violations exceed a small tolerance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wrap",
      "metadata": {},
      "source": [
        "## âœ… Conclusion & Next Steps\n",
        "Youâ€™ve implemented a **principled cleaning pipeline**:\n",
        "- Profiled the data.\n",
        "- Handled missingness with transparent, group-wise imputation.\n",
        "- Standardised messy numeric and categorical fields (decimal commas, units, percentages, Sex labels).\n",
        "- Detected exact and key-based duplicates and demonstrated a resolution policy.\n",
        "- Encoded assumptions as validation **rules**.\n",
        "\n",
        "ðŸ‘‰ Next: **3.4 Data Transformation** â€” reshaping, deriving variables, and feature engineering for analysis.\n",
        "\n",
        "---\n",
        "**Resources**  \n",
        "- pandas: Missing data â€” https://pandas.pydata.org/docs/user_guide/missing_data.html  \n",
        "- pandas: Working with text data â€” https://pandas.pydata.org/docs/user_guide/text.html  \n",
        "- pandas: Duplicate labels/rows â€” https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html  \n",
        "- On assumptions & validation in data analysis: keep a crisp `README` alongside your cleaned outputs.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
