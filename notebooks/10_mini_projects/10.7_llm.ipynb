{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0940aadd",
      "metadata": {
        "tags": []
      },
      "source": [
        "# 10.7 â€¢ LLMs in Practice (No-Paid-API Edition)\n",
        "\n",
        "This notebook gives you a **hands-on** way to use LLMs without paying for an API:\n",
        "- **Local model** with `transformers` (e.g. `distilgpt2`) â€” works offline after first download.\n",
        "- **Optional free-tier API** via **Hugging Face Inference** (needs a free token).\n",
        "- A tiny, transparent **RAG demo** (retrieve â†’ augment â†’ generate).\n",
        "- **Schema-constrained outputs** checked with Python validation.\n",
        "\n",
        "Hippo-flavoured prompts included. ðŸ¦›"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0868f3b",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 0) Setup\n",
        "\n",
        "> The first run will download small models (~100â€“200 MB). This is normal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95cebf51",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# If running on Colab, uncomment the next line to install dependencies:\n",
        "# !pip -q install transformers==4.44.2 sentence-transformers==3.0.1 torch --upgrade\n",
        "\n",
        "import os, json, math, numpy as np\n",
        "from typing import List, Dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2695999b",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 1) Local small model with `transformers` (no API key)\n",
        "\n",
        "We'll load a small open model (`distilgpt2`) and generate text with different sampling settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91541a79",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"distilgpt2\"  # small and quick to try\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "def generate_local(prompt: str, max_new_tokens=60, temperature=0.8, top_k=50):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(generate_local(\"Hippo keeper log: Today the hippo felt\", max_new_tokens=40))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1164832d",
      "metadata": {
        "tags": []
      },
      "source": [
        "### Experiment: sampling choices\n",
        "\n",
        "- Lower temperature (e.g., 0.6) â†’ safer, more repetitive.\n",
        "- Higher temperature (e.g., 1.2) â†’ more diverse, riskier.\n",
        "- `top_k` limits to the k most likely tokens at each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e665f710",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "for temp in [0.6, 0.9, 1.2]:\n",
        "    print(f\"\\n--- temperature={temp} ---\")\n",
        "    print(generate_local(\"Hippo diet note: The riverbank forage was\", max_new_tokens=40, temperature=temp, top_k=40))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8db23289",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 2) Optional: Free-tier API via Hugging Face Inference\n",
        "\n",
        "If you create a free account at **huggingface.co**, you can obtain a token and set it as an environment variable `HF_TOKEN`.\n",
        "\n",
        "- Model endpoint examples: `tiiuae/falcon-7b-instruct`, `google/gemma-2b-it`, etc. (availability may vary).\n",
        "- **No token?** This cell will simply skip the API call and explain.\n",
        "\n",
        "> This is useful for showing students a hosted model workflow without paid credit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f9c7e34",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os, requests\n",
        "\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\").strip()\n",
        "HF_MODEL = os.environ.get(\"HF_MODEL\", \"google/gemma-2-2b-it\")  # small instruct model; change if unavailable\n",
        "\n",
        "def call_hf_inference(prompt: str, max_new_tokens=128, temperature=0.8):\n",
        "    if not HF_TOKEN:\n",
        "        return \"HF_TOKEN not set. Create a free token at huggingface.co, then: export HF_TOKEN=hf_xxx\"\n",
        "    # Inference endpoint (text-generation):\n",
        "    url = f\"https://api-inference.huggingface.co/models/{HF_MODEL}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\"max_new_tokens\": max_new_tokens, \"temperature\": temperature, \"return_full_text\": False},\n",
        "    }\n",
        "    r = requests.post(url, headers=headers, json=payload, timeout=60)\n",
        "    if r.status_code != 200:\n",
        "        return f\"HF error {r.status_code}: {r.text[:2000]}\"\n",
        "    try:\n",
        "        out = r.json()\n",
        "        if isinstance(out, list) and len(out)>0 and \"generated_text\" in out[0]:\n",
        "            return out[0][\"generated_text\"]\n",
        "        return json.dumps(out, indent=2)[:2000]\n",
        "    except Exception as e:\n",
        "        return f\"Parse error: {e}\\nRaw: {r.text[:500]}\"\n",
        "\n",
        "demo_prompt = (\n",
        "    \"System: You are a concise nutrition assistant for a zoo hippo team.\\n\"\n",
        "    \"User: Summarise three key checks for a daily hippo health round.\n",
        "\"\n",
        ")\n",
        "\n",
        "print(call_hf_inference(demo_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db2de8a4",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 3) A tiny Retrieval-Augmented Generation (RAG) demo\n",
        "\n",
        "Weâ€™ll create a **mini corpus** of short notes (hippo care + nutrition), embed them with `sentence-transformers`, \n",
        "retrieve the top-3 most similar passages, and then **compose** a grounded prompt for the local model.\n",
        "\n",
        "> This demonstrates the workflow: **retrieve first, then generate**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4544c409",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# If sentence-transformers not installed, see install cell above\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "corpus = [\n",
        "    \"Hippos often graze at night; daytime wallowing helps thermoregulation.\",\n",
        "    \"Dental checks should examine tusk wear, gum inflammation, and food impaction.\",\n",
        "    \"Forage quality affects chewing time and saliva buffering, influencing dental health.\",\n",
        "    \"Sudden changes in water salinity can alter drinking behaviour and stress.\",\n",
        "    \"Keeper logs should include appetite, activity, and social interactions.\",\n",
        "]\n",
        "queries = [\n",
        "    \"What should we check during a hippo dental health round?\",\n",
        "]\n",
        "\n",
        "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "E_corpus = embedder.encode(corpus, normalize_embeddings=True)\n",
        "E_query  = embedder.encode(queries, normalize_embeddings=True)\n",
        "\n",
        "def retrieve(query_vec, k=3):\n",
        "    scores = (E_corpus @ query_vec)  # cosine similarity since normalized\n",
        "    idx = np.argsort(-scores)[:k]\n",
        "    return [(corpus[i], float(scores[i])) for i in idx]\n",
        "\n",
        "top = retrieve(E_query[0], k=3)\n",
        "top"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "925dfa24",
      "metadata": {
        "tags": []
      },
      "source": [
        "### Compose a grounded prompt and generate (local model)\n",
        "\n",
        "We build a prompt that **quotes** the retrieved passages and asks the local model for a **succinct, structured** answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2f5950d",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "context_lines = \"\\n\".join([f\"- {t[0]}\" for t in top])\n",
        "grounded_prompt = f\"\"\"Use ONLY the following notes to answer the question. Quote specific checks.\n",
        "\n",
        "Notes:\n",
        "{context_lines}\n",
        "\n",
        "Question: List 3 essential checks for a hippo dental round in bullet points.\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "print(grounded_prompt)\n",
        "print(\"\\n--- Generated (local model) ---\\n\")\n",
        "print(generate_local(grounded_prompt, max_new_tokens=80, temperature=0.7, top_k=40))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c6abc1f",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 4) Schema-constrained output (JSON) and validation\n",
        "\n",
        "For real systems, ask the model to output **JSON with required fields**, then validate it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6478f503",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import json, re\n",
        "\n",
        "json_prompt = (\n",
        "\n",
        "    \"Produce a JSON object with fields: 'checks' (list of 3 short strings), \"\n",
        "\n",
        "    \"'priority' (one of: 'low','medium','high').\\n\"\n",
        "\n",
        "    \"Use the notes above; if unclear, choose 'medium'.\\n\"\n",
        "\n",
        "    \"JSON only, no extra text.\\n\"\n",
        "\n",
        ")\n",
        "\n",
        "candidate = generate_local(grounded_prompt + \"\\n\" + json_prompt, max_new_tokens=120, temperature=0.7, top_k=50)\n",
        "\n",
        "# Extract a JSON object with a simple heuristic (first {...} block)\n",
        "match = re.search(r'\\{[\\s\\S]*\\}', candidate)\n",
        "parsed = None\n",
        "if match:\n",
        "    try:\n",
        "        parsed = json.loads(match.group(0))\n",
        "    except Exception as e:\n",
        "        parsed = {\"error\": f\"JSON parse failed: {e}\", \"raw\": candidate[:500]}\n",
        "else:\n",
        "    parsed = {\"error\": \"No JSON object found.\", \"raw\": candidate[:500]}\n",
        "\n",
        "# Simple validation\n",
        "def validate(payload: dict) -> Dict[str, str]:\n",
        "    if not isinstance(payload, dict): return {\"ok\": \"false\", \"reason\": \"not a dict\"}\n",
        "    if \"checks\" not in payload or not isinstance(payload[\"checks\"], list) or len(payload[\"checks\"]) != 3:\n",
        "        return {\"ok\":\"false\", \"reason\":\"checks must be a list of length 3\"}\n",
        "    if \"priority\" not in payload or payload[\"priority\"] not in {\"low\",\"medium\",\"high\"}:\n",
        "        return {\"ok\":\"false\", \"reason\":\"priority invalid\"}\n",
        "    return {\"ok\":\"true\"}\n",
        "\n",
        "print(\"RAW CANDIDATE:\\n\", candidate[:500], \"\\n\")\n",
        "print(\"PARSED:\", parsed)\n",
        "print(\"VALIDATION:\", validate(parsed))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40760af6",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 5) Summary\n",
        "\n",
        "- **Local models** (e.g., `distilgpt2`) let you teach LLM mechanics without paid APIs.\n",
        "- **Hugging Face Inference** provides a free-tier API with a token; good for demos.\n",
        "- **RAG** grounds answers in your corpus; show students how retrieval shapes outputs.\n",
        "- **JSON schemas** (even simple checks) harden systems against hallucinations.\n",
        "\n",
        "**Exercises**\n",
        "1. Swap `distilgpt2` for `microsoft/phi-2` or `TinyLlama/TinyLlama-1.1B-Chat-v1.0` (may need more RAM). Compare output quality and speed.\n",
        "2. Change the retrieval corpus to short **nutrition RCT abstracts**. Does the answer become more evidence-based?\n",
        "3. Extend validation to enforce that each `checks` item contains at least one **dental term** (`gum`, `tusk`, `impaction`, `inflammation`). Reject if not met."
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "Prepared for Gunter's ML/AI course"
      }
    ],
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
