{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "# 10.5 • Machine Learning the Safe Way (Hippo Edition)\n",
        "\n",
        "**Why this notebook?**  \n",
        "Many “ML tutorials” skip straight to `.fit()` and `.predict()` and then celebrate a single number.  \n",
        "Here we do it properly: clearly-defined assumptions, leakage-free evaluation, calibrated probabilities, and documented limitations.\n",
        "\n",
        "**Our playful scenario**  \n",
        "You're consulting for *Riverbend Hippo Reserve*. Keepers want to **flag hippos for a proactive dental check** next month.  \n",
        "You’ll build a **binary classifier** that outputs a probability that a hippo will **benefit from a dental check** (1 = yes, 0 = no).  \n",
        "Features are simple husbandry metrics: daily forage types, mud-bathing time, water salinity, social rank, etc.  \n",
        "It’s synthetic—but realistic enough to teach the right habits.\n",
        "\n",
        "> Rule of thumb: *If a model will influence a real decision, you must be able to defend it in an audit.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33c71b15",
      "metadata": {},
      "source": [
        "## What is leakage?\n",
        "\n",
        "In machine learning, “leakage” means that your model has accidentally seen information it shouldn’t have during training.\n",
        "\n",
        "A leakage-free workflow ensures that everything done during training is based only on the training data, not on the validation or test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "### Learning objectives\n",
        "1. Understand a leakage-free workflow (train/validation/test) and why it matters.\n",
        "2. Read learning curves to decide whether to collect more data or simplify the model.\n",
        "3. Do honest model selection with cross-validation inside a `Pipeline`.\n",
        "4. Explain model behaviour with permutation importance and partial dependence.\n",
        "5. Check and fix probability **calibration** (because 0.7 should really mean 70%).\n",
        "6. Stress-test robustness and write a mini **model card** summarising limitations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "### What you’ll do\n",
        "- **Simulate** a labelled dataset with a known ground truth (so we can verify claims).\n",
        "- **Split** into train/test once; then do **all** tuning via cross-validation on the **training data only**.\n",
        "- Compare a **regularised logistic regression** and a **random forest**.\n",
        "- Plot **learning curves** to see bias–variance trade-offs.\n",
        "- Evaluate performance **only once** on the sealed test set.\n",
        "- Interpret the model and **calibrate** probabilities.\n",
        "- Run simple **stress tests**; draft a compact **model card**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 0) Setup\n",
        "> We use only `matplotlib` for plotting (department rule of thumb), and scikit-learn for modelling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, learning_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, RocCurveDisplay, ConfusionMatrixDisplay,\n",
        "    classification_report, brier_score_loss, precision_recall_curve, PrecisionRecallDisplay\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
        "from sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\n",
        "\n",
        "# Reproducibility\n",
        "RNG = np.random.default_rng(11088)\n",
        "np.random.seed(11088)\n",
        "pd.set_option('display.max_columns', 200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 1) Data: simulate a transparent ground truth (hippo husbandry)\n",
        "\n",
        "We generate **n ≈ 1,200** hippo-days with **p = 30** features.  \n",
        "To make things realistic:\n",
        "- Some features are **correlated** (e.g., forage types co-occur).\n",
        "- Only a handful carry **true signal**; others are noise.\n",
        "- Signal includes **non-linear** patterns (thresholds and sines).\n",
        "\n",
        "**Target:** `needs_dental_check` (1/0). Mild class imbalance is typical in screening tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "n, p = 1200, 30\n",
        "X = RNG.normal(size=(n, p))\n",
        "\n",
        "# Create three correlated blocks of features (to mimic co-occurring husbandry variables)\n",
        "for b in range(3):\n",
        "    z = RNG.normal(size=(n, 1))\n",
        "    X[:, 10*b:10*(b+1)] += 0.6 * z\n",
        "\n",
        "cols = [f'feat_{i+1}' for i in range(p)]\n",
        "\n",
        "# Map a few columns to playful meanings (purely for pedagogy)\n",
        "hippo_dict = {\n",
        "    'feat_3': 'mud_minutes',         # time spent mud-bathing\n",
        "    'feat_6': 'forage_quality',      # higher is better\n",
        "    'feat_8': 'dominance_index',     # social rank proxy\n",
        "    'feat_13': 'water_salinity',     # small salinity changes affect behaviour\n",
        "    'feat_18': 'keeper_visits',      # number of keeper interactions\n",
        "    'feat_27': 'playfulness_score'   # sine-shaped relation (too low/high may correlate oddly)\n",
        "}\n",
        "\n",
        "pretty_cols = [hippo_dict.get(c, c) for c in cols]\n",
        "\n",
        "# True signal (non-linear pieces on a subset)\n",
        "sig = (\n",
        "    0.40*X[:,2]              # mud_minutes helpful (less grit accumulation)\n",
        "  - 0.35*X[:,5]              # poor forage quality increases risk\n",
        "  + 0.50*((X[:,7]**2) > 0.5).astype(float)  # dominance threshold effect\n",
        "  + 0.45*X[:,12]             # water_salinity small positive relation\n",
        "  - 0.40*X[:,17]             # more keeper_visits reduce risk (preventative care)\n",
        "  + 0.35*np.sin(X[:,26])     # playfulness has a wavy relation with dental health\n",
        ")\n",
        "\n",
        "logit = -0.2 + sig\n",
        "proba = 1/(1 + np.exp(-logit))\n",
        "y = (RNG.random(n) < 0.08 + 0.84*proba).astype(int)  # mild imbalance\n",
        "\n",
        "df = pd.DataFrame(X, columns=pretty_cols)\n",
        "df['needs_dental_check'] = y\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 2) Split once: **train (75%)** and **test (25%)**\n",
        "\n",
        "- The **test set is a sealed envelope**. Touch it only once at the very end.\n",
        "- All preprocessing (scaling), feature selection, and hyper-parameter tuning happens inside **cross-validation on the training data**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns='needs_dental_check').values\n",
        "y = df['needs_dental_check'].values\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=11088\n",
        ")\n",
        "\n",
        "X_tr.shape, X_te.shape, y.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 3) Leakage-safe pipelines and honest cross-validation\n",
        "\n",
        "Why pipelines? Because **fitting the scaler on the whole dataset leaks information** from validation folds and inflates scores.\n",
        "\n",
        "We compare two baselines:\n",
        "- **Logistic regression** with regularisation.\n",
        "- **Random forest** as a flexible non-linear model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=11088)\n",
        "\n",
        "pipe_lr = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
        "])\n",
        "grid_lr = GridSearchCV(\n",
        "    pipe_lr,\n",
        "    param_grid={'clf__C':[0.1, 0.5, 1, 2, 5]},\n",
        "    scoring='roc_auc', cv=cv, n_jobs=-1\n",
        ").fit(X_tr, y_tr)\n",
        "\n",
        "pipe_rf = Pipeline([('clf', RandomForestClassifier(random_state=11088, class_weight='balanced'))])\n",
        "grid_rf = GridSearchCV(\n",
        "    pipe_rf,\n",
        "    param_grid={\n",
        "        'clf__n_estimators':[200, 400, 800],\n",
        "        'clf__max_depth':[None, 10, 20],\n",
        "        'clf__max_features':['sqrt','log2']\n",
        "    },\n",
        "    scoring='roc_auc', cv=cv, n_jobs=-1\n",
        ").fit(X_tr, y_tr)\n",
        "\n",
        "print('Best LR:', grid_lr.best_params_, 'CV AUC:', round(grid_lr.best_score_, 3))\n",
        "print('Best RF:', grid_rf.best_params_, 'CV AUC:', round(grid_rf.best_score_, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 4) Learning curves: do we need more data, or a simpler model?\n",
        "\n",
        "Interpretation guide:\n",
        "- **Train and CV both low**: underfitting → increase model capacity or add features.\n",
        "- **Train ≫ CV**: high variance → get more data, regularise, or simplify."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def plot_learning_curve(est, X, y, title):\n",
        "    sizes, train_scores, val_scores = learning_curve(\n",
        "        est, X, y, cv=cv, scoring='roc_auc', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 8), random_state=11088\n",
        "    )\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(sizes, train_scores.mean(axis=1), label='Train')\n",
        "    plt.plot(sizes, val_scores.mean(axis=1), label='CV')\n",
        "    plt.xlabel('Training size')\n",
        "    plt.ylabel('ROC-AUC')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_learning_curve(grid_lr.best_estimator_, X_tr, y_tr, 'Logistic learning curve')\n",
        "plot_learning_curve(grid_rf.best_estimator_, X_tr, y_tr, 'RandomForest learning curve')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 5) Final evaluation on the **sealed** test set\n",
        "\n",
        "We report:\n",
        "- ROC-AUC (ranking quality)\n",
        "- Precision–Recall curve (useful if the positive class is rarer)\n",
        "- Confusion matrix at a neutral threshold (0.5) **and** at a **utility-tuned** threshold (later exercise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "for name, model in [('Logistic', grid_lr.best_estimator_), ('RandomForest', grid_rf.best_estimator_)]:\n",
        "    proba = model.predict_proba(X_te)[:,1]\n",
        "    auc = roc_auc_score(y_te, proba)\n",
        "    print(f\"\\n{name}: Test ROC-AUC={auc:.3f}\")\n",
        "    RocCurveDisplay.from_predictions(y_te, proba)\n",
        "    plt.title(f'{name} ROC')\n",
        "    plt.show()\n",
        "\n",
        "    pr, rc, _ = precision_recall_curve(y_te, proba)\n",
        "    PrecisionRecallDisplay(precision=pr, recall=rc).plot()\n",
        "    plt.title(f'{name} PR')\n",
        "    plt.show()\n",
        "\n",
        "    yhat = (proba >= 0.5).astype(int)\n",
        "    ConfusionMatrixDisplay.from_predictions(y_te, yhat)\n",
        "    plt.title(f'{name} @0.5')\n",
        "    plt.show()\n",
        "    print(classification_report(y_te, yhat, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 6) Interpreting models responsibly\n",
        "\n",
        "### Permutation importance (model-agnostic)\n",
        "Shuffle one feature at a time in the **test** set; the drop in metric estimates its importance. Less biased than tree impurity.\n",
        "\n",
        "### Partial dependence\n",
        "Average effect of a feature on predictions—useful for sanity-checking directionality (but beware of strong feature interactions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "rf = grid_rf.best_estimator_\n",
        "perm = permutation_importance(rf, X_te, y_te, scoring='roc_auc', n_repeats=20, random_state=11088)\n",
        "\n",
        "imp = (\n",
        "    pd.DataFrame({'feature': df.columns[:-1], 'importance': perm.importances_mean})\n",
        "    .sort_values('importance', ascending=False)\n",
        "    .head(12)\n",
        ")\n",
        "imp\n",
        "\n",
        "# Bar plot with matplotlib\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.barh(imp['feature'][::-1], imp['importance'][::-1])\n",
        "plt.xlabel('Permutation importance (AUC drop)')\n",
        "plt.title('Top features (RandomForest)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Partial dependence on a few pedagogically named features\n",
        "features_to_plot = []\n",
        "for f in ['mud_minutes', 'forage_quality', 'dominance_index', 'water_salinity']:\n",
        "    if f in df.columns:\n",
        "        features_to_plot.append(list(df.columns[:-1]).index(f))\n",
        "\n",
        "if features_to_plot:\n",
        "    PartialDependenceDisplay.from_estimator(rf, X_te, features=features_to_plot)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 7) Probability calibration: when 0.7 should mean 70%\n",
        "\n",
        "If decisions (e.g., scheduling a dental check) depend on **probabilities**, calibration is as important as discrimination.\n",
        "\n",
        "- **Brier score**: mean squared error of probability forecasts.\n",
        "- Fix miscalibration with **isotonic** (non-parametric) or **Platt** (sigmoid) scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "uncal = grid_rf.best_estimator_\n",
        "cal_iso = CalibratedClassifierCV(uncal, method='isotonic', cv=5).fit(X_tr, y_tr)\n",
        "\n",
        "for name, model in [('Uncalibrated RF', uncal), ('Isotonic RF', cal_iso)]:\n",
        "    p = model.predict_proba(X_te)[:,1]\n",
        "    print(f\"{name}: Brier={brier_score_loss(y_te,p):.3f}, AUC={roc_auc_score(y_te,p):.3f}\")\n",
        "    CalibrationDisplay.from_predictions(y_te, p, n_bins=10)\n",
        "    plt.title(f'Calibration: {name}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 8) Robustness checks and simple stress tests\n",
        "\n",
        "- **Add noise** to inputs (simulates measurement error) and re-check AUC.\n",
        "- **Drop top features** and see how brittle the model is.\n",
        "- **Shift** a key feature at inference-time to mimic **dataset drift**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def stress_noise(model, X, y, sigma):\n",
        "    p = model.predict_proba(X + np.random.normal(0, sigma, X.shape))[:,1]\n",
        "    return roc_auc_score(y, p)\n",
        "\n",
        "print(\"AUC under additive noise:\")\n",
        "for s in [0.0, 0.05, 0.1, 0.2]:\n",
        "    print(f\"  σ={s}: {stress_noise(uncal, X_te, y_te, s):.3f}\")\n",
        "\n",
        "# Drift: add a +0.3 shift to 'water_salinity' at test time (if present)\n",
        "X_te_drift = X_te.copy()\n",
        "if 'water_salinity' in df.columns:\n",
        "    j = list(df.columns[:-1]).index('water_salinity')\n",
        "    X_te_drift[:, j] = X_te_drift[:, j] + 0.3\n",
        "\n",
        "p_norm = uncal.predict_proba(X_te)[:,1]\n",
        "p_drift = uncal.predict_proba(X_te_drift)[:,1]\n",
        "print(\"\\nAUC normal:\", roc_auc_score(y_te, p_norm))\n",
        "print(\"AUC with +0.3 drift on water_salinity:\", roc_auc_score(y_te, p_drift))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 9) Exercises (for credit)\n",
        "1. **Utility-tuned threshold**: Suppose FP costs 1 and FN costs 5. On the test set, sweep thresholds to minimise expected cost. Report the chosen threshold and show the confusion matrix.\n",
        "2. **Top-k features**: Refit the RF using only the top-10 permutation-important features. Compare test AUC and calibration.\n",
        "3. **Monitoring plan**: Propose simple metrics to monitor **data drift** monthly (e.g., population means/SDs, PSI) and a policy for **recalibration**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 10) Mini Model Card (template)\n",
        "\n",
        "**Intended use**: Flag hippos for proactive dental checks at Riverbend.  \n",
        "**Data**: 1,200 synthetic hippo-days; 30 numeric features; mild imbalance.  \n",
        "**Preprocessing**: Standardisation (for LR) within CV folds; RF on raw inputs.  \n",
        "**Algorithms**: Logistic regression (L2), Random forest.  \n",
        "**Validation**: 5-fold stratified CV for selection; single held-out test for final report.  \n",
        "**Performance (example)**: RF test ROC-AUC ≈ 0.85–0.9 (your run may vary); calibration improved by isotonic scaling.  \n",
        "**Robustness**: Moderate sensitivity to additive noise; AUC declines under feature drift (e.g., water_salinity +0.3).  \n",
        "**Fairness**: If subgroup labels (sex/age/pen condition) existed, report per-group metrics.  \n",
        "**Limitations**: Synthetic features; unmodelled temporal/seasonal effects; no clinician/keeper feedback loop.  \n",
        "**Update policy**: Retrain quarterly or when drift metrics breach thresholds; recalibrate monthly if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "### Key takeaways\n",
        "- **Pipelines + CV** prevent information leakage.\n",
        "- **Learning curves** inform whether to simplify or collect more data.\n",
        "- **Calibration** is essential if probabilities guide actions.\n",
        "- Always document **stress tests**, **drift**, and **limitations**—that’s what trustworthy ML looks like."
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "Prepared for Gunter's ML course"
      }
    ],
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
