{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f8848f8",
   "metadata": {},
   "source": [
    "# üåü Machine Learning for Nutrition & Food Science: Cooking Up Insights! ü•ó\n",
    "\n",
    "Welcome to this tasty Jupyter Notebook on **machine learning (ML)** for nutrition and food science! Whether you're snacking at home üçé or stirring ideas in a classroom, this guide will take you on a flavorful journey through ML techniques to analyze nutrition and food data. We'll explore **classification** (e.g., healthy vs. unhealthy diets), **regression** (e.g., predicting nutrient content), **feature selection** (e.g., identifying key nutrients), and **model evaluation**! üç¥\n",
    "\n",
    "Expect hands-on code, fun exercises, and hidden treats (click the \"Details\" to reveal them)! Let's dig in! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83759ca4",
   "metadata": {},
   "source": [
    "## 1. Introduction to Machine Learning in Nutrition & Food Science üìä\n",
    "\n",
    "Nutrition and food science data are like a recipe book üìñ‚Äîfull of ingredients (nutrients), dishes (diets), and quality checks (food safety). Machine learning helps us:\n",
    "\n",
    "- **Classify** diets or foods (e.g., healthy vs. unhealthy).\n",
    "- **Predict** nutritional values (e.g., calories or protein content).\n",
    "- **Identify** key factors (e.g., nutrients driving health outcomes).\n",
    "\n",
    "We'll use Python with `scikit-learn`, `pandas`, and `matplotlib` to build ML models. No culinary degree needed‚Äîjust curiosity! üòÑ\n",
    "\n",
    "**Exercise 1**: Why is ML useful for nutrition and food science compared to traditional statistical methods? Jot down your thoughts (no code needed).\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Consider how ML handles complex datasets, non-linear relationships, and interactions between nutrients or food components.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6a1827",
   "metadata": {},
   "source": [
    "Let's start with loading the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99d2a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for Google Colab: Fetch datasets automatically or manually\n",
    "%run ../../bootstrap.py    # installs requirements + editable package\n",
    "\n",
    "import fns_toolkit as fns\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import RFE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a864b02",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression: Classifying Diets üçé\n",
    "\n",
    "Logistic regression is a classic ML method for binary classification (e.g., healthy vs. unhealthy diets). It‚Äôs simple yet powerful for nutrition data!\n",
    "\n",
    "### 2.1 Logistic Regression in Action\n",
    "\n",
    "Let's create a synthetic nutrition dataset (e.g., nutrient profiles) and classify diets as healthy or unhealthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11358506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate synthetic nutrition dataset (60 samples, 10 nutrients/features)\n",
    "np.random.seed(11088)\n",
    "data = pd.DataFrame({\n",
    "    'Calories': np.random.normal(500, 100, 60),\n",
    "    'Protein_g': np.random.normal(30, 5, 60),\n",
    "    'Carbs_g': np.random.normal(60, 10, 60),\n",
    "    'Fat_g': np.random.normal(20, 5, 60),\n",
    "    'Fiber_g': np.random.normal(10, 2, 60),\n",
    "    'Sugar_g': np.random.normal(15, 5, 60),\n",
    "    'Sodium_mg': np.random.normal(800, 200, 60),\n",
    "    'Vitamin_C_mg': np.random.normal(50, 10, 60),\n",
    "    'Calcium_mg': np.random.normal(300, 50, 60),\n",
    "    'Iron_mg': np.random.normal(5, 1, 60)\n",
    "})\n",
    "labels = np.random.choice([0, 1], size=60)  # 0 = unhealthy, 1 = healthy\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_scaled, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train logistic regression\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = log_reg.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Logistic Regression Accuracy: {accuracy:.2f} üéâ')\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix for Diet Classification ü•ó')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808aa1d3",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **StandardScaler**: Ensures nutrients (e.g., calories, protein) are on the same scale.\n",
    "- **LogisticRegression**: Predicts diet class (healthy/unhealthy) using nutrient profiles.\n",
    "- **Confusion Matrix**: Shows true positives, false positives, etc.\n",
    "\n",
    "**Exercise 2**: Add a penalty parameter (`C=0.1`) to the `LogisticRegression` model to increase regularization. Does the accuracy change? Why?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Solution</summary>\n",
    "Change the model line to:\n",
    "```python\n",
    "log_reg = LogisticRegression(C=0.1, random_state=42)\n",
    "```\n",
    "Smaller `C` increases regularization, reducing overfitting but potentially lowering accuracy if the model is too constrained.\n",
    "</details>\n",
    "\n",
    "**Learn More**: Check out [scikit-learn's Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for more details! üìö"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec0399d",
   "metadata": {},
   "source": [
    "## 3. Random Forest: Identifying Key Nutrients üå≥\n",
    "\n",
    "Random Forest is like a team of decision trees that classifies diets or ranks nutrients by importance. It‚Äôs great for pinpointing key dietary factors!\n",
    "\n",
    "### 3.1 Random Forest in Action\n",
    "\n",
    "Let‚Äôs use a Random Forest to classify diets and identify important nutrients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e5dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f'Random Forest Accuracy: {accuracy_rf:.2f} üåü')\n",
    "\n",
    "# Feature importance (important nutrients)\n",
    "importance = rf.feature_importances_\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(data.columns, importance, color='green', alpha=0.7)\n",
    "plt.title('Nutrient Importance for Diet Classification ü•ï')\n",
    "plt.xlabel('Nutrient')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig('rf_importance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552fe287",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **RandomForestClassifier**: Combines multiple decision trees for robust predictions.\n",
    "- **feature_importances_**: Ranks nutrients by their contribution to diet classification.\n",
    "\n",
    "**Exercise 3**: Increase `n_estimators` to 200. Does the accuracy improve? Are the top nutrients the same?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "More trees reduce variance but may not significantly change feature rankings. Compare the accuracy and plot!\n",
    "</details>\n",
    "\n",
    "**Learn More**: Explore [Random Forests in scikit-learn](https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees) for more ML fun! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec04d2d",
   "metadata": {},
   "source": [
    "## 4. Support Vector Machines (SVM): Classifying Food Quality ‚öîÔ∏è\n",
    "\n",
    "SVMs find the best boundary to separate classes, ideal for tasks like classifying food quality (e.g., fresh vs. spoiled) based on nutritional or sensory data.\n",
    "\n",
    "### 4.1 SVM in Action\n",
    "\n",
    "Let‚Äôs classify foods as fresh or spoiled using an SVM with a linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cce0c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Train SVM (using same dataset, now for food quality)\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train, y_train)  # Labels: 0 = spoiled, 1 = fresh\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f'SVM Accuracy: {accuracy_svm:.2f} üõ°Ô∏è')\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Greens', cbar=False)\n",
    "plt.title('Confusion Matrix for Food Quality Classification üçä')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('svm_cm.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee2241",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **SVC**: Support Vector Classifier with a linear kernel.\n",
    "- **Hyperplane**: Maximizes the margin between fresh and spoiled classes.\n",
    "\n",
    "**Exercise 4**: Try an RBF kernel (`kernel='rbf'`) in the SVM. Does the accuracy improve? Why might this happen?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Solution</summary>\n",
    "Change the SVM line to:\n",
    "```python\n",
    "svm = SVC(kernel='rbf', random_state=42)\n",
    "```\n",
    "The RBF kernel captures non-linear patterns, which may improve accuracy if the data has complex relationships.\n",
    "</details>\n",
    "\n",
    "**Learn More**: Dive into [SVMs in scikit-learn](https://scikit-learn.org/stable/modules/svm.html) for more insights! üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fa2cd0",
   "metadata": {},
   "source": [
    "## 5. Regression with Gradient Boosting: Predicting Nutrient Content üìà\n",
    "\n",
    "Sometimes we need to predict continuous outcomes, like the calorie content of a meal. Gradient Boosting is a powerful ML method for regression tasks.\n",
    "\n",
    "### 5.1 Gradient Boosting in Action\n",
    "\n",
    "Let‚Äôs predict calorie content using Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e77c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate synthetic calorie outcome\n",
    "outcome = data['Calories'] + np.random.normal(0, 20, 60)  # Correlated with calories\n",
    "\n",
    "# Use other nutrients as features\n",
    "features = data.drop(columns=['Calories'])\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(features_scaled, outcome, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Gradient Boosting\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gbr.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_gbr = gbr.predict(X_test_reg)\n",
    "mse = mean_squared_error(y_test_reg, y_pred_gbr)\n",
    "print(f'Gradient Boosting MSE: {mse:.2f} üìâ')\n",
    "\n",
    "# Plot predictions vs true values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test_reg, y_pred_gbr, c='purple', alpha=0.7)\n",
    "plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'r--')\n",
    "plt.xlabel('True Calories (kcal)')\n",
    "plt.ylabel('Predicted Calories (kcal)')\n",
    "plt.title('Gradient Boosting Predictions for Calories ü•ê')\n",
    "plt.grid(True)\n",
    "plt.savefig('gbr_predictions.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b822d7",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **GradientBoostingRegressor**: Builds trees sequentially, correcting errors of previous trees.\n",
    "- **MSE**: Measures prediction error (lower is better).\n",
    "\n",
    "**Exercise 5**: Increase `n_estimators` to 200 in the Gradient Boosting model. Does the MSE decrease? Why?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "More trees improve fit but risk overfitting. Check if the MSE stabilizes or worsens!\n",
    "</details>\n",
    "\n",
    "**Learn More**: Explore [Gradient Boosting in scikit-learn](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting) for more details! üåü"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565aaa56",
   "metadata": {},
   "source": [
    "## 6. Feature Selection: Finding Key Nutritional Factors ‚≠ê\n",
    "\n",
    "Nutrition datasets often have many features. Feature selection helps us pick the most important nutrients (e.g., for health outcomes) using Recursive Feature Elimination (RFE).\n",
    "\n",
    "### 6.1 RFE with Random Forest\n",
    "\n",
    "Let‚Äôs use RFE to select the top 5 nutrients for diet classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a64f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply RFE with Random Forest\n",
    "rfe = RFE(estimator=RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=5)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Get selected features\n",
    "selected_features = data.columns[rfe.support_]\n",
    "print('Top 5 Nutrients:', selected_features.tolist(), 'üéØ')\n",
    "\n",
    "# Train Random Forest on selected features\n",
    "X_train_selected = X_train[:, rfe.support_]\n",
    "X_test_selected = X_test[:, rfe.support_]\n",
    "rf_selected = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "accuracy_selected = accuracy_score(y_test, y_pred_selected)\n",
    "print(f'Accuracy with Selected Features: {accuracy_selected:.2f} üöÄ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355f05e5",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **RFE**: Recursively eliminates less important nutrients.\n",
    "- **selected_features**: The top 5 nutrients for diet classification.\n",
    "\n",
    "**Exercise 6**: Change `n_features_to_select` to 3. Does the accuracy drop? Why might this happen?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Solution</summary>\n",
    "Fewer features may reduce model performance if critical information is lost. Compare the accuracy and consider the trade-off!\n",
    "</details>\n",
    "\n",
    "**Learn More**: Check out [Feature Selection in scikit-learn](https://scikit-learn.org/stable/modules/feature_selection.html) for more techniques! üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033d233f",
   "metadata": {},
   "source": [
    "## 7. Summary: Your ML Toolkit for Nutrition & Food Science üß∞\n",
    "\n",
    "Here's what you've cooked up:\n",
    "\n",
    "- **Logistic Regression** üìä: Simple classification of diets.\n",
    "- **Random Forest** üå≥: Robust classification and nutrient ranking.\n",
    "- **SVM** ‚öîÔ∏è: Powerful for food quality classification.\n",
    "- **Gradient Boosting** üìà: Accurate prediction of nutrient content.\n",
    "- **Feature Selection** ‚≠ê: Identifies key nutritional factors.\n",
    "\n",
    "**Final Exercise**: Download a real nutrition dataset (e.g., from [USDA FoodData Central](https://fdc.nal.usda.gov/)) and apply one of these ML methods. Write a short paragraph summarizing your results!\n",
    "\n",
    "**What's Next?** Try advanced ML techniques like neural networks or combine ML with PCA for nutrition data. Keep exploring, and happy analyzing! üòÑ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
