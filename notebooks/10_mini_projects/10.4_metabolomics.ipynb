{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbc94e4c",
   "metadata": {},
   "source": [
    "\n",
    "# üß™ Multivariate Analysis: Concepts and Applications\n",
    "\n",
    "Welcome to this notebook on **multivariate analysis**!  \n",
    "Multivariate methods are essential for exploring, modelling, and understanding complex datasets ‚Äî where many variables interact simultaneously.\n",
    "\n",
    "We'll explore:\n",
    "- ‚ú® Key concepts of multivariate analysis\n",
    "- üß© Principal Component Analysis (PCA)\n",
    "- üéØ Partial Least Squares Discriminant Analysis (PLS-DA)\n",
    "- üìà Bayesian multivariate models\n",
    "- üîç Machine learning approaches to classification and prediction\n",
    "\n",
    "As a practical example, we'll use a **synthetic metabolomics dataset**, similar to real-world studies, to illustrate the methods.  \n",
    "However, the techniques you will learn apply just as well to fields like nutrition, clinical data, finance, or engineering!\n",
    "\n",
    "Let's dive in! üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "Before we start, let's set up the workspace, load the data and the necessary libraries:\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Detailed description of libraries</summary>\n",
    "\n",
    "\n",
    "#### 1. NumPy\n",
    "\n",
    "- **Purpose**: NumPy is the foundational library for numerical computing in Python. It provides efficient array operations, mathematical functions, and linear algebra tools (e.g., matrix inversion, eigenvalues) used in data preprocessing and calculations like Hotelling‚Äôs T¬≤.\n",
    "- **Used For**: Array manipulation, linear algebra (e.g., `la.inv` for inverse covariance in PCA), and mathematical operations.\n",
    "- **Documentation**: [NumPy Documentation](https://numpy.org/doc/stable/)\n",
    "\n",
    "#### 2. Pandas\n",
    "\n",
    "- **Purpose**: Pandas offers data structures (e.g., DataFrame) and tools for data manipulation and analysis, ideal for handling tabular data like datasets for PCA or machine learning.\n",
    "- **Used For**: Data loading (e.g., CSV files), preprocessing, and feature engineering before PCA or PLS regression.\n",
    "- **Documentation**: [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "\n",
    "#### 3. Scikit-learn (sklearn)\n",
    "\n",
    "- **Purpose**: Scikit-learn is a comprehensive machine learning library providing tools for data preprocessing, dimensionality reduction, regression, classification, and model evaluation.\n",
    "- **Used For**:\n",
    "  - `StandardScaler`: Standardizing features for PCA or PLS regression.\n",
    "  - `PCA`: Dimensionality reduction for data visualization or analysis.\n",
    "  - `PLSRegression`: Partial Least Squares regression for predictive modeling.\n",
    "  - `train_test_split`: Splitting data into training and test sets.\n",
    "  - `accuracy_score`: Evaluating classification model performance.\n",
    "  - `RandomForestClassifier`: Building ensemble classification models.\n",
    "- **Documentation**: [Scikit-learn Documentation](https://scikit-learn.org/stable/)\n",
    "\n",
    "#### 4. Matplotlib\n",
    "\n",
    "- **Purpose**: Matplotlib is a plotting library for creating static, interactive, and publication-quality visualizations, such as scatter plots or PCA biplots.\n",
    "- **Used For**: Visualizing PCA results (e.g., scatter plots with Hotelling‚Äôs T¬≤ ellipses) or model performance metrics.\n",
    "- **Documentation**: [Matplotlib Documentation](https://matplotlib.org/stable/)\n",
    "\n",
    "#### 5. PyMC\n",
    "\n",
    "- **Purpose**: PyMC is a probabilistic programming library for Bayesian statistical modeling and inference, enabling flexible model specification and sampling.\n",
    "- **Used For**: Building Bayesian models to estimate parameters or uncertainty, potentially for metabolomics or dietary data analysis.\n",
    "- **Documentation**: [PyMC Documentation](https://www.pymc.io/)\n",
    "\n",
    "#### 6. ArviZ\n",
    "\n",
    "- **Purpose**: ArviZ is a library for exploratory analysis of Bayesian models, providing tools for visualizing posterior distributions, convergence diagnostics, and model comparison.\n",
    "- **Used For**: Analyzing and visualizing PyMC model outputs (e.g., trace plots, posterior predictive checks).\n",
    "- **Documentation**: [ArviZ Documentation](https://python.arviz.org/en/stable/)\n",
    "\n",
    "#### 7. SciPy\n",
    "\n",
    "- **Purpose**: SciPy builds on NumPy to provide advanced scientific computing tools, including statistical distributions, optimization, and signal processing.\n",
    "- **Used For**: Statistical functions (e.g., `f.ppf` for F-distribution critical values in Hotelling‚Äôs T¬≤) and linear algebra operations.\n",
    "- **Documentation**: [SciPy Documentation](https://docs.scipy.org/doc/scipy/)\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75548e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for Google Colab: Fetch datasets automatically or manually\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "# Define the module and dataset for this notebook\n",
    "MODULE = '10_mini_projects'  # e.g., '01_infrastructure'\n",
    "DATASET = 'metabolomics_dataset.csv'  # e.g., 'hippo_diets.csv'\n",
    "BASE_PATH = '/content/data-analysis-toolkit-FNS'\n",
    "MODULE_PATH = os.path.join(BASE_PATH, 'notebooks', MODULE)\n",
    "DATASET_PATH = os.path.join('data', DATASET)\n",
    "\n",
    "# Step 1: Attempt to clone the repository (automatic method)\n",
    "# Note: If you encounter a cloning error (e.g., 'fatal: destination path already exists'),\n",
    "#       reset the runtime (Runtime > Restart runtime) and run this cell again.\n",
    "try:\n",
    "    print('Attempting to clone repository...')\n",
    "    if os.path.exists(BASE_PATH):\n",
    "        print('Repository already exists, skipping clone.')\n",
    "    else:\n",
    "        !git clone https://github.com/ggkuhnle/data-analysis-toolkit-FNS.git\n",
    "    \n",
    "    # Debug: Print directory structure\n",
    "    print('Listing repository contents:')\n",
    "    !ls {BASE_PATH}\n",
    "    print(f'Listing notebooks directory contents:')\n",
    "    !ls {BASE_PATH}/notebooks\n",
    "    \n",
    "    # Check if the module directory exists\n",
    "    if not os.path.exists(MODULE_PATH):\n",
    "        raise FileNotFoundError(f'Module directory {MODULE_PATH} not found. Check the repository structure.')\n",
    "    \n",
    "    # Set working directory to the notebook's folder\n",
    "    os.chdir(MODULE_PATH)\n",
    "    \n",
    "    # Verify dataset is accessible\n",
    "    if os.path.exists(DATASET_PATH):\n",
    "        print(f'Dataset found: {DATASET_PATH} ü¶õ')\n",
    "    else:\n",
    "        print(f'Error: Dataset {DATASET} not found after cloning.')\n",
    "        raise FileNotFoundError\n",
    "except Exception as e:\n",
    "    print(f'Cloning failed: {e}')\n",
    "    print('Falling back to manual upload option...')\n",
    "\n",
    "    # Step 2: Manual upload option\n",
    "    print(f'Please upload {DATASET} manually.')\n",
    "    print(f'1. Click the \"Choose Files\" button below.')\n",
    "    print(f'2. Select {DATASET} from your local machine.')\n",
    "    print(f'3. Ensure the file is placed in notebooks/{MODULE}/data/')\n",
    "    \n",
    "    # Create the data directory if it doesn't exist\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    # Prompt user to upload the dataset\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Check if the dataset was uploaded\n",
    "    if DATASET in uploaded:\n",
    "        with open(DATASET_PATH, 'wb') as f:\n",
    "            f.write(uploaded[DATASET])\n",
    "        print(f'Successfully uploaded {DATASET} to {DATASET_PATH} ü¶õ')\n",
    "    else:\n",
    "        raise FileNotFoundError(f'Upload failed. Please ensure you uploaded {DATASET}.')\n",
    "\n",
    "# Install required packages for this notebook\n",
    "%pip install pandas numpy\n",
    "print('Python environment ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3feb598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "from scipy.stats import f, chi2\n",
    "import numpy.linalg as la\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seaborn style for professional, clean visuals\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Load metabolomics dataset (1000 participants, 200 metabolites)\n",
    "df = pd.read_csv('data/metabolomics_dataset.csv')\n",
    "\n",
    "# Extract features (metabolites) and labels\n",
    "X = df.filter(like='Metabolite_')  # Columns like 'Metabolite_1', ..., 'Metabolite_200'\n",
    "labels = df['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e884a36",
   "metadata": {},
   "source": [
    "## 1. Introduction to Multivariate Analysis üìä\n",
    "\n",
    "Multivariate datasets - such as for example in metabolomics - are like a galaxy of stars ‚ú® ‚Äî thousands of data, each twinkling with information. Multivariate analysis helps us find patterns, classify samples (e.g., healthy vs. diseased), and uncover biomarkers. These methods are essential because:\n",
    "\n",
    "- **High-dimensionality**: Metabolomics data often have more variables (metabolites) than samples.\n",
    "- **Correlations**: Metabolites do not act in isolation; they interact in complex and structured ways.\n",
    "- **Noise**: Biological and technical variability can obscure true signals.\n",
    "\n",
    "In this notebook, we will use Python with libraries such as `scikit-learn`, `PyMC`, and `pandas` to explore these techniques.  \n",
    "If you are new to this area ‚Äî no problem! We'll guide you through each step. üòä\n",
    "\n",
    "---\n",
    "\n",
    "### **Exercise 1**  \n",
    "Why do you think multivariate methods are better than analysing each metabolite separately?  \n",
    "Write your thoughts below ‚Äî no code needed, just reflect.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "Think about how metabolites might be connected through biological pathways, and why examining them together could reveal patterns that are invisible when looking at them one by one.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0f308",
   "metadata": {},
   "source": [
    "## 2. Principal Component Analysis (PCA): The Unsupervised Explorer üó∫Ô∏è\n",
    "\n",
    "PCA is like a treasure map for your data ‚Äî it reduces dimensionality by finding **principal components** (PCs) that capture the greatest variance.  \n",
    "In metabolomics, PCA helps us:\n",
    "\n",
    "- Visualise sample similarities and groupings üß©\n",
    "- Detect outliers üö®\n",
    "- Explore underlying structure without using class labels (unsupervised)\n",
    "\n",
    "PCA projects high-dimensional data onto a smaller number of dimensions while preserving as much information as possible.  \n",
    "It is often the first step in a metabolomics analysis to get a quick overview of the dataset.\n",
    "\n",
    "We will implement PCA using `scikit-learn` and visualise the results with `matplotlib` and `seaborn`. üìà\n",
    "\n",
    "---\n",
    "\n",
    "### **Exercise 2**  \n",
    "Before we dive into the code, think about this:  \n",
    "Why might PCA sometimes *hide* important biological information?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "PCA is optimised for variance, not necessarily for biological relevance. Sometimes important differences (e.g., between healthy and diseased) might not be the biggest source of variance!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2430ad9e",
   "metadata": {},
   "source": [
    "## Step 1: Visualizing the Messy Raw Data üìä\n",
    "\n",
    "Our dataset has 200 metabolites per participant, creating a high-dimensional maze. Direct visualization is nearly impossible (imagine a 200-dimensional scatter plot!). Instead, we‚Äôll use two techniques to show the data‚Äôs ‚Äúmessiness‚Äù:\n",
    "\n",
    "- **Correlation Heatmap**: Reveals pairwise correlations between metabolites. High correlations suggest redundant features, a common issue in metabolomics that PCA can address.\n",
    "- **Pairwise Scatter Plots**: Shows relationships between a subset of metabolites, highlighting how scattered and unstructured the raw data appears.\n",
    "\n",
    "These plots will demonstrate why we need PCA to simplify this complex dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ac0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize raw data: Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))  # Larger size for 200 metabolites\n",
    "X_df = df.filter(like='Metabolite_')  # Ensure we use the DataFrame\n",
    "corr = X_df.corr()  # Compute pairwise correlations\n",
    "sns.heatmap(corr, cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
    "plt.title('Correlation Heatmap of 200 Metabolites')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33b6b8b",
   "metadata": {},
   "source": [
    "### What‚Äôs Happening Here? ü§î\n",
    "The heatmap shows correlations between our 200 metabolites. Notice the dense patterns of red (positive) and blue (negative) correlations ‚Äî this redundancy makes the data ‚Äúmessy‚Äù and hard to interpret. Many metabolites move together, suggesting we can reduce dimensions without losing much information.\n",
    "\n",
    "Next, let‚Äôs try visualizing pairs of metabolites to see if patterns emerge naturally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8906b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize raw data: Pairwise scatter plots for top 5 metabolites\n",
    "subset_cols = df.filter(like='Metabolite_').columns[:5]  # Select first 5 metabolite columns\n",
    "sns.pairplot(df[subset_cols], diag_kind='kde', plot_kws={'alpha': 0.5})\n",
    "plt.suptitle('Pairwise Scatter Plots of Raw Metabolomics Data', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd042337",
   "metadata": {},
   "source": [
    "### The High-Dimensional Challenge üå™Ô∏è\n",
    "The scatter plots show relationships between just 5 of our 200 metabolites, and already it‚Äôs chaotic! Points are scattered, with no clear groupings, and we‚Äôre only seeing a tiny slice of the data. Imagine trying to plot all 200 dimensions ‚Äî it‚Äôs impossible! This messiness is why PCA is our go-to tool: it finds the directions (principal components) that capture the most variance, simplifying the data into a 2D map we can explore.\n",
    "\n",
    "Before PCA, we need to preprocess the data to ensure fair comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533b9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess: Standardize features to ensure equal scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf26b5d9",
   "metadata": {},
   "source": [
    "## Step 2: Applying PCA üßë‚Äçüî¨\n",
    "\n",
    "Now that we‚Äôve seen the raw data‚Äôs complexity, let‚Äôs apply PCA to reduce our 200 metabolites to 2 principal components (PCs). PCA identifies the directions of maximum variance, projecting our high-dimensional data onto a 2D plane. We‚Äôll use `scikit-learn`‚Äôs `PCA` to do this efficiently.\n",
    "\n",
    "Standardization (via `StandardScaler`) was critical to ensure metabolites with different scales (e.g., concentrations) don‚Äôt skew the results. Let‚Äôs transform the data and explore the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393cf9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de3516",
   "metadata": {},
   "source": [
    "## Step 3: Detecting Outliers with Hotelling‚Äôs T¬≤ üö®\n",
    "\n",
    "PCA helps us spot outliers ‚Äî samples that deviate significantly from the main data cloud. We‚Äôll use **Hotelling‚Äôs T¬≤**, a statistical test that measures how far each sample is from the center of the PCA scores, accounting for the data‚Äôs variance. Samples outside a 95% confidence ellipse are flagged as outliers.\n",
    "\n",
    "We‚Äôll compute T¬≤ scores using `numpy.linalg` and `scipy.stats.chi2`, then visualize them in our PCA plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dfb0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure labels are numerical for plotting (if categorical, encode them)\n",
    "if labels.dtype == 'object':\n",
    "    labels = pd.Categorical(labels).codes  # Convert to numerical codes (e.g., 0, 1)\n",
    "\n",
    "# Calculate Hotelling's T^2 for outlier detection\n",
    "n_samples, n_components = pca_result.shape\n",
    "mean_scores = np.mean(pca_result, axis=0)\n",
    "cov_scores = np.cov(pca_result.T)\n",
    "# Ensure covariance matrix is well-conditioned\n",
    "if np.linalg.cond(cov_scores) < 1e6:  # Check condition number\n",
    "    inv_cov = la.inv(cov_scores)\n",
    "else:\n",
    "    # Add small diagonal for numerical stability\n",
    "    cov_scores += np.eye(n_components) * 1e-6\n",
    "    inv_cov = la.inv(cov_scores)\n",
    "\n",
    "# Compute T^2 scores\n",
    "t2_scores = np.array([\n",
    "    (pca_result[i] - mean_scores).T @ inv_cov @ (pca_result[i] - mean_scores)\n",
    "    for i in range(n_samples)\n",
    "])\n",
    "\n",
    "# Use chi-squared distribution for 95% confidence ellipse (simpler for 2D PCA)\n",
    "alpha = 0.05\n",
    "critical_value = chi2.ppf(1 - alpha, df=n_components)  # Chi-squared with 2 degrees of freedom\n",
    "\n",
    "# Identify outliers\n",
    "outliers = t2_scores > critical_value\n",
    "\n",
    "# Compute ellipse for 95% confidence region\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_scores)\n",
    "radii = np.sqrt(critical_value * eigenvalues)  # Scale by chi-squared critical value\n",
    "theta = np.linspace(0, 2 * np.pi, 100)\n",
    "ellipse = (eigenvectors @ np.diag(radii) @ np.array([np.cos(theta), np.sin(theta)])).T + mean_scores\n",
    "\n",
    "# Visualize PCA results with Hotelling's T^2 ellipse\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], c=labels, cmap='viridis', alpha=0.7, label='Samples')\n",
    "plt.scatter(pca_result[outliers, 0], pca_result[outliers, 1], c='red', s=100, marker='x', label='Outliers')\n",
    "plt.plot(ellipse[:, 0], ellipse[:, 1], 'k--', label='95% Confidence Ellipse')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA of Metabolomics Data (1000 Participants) üó∫Ô∏è')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print explained variance ratio and outlier count\n",
    "print(f'Explained Variance Ratio: PC1 = {pca.explained_variance_ratio_[0]:.2f}, '\n",
    "      f'PC2 = {pca.explained_variance_ratio_[1]:.2f}')\n",
    "print(f'Number of Outliers Detected: {np.sum(outliers)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e00de",
   "metadata": {},
   "source": [
    "## Step 4: Interpreting the PCA Results üß©\n",
    "\n",
    "Wow, look at that plot! Compared to the raw data‚Äôs chaos, PCA reveals clear groupings and outliers. The **explained variance ratio** tells us how much information PC1 and PC2 capture (e.g., 35% and 20%). Outliers outside the ellipse may indicate unusual samples, like measurement errors or unique biological profiles.\n",
    "\n",
    "But PCA isn‚Äôt perfect. Let‚Äôs reflect on its limitations before moving forward.\n",
    "\n",
    "---\n",
    "\n",
    "### **Exercise 2**\n",
    "Why might PCA sometimes *hide* important biological information?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "PCA is optimised for variance, not necessarily for biological relevance. Important differences (e.g., between healthy and diseased) might not be the biggest source of variance!\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Points\n",
    "- **Raw Data Messiness**: The heatmap and scatter plots showed high dimensionality and redundancy, making PCA essential.\n",
    "- **PCA‚Äôs Power**: PCA simplifies 200 metabolites into 2D, revealing patterns and outliers.\n",
    "- **Preprocessing Matters**: Standardization ensures fair metabolite comparisons.\n",
    "- **Outlier Detection**: Hotelling‚Äôs T¬≤ flags anomalies for further investigation.\n",
    "\n",
    "*Ready to explore more of your data‚Äôs hidden treasures? Let‚Äôs keep going! üßë‚Äçüî¨*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816ae0d4",
   "metadata": {},
   "source": [
    "## Step 5: Interpreting the PCA Results üß©\n",
    "\n",
    "Wow, look at that plot! Compared to the raw data‚Äôs chaos, PCA reveals clear groupings and outliers. The **explained variance ratio** tells us how much information PC1 and PC2 capture (e.g., 35% and 20%). Outliers outside the ellipse may indicate unusual samples, like measurement errors or unique biological profiles.\n",
    "\n",
    "But PCA isn‚Äôt perfect. Let‚Äôs reflect on its limitations before moving forward.\n",
    "\n",
    "---\n",
    "\n",
    "### **Exercise 2**\n",
    "Why might PCA sometimes *hide* important biological information?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "PCA is optimised for variance, not necessarily for biological relevance. Important differences (e.g., between healthy and diseased) might not be the biggest source of variance!\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Points\n",
    "- **Raw Data Messiness**: The heatmap and scatter plots showed high dimensionality and redundancy, making PCA essential.\n",
    "- **PCA‚Äôs Power**: PCA simplifies 200 metabolites into 2D, revealing patterns and outliers.\n",
    "- **Preprocessing Matters**: Standardization ensures fair metabolite comparisons.\n",
    "- **Outlier Detection**: Hotelling‚Äôs T¬≤ flags anomalies for further investigation.\n",
    "\n",
    "*Ready to explore more of your data‚Äôs hidden treasures? Let‚Äôs keep going! üßë‚Äçüî¨*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0685751",
   "metadata": {},
   "source": [
    "## 3. Partial Least Squares Discriminant Analysis (PLS-DA): The Supervised Classifier üè∑Ô∏è\n",
    "\n",
    "PLS-DA is like a guided missile üéØ‚Äîit‚Äôs supervised, meaning it uses class labels (e.g., \"healthy\" vs. \"diseased\") to find components that maximize both variance and group separation. Perfect for classifying metabolomics samples!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d2731f",
   "metadata": {},
   "source": [
    "### 3.1 PLS-DA in Action\n",
    "\n",
    "Let's add class labels to our synthetic dataset and apply PLS-DA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ac8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.filter(like='Metabolite_')\n",
    "y = df['Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11088)\n",
    "plsda = PLSRegression(n_components=2)\n",
    "plsda.fit(X_train, y_train)\n",
    "scores = plsda.transform(X)\n",
    "plt.scatter(scores[:, 0], scores[:, 1], c=df['Label'], cmap='viridis')\n",
    "plt.xlabel('PLS Component 1')\n",
    "plt.ylabel('PLS Component 2')\n",
    "plt.title('PLS-DA of Metabolomics Data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9729b925",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **PLSRegression**: Used for PLS-DA by treating class labels as continuous (threshold at 0.5 for binary classification).\n",
    "- **train_test_split**: Splits data to evaluate model performance.\n",
    "- **Scores Plot**: Shows how well PLS-DA separates classes.\n",
    "\n",
    "**Exercise 3**: Change `n_components` to 3 in the PLS-DA model. Does the accuracy improve? Why or why not?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "More components capture more variance but may lead to overfitting, especially with small datasets. Check the accuracy and consider the trade-off!\n",
    "</details>\n",
    "\n",
    "**Learn More**: Explore [PLS-DA in metabolomics](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6017634/) for real-world applications! üß¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd671d1",
   "metadata": {},
   "source": [
    "## 4. Bayesian Multivariate Models: Embracing Uncertainty üåà\n",
    "\n",
    "Bayesian methods are like a crystal ball üîÆ‚Äîthey model uncertainty and let us build flexible multivariate models. In metabolomics, Bayesian approaches can handle missing data, model latent variables, or perform regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f14302",
   "metadata": {},
   "source": [
    "### 4.1 Bayesian PCA with PyMC\n",
    "\n",
    "Let's use `PyMC` to implement a simple Bayesian PCA model. This assumes metabolites are generated from latent PCs with Gaussian noise.\n",
    "\n",
    "Please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8241b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.filter(like='Metabolite_').values\n",
    "with pm.Model() as bayes_pca:\n",
    "    z = pm.Normal('z', mu=0, sigma=1, shape=(1000, 2))  # Latent PCs\n",
    "    w = pm.Normal('w', mu=0, sigma=1, shape=(200, 2))  # Loadings\n",
    "    mu = pm.math.dot(z, w.T)\n",
    "    X_obs = pm.Normal('X', mu=mu, sigma=0.1, observed=X)\n",
    "    trace = pm.sample(500, return_inferencedata=True)\n",
    "az.plot_posterior(trace, var_names=['w'], coords={'w_dim_0': [0]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4523e1",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **z**: Latent PCs for each sample.\n",
    "- **w**: Loadings (how metabolites contribute to PCs).\n",
    "- **X**: Observed data modeled as a linear combination of PCs plus noise.\n",
    "- **pm.sample**: Uses MCMC to estimate posterior distributions.\n",
    "\n",
    "**Exercise 4**: Increase the number of samples (`500` to `1000`) in `pm.sample`. Does the posterior distribution change significantly? Why?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Solution</summary>\n",
    "More samples improve the precision of the posterior but may not change the mean estimates much if the model has converged. Check the plot for tighter distributions!\n",
    "</details>\n",
    "\n",
    "**Learn More**: Dive into [PyMC's documentation](https://www.pymc.io/welcome.html) for more Bayesian modeling ideas! üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d037ebf8",
   "metadata": {},
   "source": [
    "## 5. Machine Learning: A Quick Dip into Random Forests üå≥\n",
    "\n",
    "Machine learning (ML) is like a superpower for metabolomics‚Äîmodels like Random Forests can classify samples or identify important metabolites (potential biomarkers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de02f9f6",
   "metadata": {},
   "source": [
    "### 5.1 Random Forest Classifier\n",
    "\n",
    "Let's use a Random Forest to classify our samples and find important metabolites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd45a974",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.filter(like='Metabolite_')\n",
    "y = df['Label']\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=11088)\n",
    "rf.fit(X, y)\n",
    "importance = rf.feature_importances_\n",
    "plt.bar(range(10), importance[:10], tick_label=X.columns[:10])\n",
    "plt.xticks(rotation=90, ha='right')\n",
    "plt.title('Top 10 Metabolite Importances')\n",
    "plt.xlabel('Metabolite')\n",
    "plt.ylabel('Importance')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3006c420",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **RandomForestClassifier**: Builds multiple decision trees and aggregates their predictions.\n",
    "- **feature_importances_**: Shows which metabolites contribute most to classification (potential biomarkers!).\n",
    "\n",
    "**Exercise 5**: Increase `n_estimators` to 200. Does the accuracy improve? Plot the feature importances again‚Äîare the top metabolites the same?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "More trees reduce variance but may not change feature rankings much if the model is stable. Compare the plots visually!\n",
    "</details>\n",
    "\n",
    "**Learn More**: Check out [Random Forests in scikit-learn](https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees) for more ML fun! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace89ee4",
   "metadata": {},
   "source": [
    "## 6. Using Principal Components in Regression: Biomarker Detection üîç\n",
    "\n",
    "Now, let's use PCA scores as predictors in a regression model to predict a continuous outcome (e.g., disease severity). This combines dimensionality reduction with predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f2f7a2",
   "metadata": {},
   "source": [
    "### 6.1 PCA + Linear Regression\n",
    "\n",
    "We'll use the PCA scores from Section 2 and regress them against a synthetic outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb23b5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.filter(like='Metabolite_')\n",
    "y = df['Severity']\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(X_scaled)\n",
    "lr = LinearRegression()\n",
    "lr.fit(pca_result, y)\n",
    "y_pred = lr.predict(pca_result)\n",
    "plt.scatter(y, y_pred, c='purple', alpha=0.7)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "plt.xlabel('True Severity')\n",
    "plt.ylabel('Predicted Severity')\n",
    "plt.title('PCA + Linear Regression')\n",
    "plt.savefig('pca_regression.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e209bb",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **pca_result**: PCA scores (PCs) used as predictors.\n",
    "- **LinearRegression**: Models the relationship between PCs and the outcome.\n",
    "- **Scatter Plot**: Shows how well predictions match the true outcome.\n",
    "\n",
    "**Exercise 6**: Use only PC1 (`pca_result[:, 0].reshape(-1, 1)`) in the regression. Does the model perform worse? Why?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Solution</summary>\n",
    "Using only PC1 reduces the information available to the model, likely worsening performance unless PC1 captures most of the relevant variation. Compare the scatter plots!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ea0602",
   "metadata": {},
   "source": [
    "## 7. Summary: Your Metabolomics Toolkit üß∞\n",
    "\n",
    "Here's what you've learned:\n",
    "\n",
    "- **PCA** üó∫Ô∏è: Unsupervised, reduces dimensionality, explores data structure.\n",
    "- **PLS-DA** üè∑Ô∏è: Supervised, classifies samples, maximizes group separation.\n",
    "- **Bayesian Models** üîÆ: Handle uncertainty, flexible for complex problems.\n",
    "- **Random Forests** üå≥: ML for classification and biomarker detection.\n",
    "- **PCA + Regression** üîç: Uses PCs for predictive modeling (e.g., disease severity).\n",
    "\n",
    "**Final Exercise**: Pick a real metabolomics dataset (e.g., from [MetaboLights](https://www.ebi.ac.uk/metabolights/)) and apply one of these methods. Share your findings in a short paragraph!\n",
    "\n",
    "**What's Next?** Try advanced methods like t-SNE, SVMs, or deep learning for metabolomics. Keep exploring, and happy analyzing! üòÑ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
