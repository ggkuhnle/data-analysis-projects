{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06838f63",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "title: \"10.7_llm\"\n",
    "format:\n",
    "  html: default\n",
    "toc: false\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 10.7 • Large Language Models (LLMs): Mechanics, Limits, and Reliable Use\n",
    "\n",
    "LLMs are **next-token predictors** trained on large corpora. They *feel* intelligent because language encodes vast world structure. But the objective is not ‘truth’, it is **likelihood**.\n",
    "\n",
    "### Roadmap\n",
    "1. Tokenisation and embeddings (how text becomes vectors)\n",
    "2. Self-attention maths with shapes and masking\n",
    "3. Training objective, sampling strategies, temperature, top-k/p\n",
    "4. Why hallucinations happen; how RAG helps\n",
    "5. Prompt design, schema-constrained outputs, and evaluation\n",
    "6. Fine-tuning vs PEFT/LoRA vs RAG: when to use what\n",
    "\n",
    "We build a **count-based bigram model** (transparent baseline) and a **NumPy attention demo** you can inspect token-by-token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "np.random.seed(11088)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1) Tokenisation: from text to IDs\n",
    "Real LLMs use **BPE**/**SentencePiece** subword tokenisers. We’ll use a trivial whitespace tokenizer for transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    " \"patient reports chest pain and shortness of breath\",\n",
    " \"no chest pain today patient feels better\",\n",
    " \"shortness of breath worsened during exercise\",\n",
    " \"patient denies pain but notes dizziness\",\n",
    " \"exercise improved breath control patient better\"\n",
    "]\n",
    "tokens = [w for line in corpus for w in line.split()]\n",
    "vocab = sorted(set(tokens)); V=len(vocab)\n",
    "stoi={w:i for i,w in enumerate(vocab)}; itos={i:w for w,i in stoi.items()}\n",
    "ids = [stoi[w] for w in tokens]\n",
    "V, list(vocab)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2) Bigram language model (counts → probabilities)\n",
    "This is the simplest LM: \\(P(w_t|w_{t-1})\\) estimated by counts with smoothing. It proves that **fluency appears** even without neural nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counts = np.zeros((V,V), dtype=np.int64)\n",
    "for a,b in zip(ids, ids[1:]): counts[a,b]+=1\n",
    "probs = (counts + 1) / (counts.sum(axis=1, keepdims=True) + V)  # add-one smoothing\n",
    "\n",
    "def sample_bigram(prompt, n=10):\n",
    "    words = prompt.split(); cur = stoi.get(words[-1], None)\n",
    "    if cur is None: return prompt\n",
    "    out = words.copy()\n",
    "    for _ in range(n):\n",
    "        nxt = np.random.choice(V, p=probs[cur])\n",
    "        out.append(itos[nxt]); cur=nxt\n",
    "    return ' '.join(out)\n",
    "\n",
    "sample_bigram('patient', 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3) Self-attention: Query/Key/Value with a causal mask\n",
    "For a sequence matrix \\(X\\) (T×d), the model learns three projections:\n",
    "- \\(Q = X W_Q\\), \\(K = X W_K\\), \\(V = X W_V\\)\n",
    "- Attention weights \\(A = \\text{softmax}(QK^\\top/\\sqrt{d_k} + \\text{mask})\\)\n",
    "- Output \\(Z = AV\\)\n",
    "\n",
    "**Causal mask** ensures token *t* cannot attend to future tokens (>t)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = x - x.max(axis=-1, keepdims=True)\n",
    "    return np.exp(x)/np.exp(x).sum(axis=-1, keepdims=True)\n",
    "\n",
    "d_model=12; d_k=12\n",
    "E = np.random.normal(0,0.4,(V,d_model))  # token embeddings\n",
    "Wq = np.random.normal(0,0.3,(d_model,d_k))\n",
    "Wk = np.random.normal(0,0.3,(d_model,d_k))\n",
    "Wv = np.random.normal(0,0.3,(d_model,d_k))\n",
    "\n",
    "seq = \"patient reports chest pain\".split()\n",
    "X = E[[stoi[w] for w in seq]]  # T x d_model\n",
    "Q = X @ Wq; K = X @ Wk; Vv = X @ Wv\n",
    "scores = (Q @ K.T) / np.sqrt(d_k)  # T x T\n",
    "T = scores.shape[0]\n",
    "mask = np.triu(np.ones((T,T)), k=1) * 1e9  # large negative\n",
    "attn = softmax(scores - mask)\n",
    "Z = attn @ Vv\n",
    "pd.DataFrame(attn, index=seq, columns=seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Inspect the matrix: row *t* shows which earlier tokens the model ‘looks at’ when producing the representation for position *t*.\n",
    "\n",
    "### Positional encodings (why order matters)\n",
    "Because attention is permutation-invariant, models add sinusoidal or learned positional embeddings to encode token order.\n",
    "\n",
    "## 4) Training objective & sampling strategies\n",
    "**Objective**: minimise cross-entropy for next-token prediction.\n",
    "- Teacher forcing during training; autoregressive at inference.\n",
    "\n",
    "**Sampling**:\n",
    "- Greedy (argmax): deterministic but brittle.\n",
    "- Temperature: divides logits; T<1 = conservative, T>1 = diverse.\n",
    "- Top-k: restrict to k highest-probability tokens.\n",
    "- Nucleus (top-p): restrict to smallest set with cumulative prob p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_from_logits(logits, temperature=1.0, top_k=None, top_p=None):\n",
    "    z = logits / max(temperature, 1e-6)\n",
    "    # top-k\n",
    "    if top_k is not None:\n",
    "        idx = np.argpartition(z, -top_k)[-top_k:]\n",
    "        mask = np.full_like(z, -1e9); mask[idx]=z[idx]; z=mask\n",
    "    # top-p (nucleus)\n",
    "    if top_p is not None:\n",
    "        order = np.argsort(-z)\n",
    "        probs = softmax(z)\n",
    "        cum = np.cumsum(probs[order])\n",
    "        keep = order[cum<=top_p]\n",
    "        if len(keep)==0: keep=order[:1]\n",
    "        mask = np.full_like(z, -1e9); mask[keep]=z[keep]; z=mask\n",
    "    p = softmax(z)\n",
    "    return np.random.choice(len(z), p=p)\n",
    "\n",
    "# Demo with bigram probs as pseudo-logits\n",
    "def demo_sampling(start='patient', steps=10, temperature=0.8, top_k=5):\n",
    "    cur = stoi.get(start, 0); out=[start]\n",
    "    for _ in range(steps):\n",
    "        logits = np.log(probs[cur] + 1e-12)\n",
    "        nxt = sample_from_logits(logits, temperature=temperature, top_k=top_k)\n",
    "        out.append(itos[nxt]); cur=nxt\n",
    "    return ' '.join(out)\n",
    "\n",
    "demo_sampling('patient', 12, temperature=0.8, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5) Why hallucinations happen — and how to mitigate\n",
    "- Objective ≠ truth: it’s likelihood. The model will confidently produce plausible falsehoods when training data patterns point that way.\n",
    "- **Mitigations**:\n",
    "  - Retrieval-Augmented Generation (**RAG**): fetch relevant passages; condition the prompt.\n",
    "  - Schema-constrained outputs: ask for JSON with required fields; validate programmatically.\n",
    "  - Calibrate expectations: use confidence cues (self-consistency, citations), or instruct refusal when uncertain.\n",
    "\n",
    "### RAG architecture (conceptual)\n",
    "1) Ingest docs → chunk → embed → vector store\n",
    "2) At query time: embed query → retrieve top-k chunks\n",
    "3) Compose prompt: system + user + retrieved context\n",
    "4) Generate; optionally cite chunk IDs.\n",
    "\n",
    "**Exercise 1**: Draft a retrieval prompt template for summarising a nutrition RCT; include strict output schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6) Prompt design and guardrails\n",
    "- Give **role**, **task**, **constraints**, **examples** (few-shot), **format**.\n",
    "- Ask for chain-of-thought *only if you need it for pedagogy*; otherwise request **concise reasoning** to reduce verbosity.\n",
    "- For grading tasks, supply a rubric and require JSON with pass/fail + justifications.\n",
    "\n",
    "**Exercise 2**: Write two prompts for the same task (extract trial arms from a CONSORT abstract): one free-text, one JSON-schema; compare failure modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7) Fine-tuning vs PEFT/LoRA vs RAG\n",
    "- **Prompting**: fastest; no data or training; limited to model’s knowledge.\n",
    "- **RAG**: best first step to ground responses on your corpus; cheap to update.\n",
    "- **Full fine-tuning**: costly; risk of forgetting; requires eval harness.\n",
    "- **PEFT/LoRA**: parameter-efficient; adapt capabilities with fewer weights.\n",
    "\n",
    "**Rule of thumb**: If the task is **formatting/grounding**, use RAG + prompts. If it’s **style or domain nuance**, consider PEFT. For **new capabilities**, full FT (plus careful eval)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8) Evaluating LLM outputs (you must measure)\n",
    "- **Automatic**: exact-match / ROUGE for deterministic tasks; BLEU for translation; factuality checks against a gold knowledge base.\n",
    "- **Model-graded**: use a strong judge with a rubric; spot-check with humans.\n",
    "- **Programmatic**: JSON schema validation, unit tests for tool-use traces.\n",
    "- **Human**: double-blind rating for helpfulness, honesty, and safety.\n",
    "\n",
    "**Exercise 3**: Design an eval set of 20 nutrition Qs with gold answers + citations. Define pass criteria (e.g., exact cite present, no hallucinated trial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9) Safety and responsible use\n",
    "- Privacy: never paste secrets; mask identifiers during logging.\n",
    "- Bias: measure subgroup error rates; mitigate with counter-prompts or curated retrieval.\n",
    "- Transparency: state limits and sources; provide citations where possible.\n",
    "- Fallbacks: if retrieval empty or confidence low, return a **safe refusal** or ask for clarification.\n",
    "\n",
    "## Takeaways\n",
    "- LLMs are **likelihood machines** with impressive priors; treat outputs as **claims to verify**.\n",
    "- Retrieval and schema constraints turn them from storytellers into **tools**.\n",
    "- Evaluation and guardrails are non-negotiable for responsible deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
