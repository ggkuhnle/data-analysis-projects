{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# ðŸ”„ 3.4 Data Transformation\n",
        "\n",
        "In this notebook youâ€™ll learn how to **transform** data so itâ€™s easy to analyse and model. Weâ€™ll work with `hippo_nutrients.csv` and cover filtering, grouping, reshaping (wide/long), pivoting, and numeric transformations such as **log** and **z-score** standardisation.\n",
        "\n",
        "## ðŸŽ¯ Objectives\n",
        "- Filter rows and select columns with expressive, reproducible code.\n",
        "- Group and aggregate (mean, median, multiple aggregations).\n",
        "- Reshape between **wide** and **long** formats (melt / pivot).\n",
        "- Pivot-tabulate values for quick comparisons.\n",
        "- Create derived variables and apply **log**, **z-score**, **minâ€“max** scaling.\n",
        "- (Bonus) Group-wise standardisation and rolling transforms over time.\n",
        "\n",
        "## ðŸ“Œ Context\n",
        "Transformation is the ladder from raw data to insight. In nutrition datasets, youâ€™ll often transform by **group** (e.g., nutrient, sex, year) and reshape between **wide** (one row per ID with many columns) and **long** (one row per measurement).\n",
        "\n",
        "> **Wide vs Long**\n",
        "- **Wide**: Each variable has its own column (e.g., `Iron`, `Calcium`, `Vitamin_D` columns). Good for human reading, some stats.\n",
        "- **Long (tidy)**: One row per observation; variables encoded in *columns*, not headers (e.g., a `Nutrient` column and a `Value` column). Preferred for plotting, grouping, modelling.\n",
        "\n",
        "<details><summary>Fun Fact</summary>\n",
        "Transforming data is like a hippo rearranging its snacks â€” same nutrients, better view! ðŸ¦›\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "colab_setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup for Google Colab: Fetch datasets automatically or manually",
        "import os",
        "from google.colab import files",
        "",
        "MODULE = '03_data_handling'",
        "DATASET = 'hippo_nutrients.csv'",
        "BASE_PATH = '/content/data-analysis-projects'",
        "MODULE_PATH = os.path.join(BASE_PATH, 'notebooks', MODULE)",
        "DATASET_PATH = os.path.join('data', DATASET)",
        "",
        "try:",
        "    print('Attempting to clone repository...')",
        "    if not os.path.exists(BASE_PATH):",
        "        !git clone https://github.com/ggkuhnle/data-analysis-projects.git",
        "    print('Setting working directory...')",
        "    os.chdir(MODULE_PATH)",
        "    if os.path.exists(DATASET_PATH):",
        "        print(f'Dataset found: {DATASET_PATH} âœ…')",
        "    else:",
        "        raise FileNotFoundError('Dataset missing after clone.')",
        "except Exception as e:",
        "    print(f'Cloning failed: {e}')",
        "    print('Falling back to manual upload...')",
        "    os.makedirs('data', exist_ok=True)",
        "    uploaded = files.upload()",
        "    if DATASET in uploaded:",
        "        with open(DATASET_PATH, 'wb') as f:",
        "            f.write(uploaded[DATASET])",
        "        print(f'Successfully uploaded {DATASET} âœ…')",
        "    else:",
        "        raise FileNotFoundError(f'Upload failed. Please upload {DATASET}.')",
        "",
        "%pip install -q pandas numpy",
        "import pandas as pd, numpy as np",
        "pd.set_option('display.max_columns', 30)",
        "print('Python environment ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load_data",
      "metadata": {},
      "source": [
        "## 1) Load and Inspect\n",
        "Letâ€™s load `hippo_nutrients.csv` and inspect the first rows and data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/hippo_nutrients.csv')",
        "print('Shape:', df.shape)",
        "print('Dtypes:\\n', df.dtypes)",
        "display(df.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "filter_select",
      "metadata": {},
      "source": [
        "## 2) Filtering Rows and Selecting Columns\n",
        "Use **boolean masks** to keep the rows you want, and bracket notation to select columns. This is the bread-and-butter of transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "filter_select_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: female hippos, Iron only, years â‰¥ 2024",
        "mask = (df['Sex'] == 'F') & (df['Nutrient'] == 'Iron') & (df['Year'] >= 2024)",
        "cols = ['ID', 'Year', 'Nutrient', 'Value', 'Age', 'Sex']",
        "df_female_iron = df.loc[mask, cols].sort_values(['ID', 'Year']).reset_index(drop=True)",
        "display(df_female_iron.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "groupby",
      "metadata": {},
      "source": [
        "## 3) Grouping and Aggregation\n",
        "Summarise data by groups (e.g., nutrient, sex, or year). You can apply single or multiple aggregations.\n",
        "\n",
        "### Common patterns\n",
        "- `df.groupby('Nutrient')['Value'].mean()`\n",
        "- `df.groupby(['Nutrient','Sex']).agg({'Value': ['mean','median','count']})`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "groupby_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mean Value by Nutrient",
        "mean_by_nutrient = df.groupby('Nutrient')['Value'].mean().sort_values(ascending=False)",
        "display(mean_by_nutrient)",
        "",
        "# Multiple aggregations by Nutrient and Sex",
        "agg_ns = (",
        "    df.groupby(['Nutrient','Sex'])",
        "      .agg(Mean_Value=('Value','mean'),",
        "           Median_Value=('Value','median'),",
        "           N=('Value','size'))",
        "      .reset_index()",
        ")",
        "display(agg_ns.head(8))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wide_long_intro",
      "metadata": {},
      "source": [
        "## 4) Reshaping Between Wide and Long\n",
        "Many real datasets arrive in **wide** form; most analyses and plots like **long** (tidy) form.\n",
        "\n",
        "### Long (tidy) â†’ one row per observation\n",
        "- Columns: `ID`, `Year`, `Age`, `Sex`, `Nutrient`, `Value`\n",
        "\n",
        "### Wide â†’ columns are variables (e.g., one column per nutrient)\n",
        "- Columns: `ID`, `Year`, `Age`, `Sex`, `Iron`, `Calcium`, `Vitamin_D`, ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wide_from_long_explain",
      "metadata": {},
      "source": [
        "### 4.1 Long â†’ Wide (pivot wider)\n",
        "Use `pivot_table` with `index` = identifier columns and `columns` = the variable you want as new columns (`Nutrient`). Values come from `Value`.\n",
        "\n",
        "- Pros: human-readable, one row per subject-year.\n",
        "- Cons: harder to group/plot by Nutrient unless you re-melt later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pivot_wider",
      "metadata": {},
      "outputs": [],
      "source": [
        "id_cols = ['ID','Year','Age','Sex']",
        "wide = df.pivot_table(index=id_cols, columns='Nutrient', values='Value', aggfunc='mean')",
        "wide = wide.reset_index()  # move index back to columns",
        "display(wide.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "long_from_wide_explain",
      "metadata": {},
      "source": [
        "### 4.2 Wide â†’ Long (melt)\n",
        "Use `melt` to gather nutrient columns back into two columns: `Nutrient` (variable name) and `Value` (measurement). This is the canonical *tidy* structure for plotting, modelling, and faceting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "melt_long",
      "metadata": {},
      "outputs": [],
      "source": [
        "value_vars = [c for c in wide.columns if c not in id_cols]  # all nutrient columns",
        "long_again = wide.melt(id_vars=id_cols, value_vars=value_vars,",
        "                      var_name='Nutrient', value_name='Value')",
        "display(long_again.head(6))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pivot_tables",
      "metadata": {},
      "source": [
        "## 5) Pivot Tables for Comparisons\n",
        "Quickly cross-tabulate means by two dimensions (e.g., `Nutrient` Ã— `Year`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pivot_table_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "pt = df.pivot_table(values='Value', index='Nutrient', columns='Year', aggfunc='mean')",
        "display(pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "derived_vars",
      "metadata": {},
      "source": [
        "## 6) Derived Variables and Numeric Transformations\n",
        "Real analyses often need **transformed** variables to meet model assumptions or to compare across scales.\n",
        "\n",
        "### 6.1 Log transform\n",
        "Use when data are **right-skewed** (e.g., highly variable intakes). Add a small `Îµ` to avoid `log(0)`.\n",
        "\n",
        "### 6.2 z-score standardisation\n",
        "Convert values to standard units: `(x - mean) / std`. Helpful when combining variables on different scales.\n",
        "\n",
        "### 6.3 Minâ€“max scaling (0â€“1)\n",
        "Maps the minimum to 0 and maximum to 1. Useful for comparability in dashboards.\n",
        "\n",
        "### 6.4 Group-wise transforms\n",
        "Standardise **within** groups (e.g., within each `Nutrient`) so each nutrientâ€™s distribution is centred and scaled separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "numeric_transforms",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_trans = df.copy()",
        "",
        "# 6.1 Log transform: add epsilon to avoid log(0) if present",
        "eps = 1e-6",
        "df_trans['Value_log'] = np.log(df_trans['Value'] + eps)",
        "",
        "# 6.2 Global z-score",
        "mu = df_trans['Value'].mean()",
        "sd = df_trans['Value'].std(ddof=0)",
        "df_trans['Value_z'] = (df_trans['Value'] - mu) / (sd if sd else 1.0)",
        "",
        "# 6.3 Minâ€“max scaling",
        "vmin, vmax = df_trans['Value'].min(), df_trans['Value'].max()",
        "rng = vmax - vmin if vmax > vmin else 1.0",
        "df_trans['Value_minmax'] = (df_trans['Value'] - vmin) / rng",
        "",
        "# 6.4 Group-wise z-score by Nutrient",
        "df_trans['Value_z_by_nutrient'] = (",
        "    df_trans.groupby('Nutrient')['Value']",
        "           .transform(lambda s: (s - s.mean()) / (s.std(ddof=0) if s.std(ddof=0) else 1.0))",
        ")",
        "",
        "display(df_trans.head(8))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rolling",
      "metadata": {},
      "source": [
        "## 7) (Bonus) Time-based and Rolling Transforms\n",
        "For longitudinal data (e.g., the same `ID` measured across `Year`), we can compute rolling means or deltas. This is illustrative if your dataset has multiple years per ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rolling_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_roll = (",
        "    df.sort_values(['ID','Nutrient','Year'])",
        "      .groupby(['ID','Nutrient'], as_index=False)",
        "      .apply(lambda g: g.assign(",
        "          Value_roll_mean=g['Value'].rolling(window=2, min_periods=1).mean(),",
        "          Value_delta=g['Value'].diff()",
        "      ))",
        ")",
        "display(df_roll.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exercise_1",
      "metadata": {},
      "source": [
        "## ðŸ§ª Exercise 1 â€” Grouped Summary\n",
        "Filter to `Nutrient == 'Vitamin_D'`, then compute **median** `Value` by `Sex` and `Year`. Present the result sorted by `Year` and then `Sex`.\n",
        "\n",
        "**Hints**:\n",
        "- Filter: `df[df['Nutrient'] == 'Vitamin_D']`\n",
        "- Group and aggregate: `.groupby(['Sex','Year'])['Value'].median().reset_index()`\n",
        "- Sort: `.sort_values(['Year','Sex'])`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exercise_1_answer_here",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exercise_2",
      "metadata": {},
      "source": [
        "## ðŸ§ª Exercise 2 â€” Wide â†” Long\n",
        "1) Create a **wide** table with index `[ID, Year]` and columns as nutrients (values = mean `Value`).  \n",
        "2) Convert that wide table **back to long** using `melt` with `Nutrient` and `Value` columns.\n",
        "\n",
        "**Hints**:\n",
        "- `pivot_table(index=['ID','Year'], columns='Nutrient', values='Value', aggfunc='mean')`\n",
        "- `melt(id_vars=['ID','Year'], var_name='Nutrient', value_name='Value')`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exercise_2_answer_here",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exercise_3",
      "metadata": {},
      "source": [
        "## ðŸ§ª Exercise 3 â€” Transformations for Modelling\n",
        "Create a dataframe with these new columns on the **Iron** subset only:\n",
        "- `Value_log_iron` â€” log-transformed value with epsilon.\n",
        "- `Value_z_iron_by_sex` â€” z-score **within Sex**.\n",
        "- `Value_minmax_iron_by_year` â€” minâ€“max scaling **within Year**.\n",
        "\n",
        "**Hints**:\n",
        "- Filter: `(df['Nutrient'] == 'Iron')`\n",
        "- Group-wise transforms with `.groupby(...).transform(...)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exercise_3_answer_here",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wrap",
      "metadata": {},
      "source": [
        "## âœ… Conclusion & Next Steps\n",
        "Youâ€™ve practised the core transformation tools:\n",
        "- Filtering and selecting for clear subsets.\n",
        "- Grouping and aggregating for summaries.\n",
        "- Reshaping between **wide** and **long** for tidy analysis.\n",
        "- Pivot tables for quick comparisons.\n",
        "- Numeric transformations: **log**, **z-score**, **minâ€“max**, and **group-wise** standardisation.\n",
        "\n",
        "ðŸ‘‰ Next: **3.5 Data Aggregation** â€” robust summaries, grouped statistics, and combining pipelines for analysis-ready datasets.\n",
        "\n",
        "**Resources**:\n",
        "- Pandas GroupBy: https://pandas.pydata.org/docs/user_guide/groupby.html\n",
        "- Reshaping (melt/pivot): https://pandas.pydata.org/docs/user_guide/reshaping.html\n",
        "- Working with dtypes: https://pandas.pydata.org/docs/user_guide/basics.html#basics-dtypes"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

