{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "title",
      "metadata": {},
      "source": [
        "# üìù 6.2 Text Analysis for Qualitative Research\n",
        "\n",
        "We‚Äôll **prepare** text for human-led coding (cleaning, tokenisation, light structure) and add **small helper summaries** (frequencies, n-grams) that support‚Äînot replace‚Äîinterpretation.\n",
        "\n",
        "This notebook keeps the qualitative lens front-and-centre while giving you just enough NLP to work efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "objectives",
      "metadata": {},
      "source": [
        "## üéØ Objectives\n",
        "- Clean and tokenise open-ended responses with NLTK.\n",
        "- Lemmatise, remove stopwords/punctuation, handle case.\n",
        "- Build **n-grams** (bigrams, trigrams) to surface phrases.\n",
        "- Optional: POS tags and (careful) sentiment as exploratory aides.\n",
        "- Export a tidy table ready for **manual coding** or 6.3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q pandas nltk matplotlib seaborn scikit-learn wordcloud",
        "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns",
        "import nltk",
        "from nltk.corpus import stopwords",
        "from nltk.tokenize import word_tokenize",
        "from nltk.stem import WordNetLemmatizer",
        "from collections import Counter",
        "from wordcloud import WordCloud",
        "sns.set_theme()",
        "",
        "# NLTK data (robust to version differences)",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "except: pass\n",
        "try:\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "except: pass\n",
        "nltk.download('stopwords', quiet=True)",
        "nltk.download('wordnet', quiet=True)",
        "try:\n",
        "    nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "except:\n",
        "    nltk.download('averaged_perceptron_tagger', quiet=True)",
        "try:\n",
        "    nltk.download('vader_lexicon', quiet=True)\n",
        "except: pass",
        "print('Text/NLP environment ready.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path",
        "txt = Path('data')/'food_preferences.txt'",
        "responses = [r.strip() for r in txt.read_text(encoding='utf-8').splitlines() if r.strip()]",
        "df = pd.DataFrame({'response_id': range(1, len(responses)+1), 'text': responses})",
        "print('N responses:', len(df))",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "preproc",
      "metadata": {},
      "source": [
        "## üßº Preprocessing pipeline\n",
        "We‚Äôll lowercase, tokenise, remove stopwords/punctuation, and **lemmatise** (carrots‚Üícarrot). This **supports** coding by removing noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pipe",
      "metadata": {},
      "outputs": [],
      "source": [
        "stop = set(stopwords.words('english')).union({'hippo', 'h1','h2','h3'})",
        "lem = WordNetLemmatizer()",
        "",
        "def clean_tokens(text: str):",
        "    words = word_tokenize(text.lower())",
        "    words = [w for w in words if w.isalpha()]  # drop punctuation/numbers",
        "    words = [w for w in words if w not in stop]",
        "    words = [lem.lemmatize(w) for w in words]",
        "    return words",
        "",
        "df['tokens'] = df['text'].apply(clean_tokens)",
        "df[['response_id','text','tokens']].head(6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "freq",
      "metadata": {},
      "source": [
        "## üìä Frequencies & word cloud (orientation only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "freq_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "all_tokens = [t for row in df['tokens'] for t in row]",
        "freq = Counter(all_tokens).most_common(15)",
        "pd.DataFrame(freq, columns=['word','count'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wordcloud",
      "metadata": {},
      "outputs": [],
      "source": [
        "wc = WordCloud(width=800, height=400, background_color='white').generate(' '.join(all_tokens))",
        "plt.figure(figsize=(10,4)); plt.imshow(wc); plt.axis('off'); plt.title('Word Cloud'); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ngrams",
      "metadata": {},
      "source": [
        "## üîó N-grams (bigrams & trigrams)\n",
        "Short phrases can reveal food pairings (e.g., *fresh fruit*, *crunchy carrot*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ngram_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.util import ngrams",
        "",
        "def ngram_counts(tokens_list, n=2, top=15):",
        "    ng = Counter()",
        "    for toks in tokens_list:",
        "        ng.update(ngrams(toks, n))",
        "    return pd.DataFrame(ng.most_common(top), columns=[f'{n}-gram','count'])",
        "",
        "bigrams = ngram_counts(df['tokens'], n=2, top=15)",
        "trigrams = ngram_counts(df['tokens'], n=3, top=10)",
        "display(bigrams); display(trigrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pos_sent",
      "metadata": {},
      "source": [
        "## üß™ Optional aides: POS tags & VADER sentiment\n",
        "Use sparingly; these are **exploratory hints**, not findings. Sentiment can be noisy in domain language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pos",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:",
        "    from nltk import pos_tag",
        "    df['pos'] = df['tokens'].apply(pos_tag)",
        "    df[['response_id','pos']].head(4)",
        "except Exception as e:",
        "    print('POS tagging unavailable:', e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vader",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:",
        "    from nltk.sentiment import SentimentIntensityAnalyzer",
        "    sia = SentimentIntensityAnalyzer()",
        "    df['sentiment'] = df['text'].apply(lambda x: sia.polarity_scores(x)['compound'])",
        "    sns.histplot(df['sentiment']); plt.title('Sentiment (VADER) ‚Äî exploratory only'); plt.show()",
        "except Exception as e:",
        "    print('VADER unavailable:', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "export",
      "metadata": {},
      "source": [
        "## üì§ Export a coding-ready table\n",
        "We create a simple structure that supports **manual coding** (e.g., in Excel/Sheets or in 6.3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "export_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "out = df[['response_id','text','tokens']].copy()",
        "out['initial_code'] = ''  # analyst will fill codes",
        "out['notes'] = ''         # memo/comments",
        "out_path = 'qual_coding_sheet.csv'",
        "out.to_csv(out_path, index=False)",
        "print('Wrote:', out_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exercises",
      "metadata": {},
      "source": [
        "## üß© Exercises\n",
        "1) **Stoplist tuning**: add domain-specific stopwords (e.g., *like, really, very*)‚Äîhow do top words change?\n",
        "2) **Phrase mining**: examine bigrams containing *fruit* or *carrot*; collect example quotes.\n",
        "3) **Coding sheet**: add 2‚Äì4 provisional **initial codes** per 10 responses (keep them short & action-oriented)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wrap",
      "metadata": {},
      "source": [
        "## ‚úÖ Conclusion\n",
        "You prepared text for analysis and produced a coding-ready table. Next: formal **coding & thematic analysis** with reliability checks (6.3).\n",
        "\n",
        "<details><summary>More</summary>\n",
        "- NLTK docs (tokenisation, POS, stopwords)\n",
        "- Practical theming workflows\n",
        "</details>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
    "language_info": {"name": "python", "version": "3.9"}
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
