{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75df614e",
   "metadata": {},
   "source": [
    "# 10.7 â€¢ LLMs in Practice (No-Paid-API Edition)\n",
    "\n",
    "This notebook gives you a **hands-on** way to use LLMs without paying for an API:\n",
    "- **Local model** with `transformers` (e.g. `distilgpt2`) â€” works offline after first download.\n",
    "- **Optional free-tier API** via **Hugging Face Inference** (needs a free token).\n",
    "- A tiny, transparent **RAG demo** (retrieve â†’ augment â†’ generate).\n",
    "- **Schema-constrained outputs** checked with Python validation.\n",
    "\n",
    "Hippo-flavoured prompts included. ðŸ¦›"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66eab35",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "> The first run will download small models (~100â€“200 MB). This is normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install transformers==4.44.2 sentence-transformers==3.0.1 torch\n",
    "\n",
    "import os, json, math, numpy as np\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90dbbf5",
   "metadata": {},
   "source": [
    "## 1) Local small model with `transformers` (no API key)\n",
    "\n",
    "We'll load a small open model (`distilgpt2`) and generate text with different sampling settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94963e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"distilgpt2\"  # small and quick to try\n",
    "\n",
    "# Device & seed\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "_ = torch.manual_seed(11088)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# GPT-2 has no pad token by default; use eos as pad\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device).eval()\n",
    "\n",
    "def generate_local(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 60,\n",
    "    temperature: float = 0.8,\n",
    "    top_k: int = 50,\n",
    "    top_p: float = 0.0,  # set >0 to enable nucleus sampling\n",
    "):\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **enc,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p if top_p > 0 else None,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generate_local(\"Hippo keeper log: Today the hippo felt\", max_new_tokens=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fbd908",
   "metadata": {},
   "source": [
    "### Experiment: sampling choices\n",
    "\n",
    "- Lower temperature (e.g., 0.6) â†’ safer, more repetitive.\n",
    "- Higher temperature (e.g., 1.2) â†’ more diverse, riskier.\n",
    "- `top_k` limits to the k most likely tokens at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76e2e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in [0.6, 0.9, 1.2]:\n",
    "    print(f\"\\n--- temperature={temp} ---\")\n",
    "    print(generate_local(\"Hippo diet note: The riverbank forage was\", max_new_tokens=40, temperature=temp, top_k=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9edb4bc",
   "metadata": {},
   "source": [
    "## 2) A tiny Retrieval-Augmented Generation (RAG) demo\n",
    "\n",
    "Weâ€™ll create a **mini corpus** of short notes (hippo care + nutrition), embed them with `sentence-transformers`, \n",
    "retrieve the top-3 most similar passages, and then **compose** a grounded prompt for the local model.\n",
    "\n",
    "> This demonstrates the workflow: **retrieve first, then generate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd757ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If sentence-transformers not installed, see install cell above\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "corpus = [\n",
    "    \"Hippos often graze at night; daytime wallowing helps thermoregulation.\",\n",
    "    \"Dental checks should examine tusk wear, gum inflammation, and food impaction.\",\n",
    "    \"Forage quality affects chewing time and saliva buffering, influencing dental health.\",\n",
    "    \"Sudden changes in water salinity can alter drinking behaviour and stress.\",\n",
    "    \"Keeper logs should include appetite, activity, and social interactions.\",\n",
    "]\n",
    "queries = [\n",
    "    \"What should we check during a hippo dental health round?\",\n",
    "]\n",
    "\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "E_corpus = embedder.encode(corpus, normalize_embeddings=True)\n",
    "E_query  = embedder.encode(queries, normalize_embeddings=True)\n",
    "\n",
    "def retrieve(query_vec, k=3):\n",
    "    # cosine similarity because embeddings are normalized\n",
    "    scores = E_corpus @ query_vec  # (N,d) @ (d,) -> (N,)\n",
    "    idx = np.argsort(-scores)[:k]\n",
    "    return [(corpus[i], float(scores[i])) for i in idx]\n",
    "\n",
    "top = retrieve(E_query[0], k=3)\n",
    "top"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c1743b",
   "metadata": {},
   "source": [
    "### Compose a grounded prompt and generate (local model)\n",
    "\n",
    "We build a prompt that **quotes** the retrieved passages and asks the local model for a **succinct, structured** answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6036d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_lines = \"\\n\".join([f\"- {t[0]}\" for t in top])\n",
    "grounded_prompt = f\"\"\"Use ONLY the following notes to answer the question. Quote specific checks.\n",
    "\n",
    "Notes:\n",
    "{context_lines}\n",
    "\n",
    "Question: List 3 essential checks for a hippo dental round in bullet points.\n",
    "Answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(grounded_prompt)\n",
    "print(\"\\n--- Generated (local model) ---\\n\")\n",
    "print(generate_local(grounded_prompt, max_new_tokens=80, temperature=0.7, top_k=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa5c36",
   "metadata": {},
   "source": [
    "## 3) Schema-constrained output (JSON) and validation\n",
    "\n",
    "For real systems, ask the model to output **JSON with required fields**, then validate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b61961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re\n",
    "\n",
    "json_prompt = (\n",
    "    \"Produce a JSON object with fields: 'checks' (list of 3 short strings), \"\n",
    "    \"'priority' (one of: 'low','medium','high').\\n\"\n",
    "    \"Use the notes above; if unclear, choose 'medium'.\\n\"\n",
    "    \"JSON only, no extra text.\\n\"\n",
    ")\n",
    "\n",
    "candidate = generate_local(\n",
    "    grounded_prompt + \"\\n\" + json_prompt,\n",
    "    max_new_tokens=120, temperature=0.7, top_k=50\n",
    ")\n",
    "\n",
    "# Extract the first {...} block (greedy to get a full object)\n",
    "match = re.search(r\"\\{[\\s\\S]*\\}\", candidate)\n",
    "parsed = None\n",
    "if match:\n",
    "    try:\n",
    "        parsed = json.loads(match.group(0))\n",
    "    except Exception as e:\n",
    "        parsed = {\"error\": f\"JSON parse failed: {e}\", \"raw\": candidate[:500]}\n",
    "else:\n",
    "    parsed = {\"error\": \"No JSON object found.\", \"raw\": candidate[:500]}\n",
    "\n",
    "def validate(payload: dict) -> dict:\n",
    "    if not isinstance(payload, dict):\n",
    "        return {\"ok\": \"false\", \"reason\": \"payload is not a JSON object\"}\n",
    "    if \"checks\" not in payload or not isinstance(payload[\"checks\"], list) or len(payload[\"checks\"]) != 3:\n",
    "        return {\"ok\": \"false\", \"reason\": \"checks must be a list of exactly 3 strings\"}\n",
    "    if \"priority\" not in payload or payload[\"priority\"] not in {\"low\",\"medium\",\"high\"}:\n",
    "        return {\"ok\": \"false\", \"reason\": \"priority must be one of: low, medium, high\"}\n",
    "    return {\"ok\": \"true\"}\n",
    "\n",
    "print(\"RAW CANDIDATE:\\n\", candidate[:500], \"\\n\")\n",
    "print(\"PARSED:\", parsed)\n",
    "print(\"VALIDATION:\", validate(parsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdd2b24",
   "metadata": {},
   "source": [
    "## 4) Summary\n",
    "\n",
    "- **Local models** (e.g., `distilgpt2`) let you teach LLM mechanics without paid APIs.\n",
    "- **Hugging Face Inference** provides a free-tier API with a token; good for demos.\n",
    "- **RAG** grounds answers in your corpus; show students how retrieval shapes outputs.\n",
    "- **JSON schemas** (even simple checks) harden systems against hallucinations.\n",
    "\n",
    "**Exercises**\n",
    "1. Swap `distilgpt2` for `microsoft/phi-2` or `TinyLlama/TinyLlama-1.1B-Chat-v1.0` (may need more RAM). Compare output quality and speed.\n",
    "2. Change the retrieval corpus to short **nutrition RCT abstracts**. Does the answer become more evidence-based?\n",
    "3. Extend validation to enforce that each `checks` item contains at least one **dental term** (`gum`, `tusk`, `impaction`, `inflammation`). Reject if not met."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Prepared for Gunter's ML/AI course"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
