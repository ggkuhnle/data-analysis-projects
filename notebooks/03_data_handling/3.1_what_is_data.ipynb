{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# ðŸ¦› 3.1 Data Types and Structures â€” *Wide vs Long (Tidy)*\n",
        "\n",
        "This notebook explores core tabular data layouts used in nutrition research, with a focus on **tidy data** and how to move between **wide** and **long** forms using pandas.\n",
        "\n",
        "**Objectives**:\n",
        "- Understand vectors, tables, and the difference between **wide** and **long** (tidy) data.\n",
        "- Use `pandas.pivot` (wide) and `pandas.melt` (long) appropriately.\n",
        "- Apply tidy principles to `hippo_nutrients.csv`.\n",
        "\n",
        "**Context**: Tidy data is critical for efficient analysis of nutrition datasets. ðŸ¦›\n",
        "\n",
        "<details><summary>Fun Fact</summary>\n",
        "Tidy data is like a hippoâ€™s lunch trayâ€”neat and ready to munch!\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "colab_setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup for Google Colab: Fetch datasets automatically or manually\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Define the module and dataset for this notebook\n",
        "MODULE = '03_data_handling'  # e.g., '01_infrastructure'\n",
        "DATASET = 'hippo_nutrients.csv'  # e.g., 'hippo_diets.csv'\n",
        "BASE_PATH = '/content/data-analysis-projects'\n",
        "MODULE_PATH = os.path.join(BASE_PATH, 'notebooks', MODULE)\n",
        "DATASET_PATH = os.path.join('data', DATASET)\n",
        "\n",
        "try:\n",
        "    print('Attempting to clone repository...')\n",
        "    if os.path.exists(BASE_PATH):\n",
        "        print('Repository already exists, skipping clone.')\n",
        "    else:\n",
        "        !git clone https://github.com/ggkuhnle/data-analysis-projects.git\n",
        "\n",
        "    # Debug: Print directory structure\n",
        "    print('Listing repository contents:')\n",
        "    !ls {BASE_PATH}\n",
        "    print('Listing notebooks directory contents:')\n",
        "    !ls {BASE_PATH}/notebooks\n",
        "\n",
        "    if not os.path.exists(MODULE_PATH):\n",
        "        raise FileNotFoundError(f'Module directory {MODULE_PATH} not found. Check the repository structure.')\n",
        "\n",
        "    os.chdir(MODULE_PATH)\n",
        "\n",
        "    if os.path.exists(DATASET_PATH):\n",
        "        print(f'Dataset found: {DATASET_PATH} âœ…')\n",
        "    else:\n",
        "        print(f'Error: Dataset {DATASET} not found after cloning.')\n",
        "        raise FileNotFoundError\n",
        "except Exception as e:\n",
        "    print(f'Cloning failed: {e}')\n",
        "    print('Falling back to manual upload option...')\n",
        "    os.makedirs('data', exist_ok=True)\n",
        "    uploaded = files.upload()\n",
        "    if DATASET in uploaded:\n",
        "        with open(DATASET_PATH, 'wb') as f:\n",
        "            f.write(uploaded[DATASET])\n",
        "        print(f'Successfully uploaded {DATASET} to {DATASET_PATH} âœ…')\n",
        "    else:\n",
        "        raise FileNotFoundError(f'Upload failed. Please ensure you uploaded {DATASET}.')\n",
        "\n",
        "%pip install pandas numpy -q\n",
        "print('Python environment ready.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('display.max_columns', 20)\n",
        "print('Data handling environment ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wide_long_concepts",
      "metadata": {},
      "source": [
        "## Wide vs Long (Tidy): the essential distinction\n",
        "\n",
        "**Long/tidy data** (what most modelling/plotting functions prefer):\n",
        "- One **row per observation**.\n",
        "- One **column per variable**.\n",
        "- Example here: each row gives (`ID`, `Nutrient`, `Year`, `Value`, `Age`, `Sex`).\n",
        "\n",
        "**Wide data** (often what you get from spreadsheets):\n",
        "- One row per **entity/time-point**, with **many columns** for different measures.\n",
        "- Example: one row per (`ID`, `Year`) and separate columns `Iron`, `Calcium`, `Zinc`, ...\n",
        "\n",
        "ðŸ‘‰ Use **`pivot`** to go from **long â†’ wide**.\n",
        "ðŸ‘‰ Use **`melt`** to go from **wide â†’ long**.\n",
        "\n",
        "The trick is to correctly identify:\n",
        "- **ID variables** (stay as columns and define the row): e.g., `ID`, `Year`, `Age`, `Sex`.\n",
        "- **Measured variables** (become columns in wide; become rows in long): e.g., nutrient names like `Iron`, `Calcium`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load_data",
      "metadata": {},
      "source": [
        "## Load and inspect\n",
        "Load `hippo_nutrients.csv` and inspect its structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/hippo_nutrients.csv')\n",
        "df.head(6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "is_tidy",
      "metadata": {},
      "source": [
        "### Is the dataset already tidy (long)?\n",
        "We check the columns and a few rows. If we have `Nutrient` **as a column of names** and `Value` **as a single measurement column**, the data is already **long/tidy**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "check_tidy",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Columns:', list(df.columns))\n",
        "print('\\nDistinct Nutrients:', df['Nutrient'].unique()[:5], '...')\n",
        "print('\\nSample:')\n",
        "display(df.sample(min(5, len(df))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pivot_wide",
      "metadata": {},
      "source": [
        "## Long â†’ Wide with `pivot`\n",
        "Our dataset is already long/tidy. To see the **wide** form (each nutrient as its own column), we can **pivot**:\n",
        "\n",
        "- **Index** (row identifiers): `['ID', 'Year', 'Age', 'Sex']`\n",
        "- **Columns** (new wide columns): `'Nutrient'`\n",
        "- **Values** (cells): `'Value'`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "do_pivot",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_wide = (\n",
        "    df.pivot(index=['ID','Year','Age','Sex'], columns='Nutrient', values='Value')\n",
        "      .reset_index()\n",
        ")\n",
        "df_wide.head(6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "melt_long",
      "metadata": {},
      "source": [
        "## Wide â†’ Long with `melt`\n",
        "If we receive data in **wide** form (e.g., columns `Iron`, `Calcium`, ...), we convert back to **long/tidy** with `melt`.\n",
        "\n",
        "Key idea: **`id_vars`** are the identifier columns to keep as-is; **`value_vars`** are the columns to unpivot into rows. Here, the value columns are the nutrient names (e.g., `'Iron'`, `'Calcium'`, ...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "do_melt",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Work from the wide table we just created\n",
        "id_cols = ['ID','Year','Age','Sex']\n",
        "value_cols = [c for c in df_wide.columns if c not in id_cols]\n",
        "\n",
        "df_long_again = df_wide.melt(\n",
        "    id_vars=id_cols,\n",
        "    value_vars=value_cols,\n",
        "    var_name='Nutrient',\n",
        "    value_name='Value'\n",
        ")\n",
        "df_long_again.sort_values(['ID','Year','Nutrient']).head(8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "why_melt_wrong_before",
      "metadata": {},
      "source": [
        "### Why the earlier `melt` was wrong\n",
        "If you call:\n",
        "\n",
        "```python\n",
        "df.melt(id_vars=['ID','Age','Sex'], var_name='Nutrient', value_name='value')\n",
        "```\n",
        "\n",
        "on the **already-long** `df`, pandas will try to unpivot *everything that isnâ€™t* in `id_vars` â€” i.e. it will stack the columns `['Nutrient','Year','Value']`. The new `'Nutrient'` column then contains the **former column names** (`'Nutrient'`, `'Year'`, `'Value'`), which is not what we want.\n",
        "\n",
        "âœ… **Rule of thumb**:\n",
        "- If your data already has a single measurement column (e.g. `'Value'`) and a variable name column (e.g. `'Nutrient'`), itâ€™s **already long** â€” you donâ€™t need `melt`.\n",
        "- Use `melt` only when you have separate **value columns** (like `Iron`, `Calcium`, â€¦) that you want to gather into rows."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exercise",
      "metadata": {},
      "source": [
        "## Exercise 1: Filter Tidy Data\n",
        "\n",
        "Filter the **long/tidy** data to show only iron intakes and describe the result in a Markdown cell.\n",
        "\n",
        "**Guidance**: Use either the original `df` (already tidy) or `df_long_again`:\n",
        "\n",
        "```python\n",
        "df_iron = df[df['Nutrient'] == 'Iron']\n",
        "df_iron.sort_values(['ID','Year']).head()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "your_answer",
      "metadata": {},
      "source": [
        "**Answer**:\n",
        "\n",
        "The filtered iron data shows..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wrap",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Youâ€™ve learned how to recognise and transform between **wide** (many value columns) and **long/tidy** (one measurement column + variable name column). Use `pivot` to widen and `melt` to tidy.\n",
        "\n",
        "**Resources**:\n",
        "- [Tidy Data Paper](https://vita.had.co.nz/papers/tidy-data.pdf)\n",
        "- [pandas `melt` docs](https://pandas.pydata.org/docs/reference/api/pandas.melt.html)\n",
        "- [pandas `pivot` docs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html)\n",
        "- Repository: [github.com/ggkuhnle/data-analysis-projects](https://github.com/ggkuhnle/data-analysis-projects)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

