{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# ðŸ§¹ 3.3 Data Cleaning â€” Principles and Practice\n",
        "\n",
        "Clean data is the foundation of valid inference. In nutrition research (NDNS-style surveys, trials, food logs), data often arrives **messy**: missing values, inconsistent formats, duplicates, and subtle logic errors. This notebook gives you a practical, principled workflow for cleaning data in pandas.\n",
        "\n",
        "---\n",
        "## ðŸŽ¯ Objectives\n",
        "By the end of this notebook you can:\n",
        "- Apply **principles of data cleaning**: make data *correct, consistent, complete, and documented*.\n",
        "- Handle **missing values** sensibly (inspect, decide, impute, or exclude).\n",
        "- Fix **poorly formatted values** (strings instead of numbers, units like `%`, decimal commas, stray symbols).\n",
        "- Detect and manage **duplicates** (exact, key-based, and near-duplicates).\n",
        "- Validate cleaned data against simple **rules**.\n",
        "\n",
        "---\n",
        "## ðŸ§­ Principles of Data Cleaning (Field-tested)\n",
        "1. **Profile first**: *Look* before you leap. Inspect shapes, column types, unique values, and obvious anomalies.\n",
        "2. **Make minimal, reversible changes**: Keep raw data intact; work on a copy. Store transformations in code, not by hand.\n",
        "3. **Be explicit about rules**: Write *why* you drop/impute/convert. Use comments and validation checks.\n",
        "4. **Prefer tidy structure**: One variable per column, one observation per row, one unit per table.\n",
        "5. **Treat missingness as information**: The pattern of missing values is a signal. Donâ€™t blindly fill everything.\n",
        "6. **Standardise representations**: Dates, decimals, units, categories (e.g., sex codes) should be uniform.\n",
        "7. **Idempotence**: Running the cleaning script twice should not change results further.\n",
        "8. **Document decisions**: Note assumptions (e.g., iron in mg/day), thresholds (plausibility bounds), and imputations used.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "colab_setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup for Google Colab: Fetch datasets automatically or manually",
        "import os",
        "from google.colab import files",
        "",
        "MODULE = '03_data_handling'",
        "DATASET = 'hippo_nutrients.csv'",
        "BASE_PATH = '/content/data-analysis-projects'",
        "MODULE_PATH = os.path.join(BASE_PATH, 'notebooks', MODULE)",
        "DATASET_PATH = os.path.join('data', DATASET)",
        "",
        "try:",
        "    if not os.path.exists(BASE_PATH):",
        "        !git clone https://github.com/ggkuhnle/data-analysis-projects.git",
        "    os.chdir(MODULE_PATH)",
        "    assert os.path.exists(DATASET_PATH)",
        "    print(f'Dataset found: {DATASET_PATH} âœ…')",
        "except Exception as e:",
        "    print(f'Automatic clone failed: {e}')",
        "    os.makedirs('data', exist_ok=True)",
        "    uploaded = files.upload()",
        "    if DATASET in uploaded:",
        "        with open(DATASET_PATH, 'wb') as f:",
        "            f.write(uploaded[DATASET])",
        "        print(f'Successfully uploaded {DATASET} âœ…')",
        "    else:",
        "        raise FileNotFoundError(f'Upload failed. Please ensure you uploaded {DATASET}.')",
        "",
        "%pip install pandas numpy -q",
        "import pandas as pd, numpy as np",
        "pd.set_option('display.max_columns', 30)",
        "print('Environment ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load_data",
      "metadata": {},
      "source": [
        "## 1) Load and Profile\n",
        "We start by loading the data and building a quick profile: shapes, dtypes, head/tail, and simple summaries. Profiling guides all subsequent cleaning steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "profile",
      "metadata": {},
      "outputs": [],
      "source": [
        "raw = pd.read_csv('data/hippo_nutrients.csv')",
        "df = raw.copy()  # work on a copy; keep raw intact",
        "",
        "print('Shape:', df.shape)",
        "print('\\nDtypes:')",
        "print(df.dtypes)",
        "",
        "print('\\nHead:')",
        "display(df.head(5))",
        "",
        "print('\\nBasic summary:')",
        "display(df.describe(include='all'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "missing_overview",
      "metadata": {},
      "source": [
        "## 2) Missing Values â€” Inspect before you impute\n",
        "Missing data can be **MCAR** (completely at random), **MAR** (at random given observed variables), or **MNAR** (not at random). Your strategy depends on the context.\n",
        "\n",
        "Typical steps:\n",
        "- Identify missingness overall and by variable.\n",
        "- Check missingness patterns by groups (e.g., by Nutrient, Year, Sex).\n",
        "- Decide: drop, impute (mean/median/group-wise), or model-based methods (later modules).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "missing_counts",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Missing counts per column:')",
        "print(df.isna().sum().sort_values(ascending=False))",
        "",
        "# Missingness by nutrient (for Value only)",
        "if 'Value' in df.columns and 'Nutrient' in df.columns:",
        "    miss_by_nutrient = df.groupby('Nutrient')['Value'].apply(lambda s: s.isna().mean()).sort_values(ascending=False)",
        "    print('\\nProportion missing in Value by Nutrient:')",
        "    display(miss_by_nutrient.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "missing_strategies",
      "metadata": {},
      "source": [
        "### Sensible Imputation Patterns (for teaching datasets)\n",
        "- **Numeric measurements (e.g., `Value`)**: often impute by **group** (by `Nutrient`, sometimes by `Nutrient`Ã—`Sex`), using **median** to reduce outlier influence.\n",
        "- **Categoricals (e.g., `Sex`)**: leave missing or impute with a new category like `'Unknown'` (document this!).\n",
        "- **IDs/keys**: missing IDs are usually fatal â€” investigate or drop those rows with a note.\n",
        "\n",
        "Below we demonstrate **group-wise median imputation** for `Value` and keep everything else untouched (for transparency)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "impute_value",
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'Value' in df.columns and 'Nutrient' in df.columns:",
        "    df['Value_imputed'] = df['Value']  # keep original and add an imputed version",
        "    df['Value_imputed_flag'] = df['Value'].isna()  # track which were imputed",
        "    df['Value_imputed'] = (",
        "        df.groupby('Nutrient')['Value_imputed']",
        "          .transform(lambda s: s.fillna(s.median()))",
        "    )",
        "",
        "    print('Imputed Value (group-median by Nutrient). Rows imputed:', int(df['Value_imputed_flag'].sum()))",
        "    display(df.loc[df['Value_imputed_flag']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "poor_formats_intro",
      "metadata": {},
      "source": [
        "## 3) Poorly Formatted Values â€” Standardise representations\n",
        "Common issues in nutrition datasets:\n",
        "- **Numbers stored as strings**: e.g., `'8.5'`, `' 8.5 '`, `'8,5'` (decimal comma), `'8.5 mg'`.\n",
        "- **Percentages**: e.g., `'12%'` instead of `0.12` or `12`.\n",
        "- **Thousands separators**: `'1,234.5'` or in some locales `'1.234,5'`.\n",
        "- **Category typos/casing**: `'female'`, `'FEMALE'`, `'F'` representing the same concept.\n",
        "\n",
        "Goal: convert to **consistent numeric types**, **consistent units**, and **canonical categories**. Always document unit assumptions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "decimal_commas",
      "metadata": {},
      "source": [
        "### Example: Decimal commas (German-style) â†’ decimal points\n",
        "If values arrive as strings with commas (e.g., `'8,5'`), convert them safely using `str.replace` + `pd.to_numeric`.\n",
        "\n",
        "**Note**: Do not blanket-replace commas if the column also uses commas as thousands separators. Decide the rule from the data context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decimal_commas_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo column with mixed formats (create if not present)",
        "if 'Value_raw' not in df.columns:",
        "    df['Value_raw'] = df['Value'].astype(str)",
        "    # inject a few messy examples for teaching (won't affect original Value)",
        "    ex_idx = df.sample(min(3, len(df)), random_state=1).index",
        "    df.loc[ex_idx, 'Value_raw'] = ['8,5', ' 12,0 ', '7,9 mg'][:len(ex_idx)]",
        "",
        "def to_numeric_decomma(s: pd.Series) -> pd.Series:",
        "    s = s.astype(str).str.strip()",
        "    # remove unit strings like 'mg' or 'kcal' (simple demo)",
        "    s = s.str.replace(r'[^0-9,.-]', '', regex=True)",
        "    # convert decimal comma to point (assumes comma == decimal separator here)",
        "    s = s.str.replace(',', '.', regex=False)",
        "    return pd.to_numeric(s, errors='coerce')",
        "",
        "df['Value_clean_decimal'] = to_numeric_decomma(df['Value_raw'])",
        "display(df[['Value_raw','Value_clean_decimal']].head(6))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "percentages",
      "metadata": {},
      "source": [
        "### Example: Percent strings â†’ numeric fractions or percents\n",
        "Decide on a convention:\n",
        "- Store **fractions** (0â€“1), or\n",
        "- Store **percentage points** (0â€“100).\n",
        "\n",
        "Below, we convert `'12%'` â†’ `12.0` (percentage points). Adjust as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "percentages_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "demo = pd.Series(['12%', ' 7.5 %', '0.5%', None, 'n/a'])",
        "pct = (demo.astype(str).str.strip()",
        "              .str.replace('%','', regex=False)",
        "              .str.replace(',','.', regex=False))",
        "pct_num = pd.to_numeric(pct, errors='coerce')  # percentage points",
        "display(pd.DataFrame({'raw': demo, 'pct_points': pct_num}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "thousands",
      "metadata": {},
      "source": [
        "### Example: Thousands separators\n",
        "Remove separators explicitly. Beware of locale mixes.\n",
        "\n",
        "- UK/US style: `1,234.5` â†’ remove commas â†’ `1234.5`.\n",
        "- DE style: `1.234,5` â†’ remove dots, replace comma with dot â†’ `1234.5`.\n",
        "\n",
        "Whenever possible, confirm the **intended locale** and **units** from metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thousands_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "us = pd.Series(['1,234.5', '2,000', '10,050.75'])",
        "us_clean = pd.to_numeric(us.str.replace(',', '', regex=False), errors='coerce')",
        "",
        "de = pd.Series(['1.234,5', '2.000', '10.050,75'])",
        "de_clean = pd.to_numeric(de.str.replace('.', '', regex=False).str.replace(',', '.', regex=False), errors='coerce')",
        "",
        "display(pd.DataFrame({'us_raw': us, 'us_clean': us_clean}))",
        "display(pd.DataFrame({'de_raw': de, 'de_clean': de_clean}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "categoricals",
      "metadata": {},
      "source": [
        "### Example: Canonical categories (e.g., Sex)\n",
        "Standardise category labels and document the mapping. Keep original in case you need to audit later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "categoricals_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'Sex' in df.columns:",
        "    df['Sex_raw'] = df['Sex'].astype(str)",
        "    sex_map = {",
        "        'f':'F', 'female':'F', 'FEMALE':'F', 'F':'F',",
        "        'm':'M', 'male':'M', 'MALE':'M', 'M':'M'",
        "    }",
        "    df['Sex_std'] = df['Sex_raw'].str.strip().str.lower().map(sex_map).fillna('Unknown')",
        "    display(df[['Sex_raw','Sex_std']].drop_duplicates().head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "duplicates_intro",
      "metadata": {},
      "source": [
        "## 4) Duplicates â€” Exact, key-based, and near-duplicates\n",
        "Duplicates can arise from repeated data entry, merges, or concatenations.\n",
        "\n",
        "### Types\n",
        "- **Exact duplicates**: Entire rows identical.\n",
        "- **Key-based duplicates**: Same (`ID`,`Nutrient`,`Year`) but different values.\n",
        "- **Near-duplicates**: Slightly different strings or values (e.g., whitespace, casing, rounding). Often require domain judgment.\n",
        "\n",
        "Strategy:\n",
        "1) Count exact duplicates.\n",
        "2) Define a **key** and check for multiple rows per key.\n",
        "3) Decide how to resolve (e.g., prefer non-missing, average within key, keep latest record, or flag for manual review)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duplicates_exact",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.1 Exact duplicates (full-row)",
        "exact_dupes = df.duplicated(keep=False)",
        "print('Exact duplicate rows (including originals):', int(exact_dupes.sum()))",
        "display(df[exact_dupes].head(6))",
        "",
        "# If you decide to drop exact duplicates:",
        "df_no_exact_dupes = df.drop_duplicates().copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duplicates_key",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.2 Key-based duplicates",
        "key = ['ID','Nutrient','Year']",
        "if all(k in df.columns for k in key):",
        "    counts = df.groupby(key).size().reset_index(name='n')",
        "    multi = counts[counts['n'] > 1]",
        "    print('Keys with >1 row:', len(multi))",
        "    display(multi.head(10))",
        "",
        "    # Inspect conflicts for a sample key (if present)",
        "    if not multi.empty:",
        "        sample_key = multi.iloc[0][key].to_dict()",
        "        print('Example conflicting key:', sample_key)",
        "        q = (df['ID'].eq(sample_key['ID']) &",
        "             df['Nutrient'].eq(sample_key['Nutrient']) &",
        "             df['Year'].eq(sample_key['Year']))",
        "        display(df.loc[q])",
        "",
        "    # Example resolution: within each key, keep the row with the least missingness in Value/Value_imputed",
        "    # (Alternatively: take mean, prefer non-missing, or most recent record if you have timestamps.)",
        "    def resolve_group(g):",
        "        # prefer non-missing Value; fall back to Value_imputed",
        "        g = g.copy()",
        "        score = g['Value'].notna().astype(int) + g.get('Value_imputed_flag', False).map({True:0, False:0})",
        "        # keep the first row with highest score",
        "        return g.loc[score.sort_values(ascending=False).index[0:1]]",
        "",
        "    df_resolved = (df.groupby(key, as_index=False, group_keys=False)",
        "                     .apply(resolve_group))",
        "    print('After key-based resolution, shape:', df_resolved.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "validation_rules",
      "metadata": {},
      "source": [
        "## 5) Validate with Simple Rules\n",
        "Codify assumptions as checks. Fail **loudly** when rules are broken:\n",
        "- **Type checks**: numeric columns should be numeric.\n",
        "- **Range checks**: plausible ranges (e.g., iron intake in mg/day is rarely negative or 10,000).\n",
        "- **Membership checks**: categories in known sets.\n",
        "\n",
        "These checks turn silent data issues into visible test failures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "run_checks",
      "metadata": {},
      "outputs": [],
      "source": [
        "problems = []",
        "",
        "# Type checks",
        "if 'Value_imputed' in df.columns:",
        "    if not np.issubdtype(df['Value_imputed'].dtype, np.number):",
        "        problems.append('Value_imputed is not numeric.')",
        "",
        "# Range checks (example plausible bounds, adjust to your domain)",
        "if 'Value_imputed' in df.columns:",
        "    out_of_range = ~df['Value_imputed'].between(0, 10000)  # generous upper bound",
        "    if out_of_range.any():",
        "        problems.append(f\"{int(out_of_range.sum())} rows have Value_imputed outside [0, 10000].\")",
        "",
        "# Membership checks",
        "if 'Sex_std' in df.columns:",
        "    allowed = {'M','F','Unknown'}",
        "    bad = ~df['Sex_std'].isin(allowed)",
        "    if bad.any():",
        "        problems.append(f\"{int(bad.sum())} rows have unexpected Sex_std labels.\")",
        "",
        "print('Validation summary:')",
        "if problems:",
        "    for p in problems:",
        "        print(' -', p)",
        "else:",
        "    print('All checks passed âœ…')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pipeline_export",
      "metadata": {},
      "source": [
        "## 6) Save the Cleaned Dataset (Optional)\n",
        "Export a clean, analysis-ready table. Keep both **raw** and **clean** versions under version control, and record cleaning decisions in `README` or the notebook header."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "save_clean",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose the resolved DataFrame if you performed key-based resolution; otherwise use df",
        "clean = df.copy()",
        "clean_path = 'data/hippo_nutrients_clean.csv'",
        "clean.to_csv(clean_path, index=False)",
        "print('Saved:', clean_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exercises",
      "metadata": {},
      "source": [
        "## ðŸ§ª Exercises (Guided)\n",
        "1) **Missingness by subgroup**  \n",
        "   - Compute the proportion of missing `Value` by `(Nutrient, Sex)`.  \n",
        "   - Impute `Value` using **median by NutrientÃ—Sex** and track imputed rows with a boolean flag.\n",
        "\n",
        "2) **Locale & units clean-up**  \n",
        "   - Create a column `Value_text` by copying `Value` as strings and inject a few messy entries (with `'mg'`, spaces, decimal commas).  \n",
        "   - Write a `clean_numeric()` function that removes units, trims, converts decimal commas, and returns numeric.  \n",
        "   - Compare distributions before/after cleaning.\n",
        "\n",
        "3) **Duplicates resolution policy**  \n",
        "   - Identify keys with multiple rows.  \n",
        "   - Implement *two* resolution policies: (a) prefer non-missing `Value`, (b) take the mean within key.  \n",
        "   - Compare results â€” do any policies change downstream summaries?\n",
        "\n",
        "4) **Validation rules**  \n",
        "   - Add a rule: `Year` must be between 1990 and 2035.  \n",
        "   - Add a rule: `Nutrient` must belong to a specific allowed set found in `df['Nutrient'].unique()` (or a curated list).  \n",
        "   - Make the notebook *fail loudly* (raise an Exception) if violations exceed a small tolerance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wrap",
      "metadata": {},
      "source": [
        "## âœ… Conclusion & Next Steps\n",
        "Youâ€™ve implemented a **principled cleaning pipeline**:\n",
        "- Profiled the data.\n",
        "- Handled missingness with transparent, group-wise imputation.\n",
        "- Standardised messy numeric and categorical fields (decimal commas, units, percentages, Sex labels).\n",
        "- Detected exact and key-based duplicates and demonstrated a resolution policy.\n",
        "- Encoded assumptions as validation **rules**.\n",
        "\n",
        "ðŸ‘‰ Next: **3.4 Data Transformation** â€” reshaping, deriving variables, and feature engineering for analysis.\n",
        "\n",
        "---\n",
        "**Resources**  \n",
        "- pandas: Missing data â€” https://pandas.pydata.org/docs/user_guide/missing_data.html  \n",
        "- pandas: Working with text data â€” https://pandas.pydata.org/docs/user_guide/text.html  \n",
        "- pandas: Duplicate labels/rows â€” https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html  \n",
        "- On assumptions & validation in data analysis: keep a crisp `README` alongside your cleaned outputs.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

