{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 10.5 • Machine Learning: From First Principles to Trustworthy Models\n",
    "\n",
    "This notebook demystifies ML: there’s no magic, only **assumptions + optimisation + evaluation**. We’ll show not only *how* to fit models, but *why* specific steps are necessary for results you can defend in a viva or a regulatory audit.\n",
    "\n",
    "### Learning objectives\n",
    "1. Data workflow: leakage-free preprocessing, train/validation/test, reproducibility.\n",
    "2. Bias–variance and sample complexity: learning curves and when to stop.\n",
    "3. Honest model selection: cross-validation that doesn’t lie.\n",
    "4. Interpretable outputs: permutation importance, partial dependence, SHAP (optional).\n",
    "5. Probability quality: calibration curves, Brier score, thresholds tuned to utility.\n",
    "6. Robustness: stress tests, subgroup performance, drift, and documentation.\n",
    "\n",
    "We simulate a medium-sized clinical/metabolomics-like dataset (n≈1,200, p=30) with weak correlations and non-linear signal in a handful of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from sklearn.model_selection import (train_test_split, StratifiedKFold, GridSearchCV,\n",
    "                                     cross_val_score, learning_curve)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (roc_auc_score, RocCurveDisplay, ConfusionMatrixDisplay,\n",
    "                             classification_report, brier_score_loss, precision_recall_curve,\n",
    "                             PrecisionRecallDisplay)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\n",
    "np.random.seed(11088); sns.set_style('whitegrid')\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0) Data simulation (transparent ground truth)\n",
    "We bake in structure so we know whether models recover it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n, p = 1200, 30\n",
    "X = np.random.normal(size=(n, p))\n",
    "# Inject correlated blocks\n",
    "for b in range(3):\n",
    "    z = np.random.normal(size=(n,1))\n",
    "    X[:, 10*b:10*(b+1)] += 0.6*z\n",
    "cols = [f'feat_{i+1}' for i in range(p)]\n",
    "\n",
    "# Non-linear signal in a subset\n",
    "sig = (0.4*X[:,2] - 0.35*X[:,5] + 0.5*(X[:,7]**2>0.5).astype(float)\n",
    "       + 0.45*X[:,12] - 0.4*X[:,17] + 0.35*np.sin(X[:,26]))\n",
    "logit = -0.2 + sig\n",
    "proba = 1/(1+np.exp(-logit))\n",
    "y = (np.random.rand(n) < 0.08 + 0.84*proba).astype(int)  # mild imbalance\n",
    "df = pd.DataFrame(X, columns=cols); df['target'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1) Reproducible split: train (75%) / test (25%)\n",
    "**Golden rule**: the test set is a sealed envelope — use **once** at the end. Everything else (preprocessing, hyper-parameters, model choice) happens *inside* cross-validation on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns='target').values\n",
    "y = df['target'].values\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y, random_state=11088)\n",
    "X_tr.shape, X_te.shape, y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2) Baselines and leakage-safe pipelines\n",
    "- Numerical scaling *must* be fit on training folds only.\n",
    "- Use `Pipeline` so cross-validation doesn’t leak information from validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=11088)\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "grid_lr = GridSearchCV(pipe_lr, param_grid={'clf__C':[0.1, 0.5, 1, 2, 5]},\n",
    "                       scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "grid_lr.fit(X_tr, y_tr)\n",
    "grid_lr.best_params_, grid_lr.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe_rf = Pipeline([('clf', RandomForestClassifier(random_state=11088, class_weight='balanced'))])\n",
    "grid_rf = GridSearchCV(pipe_rf,\n",
    "    param_grid={'clf__n_estimators':[200, 400, 800],\n",
    "                'clf__max_depth':[None, 10, 20],\n",
    "                'clf__max_features':['sqrt','log2']},\n",
    "    scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "grid_rf.fit(X_tr, y_tr)\n",
    "grid_rf.best_params_, grid_rf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3) Learning curves: do we need more data or a simpler model?\n",
    "Bias–variance in practice: if training and CV scores are both low → underfit (increase model capacity). If training ≫ CV → high variance (more data/regularisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(est, X, y, title):\n",
    "    sizes, train_scores, val_scores = learning_curve(\n",
    "        est, X, y, cv=cv, scoring='roc_auc', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 8), random_state=11088)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(sizes, train_scores.mean(axis=1), label='Train')\n",
    "    plt.plot(sizes, val_scores.mean(axis=1), label='CV')\n",
    "    plt.xlabel('Training size'); plt.ylabel('ROC-AUC'); plt.title(title); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "plot_learning_curve(grid_lr.best_estimator_, X_tr, y_tr, 'Logistic learning curve')\n",
    "plot_learning_curve(grid_rf.best_estimator_, X_tr, y_tr, 'RandomForest learning curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4) Final evaluation on the *sealed* test set\n",
    "Report ROC-AUC, PR curve (useful for imbalance), confusion matrix at a utility-tuned threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, model in [('Logistic', grid_lr.best_estimator_), ('RandomForest', grid_rf.best_estimator_)]:\n",
    "    proba = model.predict_proba(X_te)[:,1]\n",
    "    auc = roc_auc_score(y_te, proba)\n",
    "    print(f\"\\n{name}: Test ROC-AUC={auc:.3f}\")\n",
    "    RocCurveDisplay.from_predictions(y_te, proba); plt.title(f'{name} ROC'); plt.show()\n",
    "    pr, rc, _ = precision_recall_curve(y_te, proba)\n",
    "    PrecisionRecallDisplay(precision=pr, recall=rc).plot(); plt.title(f'{name} PR'); plt.show()\n",
    "    yhat = (proba >= 0.5).astype(int)\n",
    "    ConfusionMatrixDisplay.from_predictions(y_te, yhat); plt.title(f'{name} @0.5'); plt.show()\n",
    "    print(classification_report(y_te, yhat, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5) Interpreting models responsibly\n",
    "### Permutation importance (model-agnostic)\n",
    "Randomly shuffle one feature in the test set; the metric drop estimates its importance. Less biased than tree impurity.\n",
    "\n",
    "### Partial dependence (marginal effect curves)\n",
    "Average effect of a feature on the prediction, holding others at their empirical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf = grid_rf.best_estimator_\n",
    "perm = permutation_importance(rf, X_te, y_te, scoring='roc_auc', n_repeats=20, random_state=11088)\n",
    "imp = pd.DataFrame({'feature': cols, 'importance': perm.importances_mean}).sort_values('importance', ascending=False).head(12)\n",
    "sns.barplot(data=imp, x='importance', y='feature'); plt.title('Permutation importance (RF)'); plt.tight_layout(); plt.show()\n",
    "PartialDependenceDisplay.from_estimator(rf, X_te, features=[2,5,7,12]); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6) Probability calibration: when 0.7 should mean 70%\n",
    "If you’ll **act** on probabilities (screening, triage), calibration matters as much as discrimination.\n",
    "- Brier score: mean squared error of probabilities vs outcomes.\n",
    "- Fix with **isotonic** or **Platt** (sigmoid) calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uncal = grid_rf.best_estimator_\n",
    "cal_iso = CalibratedClassifierCV(uncal, method='isotonic', cv=5).fit(X_tr, y_tr)\n",
    "for name, model in [('Uncalibrated', uncal), ('Isotonic', cal_iso)]:\n",
    "    p = model.predict_proba(X_te)[:,1]\n",
    "    print(f\"{name}: Brier={brier_score_loss(y_te,p):.3f}, AUC={roc_auc_score(y_te,p):.3f}\")\n",
    "    CalibrationDisplay.from_predictions(y_te, p, n_bins=10)\n",
    "    plt.title(f'Calibration: {name}'); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7) Robustness, fairness, and documentation\n",
    "- **Stress tests**: add Gaussian noise, drop top-k features, simulate shift (mean offset) and re-evaluate.\n",
    "- **Subgroup performance**: if you had a sex/age flag — always report per-group metrics.\n",
    "- **Model card (mini)**: purpose, data, preprocessing, metrics, limitations, update policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple stress test: additive noise\n",
    "def stress(model, X, y, sigma):\n",
    "    p = model.predict_proba(X + np.random.normal(0, sigma, X.shape))[:,1]\n",
    "    return roc_auc_score(y, p)\n",
    "for s in [0.0, 0.05, 0.1, 0.2]:\n",
    "    print(f\"RF AUC under noise σ={s}: {stress(uncal, X_te, y_te, s):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercises\n",
    "1. **Utility-tuned threshold**: Suppose FP costs 1 and FN costs 5. On the test set, sweep thresholds to minimise expected cost.\n",
    "2. **Top-k features**: Refit RF on only the top-10 permutation-important features. What happens to test AUC and calibration?\n",
    "3. **Data drift**: Add a +0.3 shift to `feat_12` in the test set only. How much AUC drops? How would you monitor for this in production?\n",
    "\n",
    "## Key takeaways\n",
    "- Pipelines + CV prevent leakage.\n",
    "- Always show a **learning curve**.\n",
    "- Report **calibration** if probabilities drive action.\n",
    "- Document stress tests and subgroup performance — that’s what trust looks like."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
