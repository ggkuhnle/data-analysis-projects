{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3c36723",
   "metadata": {},
   "source": [
    "# üåü Artificial Intelligence for Nutrition & Food Science: Serving Up the Future! ü§ñ\n",
    "\n",
    "Welcome to this delicious Jupyter Notebook on **Artificial Intelligence (AI)** for nutrition and food science! Whether you're munching at home üç¥ or cooking up ideas in a classroom, this guide will whisk you through the exciting world of AI, with a focus on nutrition and food science. We'll dive into **neural networks**, **deep learning**, and their superpowers for classifying diets, predicting nutrient content, and analyzing food quality! ü•ï\n",
    "\n",
    "Get ready for code, exercises, and hidden surprises (click the \"Details\" to uncover them)! Let‚Äôs blast off into the AI kitchen! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a3195b",
   "metadata": {},
   "source": [
    "## 1. Introduction to AI in Nutrition & Food Science ü•ó\n",
    "\n",
    "Nutrition and food science data are like a recipe book üìñ‚Äîpacked with nutrients, diets, and quality metrics. AI, especially **neural networks** and **deep learning**, helps us:\n",
    "\n",
    "- **Classify** diets (e.g., healthy vs. unhealthy).\n",
    "- **Predict** nutritional values (e.g., calorie or protein content).\n",
    "- **Analyze** food quality (e.g., fresh vs. spoiled).\n",
    "\n",
    "We'll use Python with `tensorflow`, `keras`, `scikit-learn`, and `pandas` to build AI models. No chef‚Äôs hat required‚Äîjust enthusiasm! üòÑ\n",
    "\n",
    "**Exercise 1**: Why might AI be more powerful than traditional machine learning for nutrition and food science? Write your thoughts (no code needed).\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Think about how AI can learn complex patterns, handle high-dimensional data, and model intricate nutritional interactions.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d51fa",
   "metadata": {},
   "source": [
    "Let's start with loading the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d00b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de3fb73",
   "metadata": {},
   "source": [
    "## 2. Neural Networks: The Brain of AI üß†\n",
    "\n",
    "Neural networks mimic the human brain, with layers of **neurons** that learn patterns from data. In nutrition, they‚Äôre great for classifying diets!\n",
    "\n",
    "### 2.1 A Simple Neural Network\n",
    "\n",
    "Let‚Äôs build a basic neural network to classify synthetic dietary profiles as healthy or unhealthy using `tensorflow` and `keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c802aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate synthetic nutrition dataset (60 samples, 10 nutrients/features)\n",
    "np.random.seed(42)\n",
    "data = pd.DataFrame({\n",
    "    'Calories': np.random.normal(500, 100, 60),\n",
    "    'Protein_g': np.random.normal(30, 5, 60),\n",
    "    'Carbs_g': np.random.normal(60, 10, 60),\n",
    "    'Fat_g': np.random.normal(20, 5, 60),\n",
    "    'Fiber_g': np.random.normal(10, 2, 60),\n",
    "    'Sugar_g': np.random.normal(15, 5, 60),\n",
    "    'Sodium_mg': np.random.normal(800, 200, 60),\n",
    "    'Vitamin_C_mg': np.random.normal(50, 10, 60),\n",
    "    'Calcium_mg': np.random.normal(300, 50, 60),\n",
    "    'Iron_mg': np.random.normal(5, 1, 60)\n",
    "})\n",
    "labels = np.random.choice([0, 1], size=60)  # 0 = unhealthy, 1 = healthy\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_scaled, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build neural network\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(10,)),  # Hidden layer with 64 neurons\n",
    "    Dense(32, activation='relu'),                    # Second hidden layer\n",
    "    Dense(1, activation='sigmoid')                   # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int).flatten()\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Neural Network Accuracy: {accuracy:.2f} üéâ')\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix for Diet Classification ü•ó')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('nn_cm.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1737bf0",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **Sequential**: A stack of layers (input, hidden, output).\n",
    "- **Dense**: Fully connected layers; `relu` for hidden, `sigmoid` for binary output.\n",
    "- **adam**: Optimizer for efficient training.\n",
    "- **binary_crossentropy**: Loss function for binary classification.\n",
    "\n",
    "**Exercise 2**: Add another hidden layer with 16 neurons (`Dense(16, activation='relu')`) before the output layer. Does the accuracy improve? Why?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Solution</summary>\n",
    "Modify the model to:\n",
    "```python\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(10,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "```\n",
    "A deeper network may capture more complex patterns but risks overfitting on small datasets.\n",
    "</details>\n",
    "\n",
    "**Learn More**: Check out [Keras Documentation](https://keras.io/) for neural network tips! üìö"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213765d5",
   "metadata": {},
   "source": [
    "## 3. Deep Learning: Going Deeper with CNNs for Food Quality üåê\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are great for image-based tasks, but we can use 1D CNNs to analyze nutritional profiles as \"signals\" for tasks like food quality classification.\n",
    "\n",
    "### 3.1 1D CNN for Food Quality\n",
    "\n",
    "Let‚Äôs build a 1D CNN to classify foods as fresh or spoiled based on nutrient profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff44ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reshape data for 1D CNN (samples, nutrients, 1 channel)\n",
    "X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Build 1D CNN\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(10, 1)),  # 1D convolution\n",
    "    MaxPooling1D(pool_size=2),                                         # Downsampling\n",
    "    Conv1D(16, kernel_size=3, activation='relu'),                      # Second conv layer\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),                                                         # Flatten for dense layers\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "cnn_history = cnn_model.fit(X_train_cnn, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred_cnn = (cnn_model.predict(X_test_cnn) > 0.5).astype(int).flatten()\n",
    "accuracy_cnn = accuracy_score(y_test, y_pred_cnn)\n",
    "print(f'1D CNN Accuracy: {accuracy_cnn:.2f} üåü')\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(cnn_history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(cnn_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('1D CNN Training Progress for Food Quality üìà')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('cnn_history.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e50b9a",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **Conv1D**: Extracts local patterns in nutrient profiles.\n",
    "- **MaxPooling1D**: Reduces dimensionality while keeping key features.\n",
    "- **Flatten**: Prepares data for dense layers.\n",
    "\n",
    "**Exercise 3**: Increase the `kernel_size` to 5 in the first `Conv1D` layer. Does the accuracy change? Why?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "A larger kernel captures broader patterns but may miss fine details. Check if the model generalizes better or overfits!\n",
    "</details>\n",
    "\n",
    "**Learn More**: Explore [CNNs in TensorFlow](https://www.tensorflow.org/tutorials/images/cnn) for more ideas! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741d7b25",
   "metadata": {},
   "source": [
    "## 4. Autoencoders: Unsupervised Feature Learning üïµÔ∏è‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Autoencoders are neural networks that learn compressed representations of data (like PCA but non-linear). In nutrition, they‚Äôre useful for dimensionality reduction or identifying dietary patterns.\n",
    "\n",
    "### 4.1 Autoencoder for Nutrition Data\n",
    "\n",
    "Let‚Äôs build an autoencoder to compress our nutritional profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Build autoencoder\n",
    "input_layer = Input(shape=(10,))\n",
    "encoded = Dense(32, activation='relu')(input_layer)  # Encoder\n",
    "encoded = Dense(16, activation='relu')(encoded)      # Bottleneck\n",
    "decoded = Dense(32, activation='relu')(encoded)      # Decoder\n",
    "decoded = Dense(10, activation='linear')(decoded)    # Reconstruction\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "encoder = Model(input_layer, encoded)  # For extracting compressed features\n",
    "\n",
    "# Compile and train\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(data_scaled, data_scaled, epochs=50, batch_size=16, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Get compressed features\n",
    "compressed_features = encoder.predict(data_scaled)\n",
    "\n",
    "# Plot original vs. reconstructed data for first sample\n",
    "reconstructed = autoencoder.predict(data_scaled)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data_scaled[0], label='Original')\n",
    "plt.plot(reconstructed[0], label='Reconstructed')\n",
    "plt.title('Autoencoder Reconstruction of Nutritional Profile ü•ê')\n",
    "plt.xlabel('Nutrient Index')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('autoencoder_reconstruction.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c3a6c",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **Encoder**: Compresses nutritional data into a lower-dimensional space.\n",
    "- **Decoder**: Reconstructs the original data.\n",
    "- **mse**: Loss function to minimize reconstruction error.\n",
    "\n",
    "**Exercise 4**: Reduce the bottleneck layer to 8 neurons (`Dense(8, activation='relu')`). Does the reconstruction quality change? Why?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Solution</summary>\n",
    "A smaller bottleneck forces more compression, which may lead to information loss and poorer reconstruction. Compare the plots!\n",
    "</details>\n",
    "\n",
    "**Learn More**: Check out [Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html) for more applications! üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537a97a6",
   "metadata": {},
   "source": [
    "## 5. Using Compressed Features for Classification üåü\n",
    "\n",
    "Let‚Äôs use the autoencoder‚Äôs compressed features as input to a neural network for diet classification, combining unsupervised and supervised learning.\n",
    "\n",
    "### 5.1 Classification with Compressed Features\n",
    "\n",
    "We‚Äôll train a neural network on the compressed features from the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416c02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split compressed features\n",
    "X_train_comp, X_test_comp, y_train_comp, y_test_comp = train_test_split(compressed_features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build classification model\n",
    "comp_model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(16,)),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "comp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "comp_model.fit(X_train_comp, y_train_comp, epochs=50, batch_size=16, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_comp = (comp_model.predict(X_test_comp) > 0.5).astype(int).flatten()\n",
    "accuracy_comp = accuracy_score(y_test_comp, y_pred_comp)\n",
    "print(f'Accuracy with Compressed Features: {accuracy_comp:.2f} üöÄ')\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm_comp = confusion_matrix(y_test_comp, y_pred_comp)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm_comp, annot=True, fmt='d', cmap='Purples', cbar=False)\n",
    "plt.title('Confusion Matrix for Compressed Features ü•ó')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('comp_cm.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba48ce7",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **compressed_features**: Lower-dimensional representations from the autoencoder.\n",
    "- **Sequential**: A neural network for classification using these features.\n",
    "\n",
    "**Exercise 5**: Compare the accuracy here to the neural network in Section 2. Why might compressed features perform differently?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Compressed features reduce noise but may lose some information. Check if the trade-off improves or harms performance!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cc0573",
   "metadata": {},
   "source": [
    "## 6. Summary: Your AI Toolkit for Nutrition & Food Science üß∞\n",
    "\n",
    "Here‚Äôs what you‚Äôve whipped up:\n",
    "\n",
    "- **Neural Networks** üß†: Basic AI for diet classification.\n",
    "- **1D CNNs** üåê: Deep learning for nutritional profiles.\n",
    "- **Autoencoders** üïµÔ∏è‚Äç‚ôÇÔ∏è: Unsupervised feature learning.\n",
    "- **Compressed Features** üåü: Combining unsupervised and supervised AI.\n",
    "\n",
    "**Final Exercise**: Download a real nutrition dataset from [USDA FoodData Central](https://fdc.nal.usda.gov/) and apply one of these AI methods. Share your findings in a short paragraph!\n",
    "\n",
    "**What‚Äôs Next?** Explore advanced AI techniques like GANs, transfer learning, or graph neural networks for nutrition and food science. Keep dreaming big, and happy AI-ing! üòÑ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
