{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ‚öôÔ∏è 5.2 Workflow Automation\n",
    "\n",
    "Automate common nutrition data tasks so they‚Äôre **repeatable**, **robust**, and **shareable**. We‚Äôll turn a manual analysis on `large_food_log.csv` into a small pipeline: clean ‚Üí validate ‚Üí transform ‚Üí summarise ‚Üí export.\n",
    "\n",
    "**You will:**\n",
    "- Build reusable functions with clear inputs/outputs.\n",
    "- Parameterise runs (date ranges, filters, output paths).\n",
    "- Add sanity checks and lightweight logging.\n",
    "- Export tidy artefacts (CSV/Parquet) for downstream notebooks.\n",
    "\n",
    "<details><summary>When to automate?</summary>\n",
    "When you repeat the same steps across files, days, or projects; when colleagues need to run it; or when you want reliable, version-controlled outputs.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab_setup",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup for Google Colab: clone repo and locate data\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "MODULE = '05_advanced'\n",
    "DATASET = 'large_food_log.csv'\n",
    "BASE_PATH = '/content/data-analysis-projects'\n",
    "MODULE_PATH = os.path.join(BASE_PATH, 'notebooks', MODULE)\n",
    "DATASET_PATH = os.path.join('data', DATASET)\n",
    "\n",
    "try:\n",
    "    print('Attempting to clone repository...')\n",
    "    if not os.path.exists(BASE_PATH):\n",
    "        !git clone https://github.com/ggkuhnle/data-analysis-projects.git\n",
    "    print('Setting working directory...')\n",
    "    os.chdir(MODULE_PATH)\n",
    "    if os.path.exists(DATASET_PATH):\n",
    "        print(f'Dataset found: {DATASET_PATH} ‚úÖ')\n",
    "    else:\n",
    "        raise FileNotFoundError('Dataset missing after clone.')\n",
    "except Exception as e:\n",
    "    print(f'Cloning failed: {e}')\n",
    "    print('Falling back to manual upload...')\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    uploaded = files.upload()\n",
    "    if DATASET in uploaded:\n",
    "        with open(DATASET_PATH, 'wb') as f:\n",
    "            f.write(uploaded[DATASET])\n",
    "        print(f'Successfully uploaded {DATASET} ‚úÖ')\n",
    "    else:\n",
    "        raise FileNotFoundError(f'Upload failed. Please upload {DATASET}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_libs",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -q pandas numpy pyarrow tqdm\n",
    "import pandas as pd, numpy as np, datetime as dt, os, sys, json, textwrap, pathlib\n",
    "from tqdm.auto import tqdm\n",
    "pd.set_option('display.max_columns', 40)\n",
    "print('Automation environment ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "params",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üîß Run Parameters\n",
    "Central place to tweak what the pipeline does without changing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "default_params",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"input_csv\": \"data/large_food_log.csv\",\n",
    "    \"out_dir\": \"artifacts/5_2_automation\",\n",
    "    \"date_col\": \"Date\",\n",
    "    \"filters\": {  # optional filters\n",
    "        \"Nutrient_in\": [\"Iron\", \"Calcium\", \"Vitamin_D\", \"Protein\"]\n",
    "    },\n",
    "    \"date_range\": {  # optional date range\n",
    "        \"start\": None,   # e.g., \"2024-01-01\"\n",
    "        \"end\": None      # e.g., \"2024-12-31\"\n",
    "    },\n",
    "    \"export\": {\n",
    "        \"formats\": [\"csv\", \"parquet\"],\n",
    "        \"summaries\": [\n",
    "            {\"by\": [\"Date\", \"Nutrient\"], \"value\": \"Amount\", \"agg\": \"sum\", \"name\": \"sum_by_date_nutrient\"},\n",
    "            {\"by\": [\"Meal\", \"Nutrient\"], \"value\": \"Amount\", \"agg\": \"mean\", \"name\": \"mean_by_meal_nutrient\"}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "PARAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpers",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üß∞ Reusable Helpers\n",
    "Small, single-purpose functions make the pipeline readable and testable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpers_code",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log(msg: str):\n",
    "    now = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(f\"[{now}] {msg}\")\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def read_csv_safe(path: str) -> pd.DataFrame:\n",
    "    log(f\"Reading: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Input CSV is empty.\")\n",
    "    return df\n",
    "\n",
    "def clean_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # lower snake_case\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip().replace(' ', '_') for c in df.columns]\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def parse_dates(df: pd.DataFrame, date_col: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if date_col.lower() not in df.columns:\n",
    "        raise KeyError(f\"Date column '{date_col}' not found after cleaning.\")\n",
    "    df[date_col.lower()] = pd.to_datetime(df[date_col.lower()], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def standardise_units(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Example: if Amount sometimes given in grams vs milligrams; here we assume it's already coherent\n",
    "    # Add hooks here if your real data needs scaling (e.g., mg‚Üíg)\n",
    "    return df\n",
    "\n",
    "def apply_filters(df: pd.DataFrame, params: dict) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    f = params.get(\"filters\", {})\n",
    "    if f.get(\"Nutrient_in\") and \"nutrient\" in df.columns:\n",
    "        df = df[df[\"nutrient\"].isin(f[\"Nutrient_in\"])].copy()\n",
    "    # Date range\n",
    "    dr = params.get(\"date_range\", {})\n",
    "    date_col = params.get(\"date_col\", \"Date\").lower()\n",
    "    start = dr.get(\"start\"); end = dr.get(\"end\")\n",
    "    if start:\n",
    "        df = df[df[date_col] >= pd.to_datetime(start)]\n",
    "    if end:\n",
    "        df = df[df[date_col] <= pd.to_datetime(end)]\n",
    "    return df\n",
    "\n",
    "def validate(df: pd.DataFrame):\n",
    "    # Minimal sanity checks; extend as needed\n",
    "    required = {\"id\", \"meal\", \"nutrient\", \"amount\", \"date\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {sorted(missing)}\")\n",
    "    if df[\"amount\"].isna().any():\n",
    "        raise ValueError(\"Amount contains missing values.\")\n",
    "    if (df[\"amount\"] < 0).any():\n",
    "        raise ValueError(\"Amount contains negative values.\")\n",
    "    if df[\"date\"].isna().any():\n",
    "        raise ValueError(\"Date contains unparseable values.\")\n",
    "    return True\n",
    "\n",
    "def summarise(df: pd.DataFrame, *, by, value: str, agg: str) -> pd.DataFrame:\n",
    "    if agg not in {\"sum\", \"mean\", \"median\", \"count\"}:\n",
    "        raise ValueError(\"Unsupported aggregation; choose from sum, mean, median, count\")\n",
    "    grp = df.groupby(by, observed=True)[value]\n",
    "    res = getattr(grp, agg)().reset_index(name=f\"{value}_{agg}\")\n",
    "    return res\n",
    "\n",
    "def export(df: pd.DataFrame, out_dir: str, name: str, formats=(\"csv\", \"parquet\")):\n",
    "    ensure_dir(out_dir)\n",
    "    if \"csv\" in formats:\n",
    "        path_csv = os.path.join(out_dir, f\"{name}.csv\")\n",
    "        df.to_csv(path_csv, index=False)\n",
    "        log(f\"Wrote: {path_csv}\")\n",
    "    if \"parquet\" in formats:\n",
    "        path_pq = os.path.join(out_dir, f\"{name}.parquet\")\n",
    "        df.to_parquet(path_pq, index=False)\n",
    "        log(f\"Wrote: {path_pq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üèóÔ∏è The Pipeline\n",
    "All steps connected with logging and idempotent artefact writes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline_code",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_pipeline(params: dict):\n",
    "    log(\"Starting pipeline‚Ä¶\")\n",
    "    cfg = params.copy()\n",
    "    ensure_dir(cfg[\"out_dir\"])\n",
    "\n",
    "    # 1) Load\n",
    "    raw = read_csv_safe(cfg[\"input_csv\"])\n",
    "\n",
    "    # 2) Clean/standardise\n",
    "    df = clean_columns(raw)\n",
    "    df = parse_dates(df, cfg[\"date_col\"])\n",
    "    df = standardise_units(df)\n",
    "\n",
    "    # 3) Validate\n",
    "    validate(df)\n",
    "\n",
    "    # 4) Filter (optional)\n",
    "    df_f = apply_filters(df, cfg)\n",
    "    log(f\"Rows after filtering: {len(df_f):,}\")\n",
    "\n",
    "    # 5) Export a cleaned/filtered snapshot for reproducibility\n",
    "    export(df_f, cfg[\"out_dir\"], name=\"clean_filtered\", formats=cfg[\"export\"][\"formats\"])\n",
    "\n",
    "    # 6) Summaries\n",
    "    summaries = cfg[\"export\"][\"summaries\"]\n",
    "    outputs = {}\n",
    "    for spec in summaries:\n",
    "        res = summarise(df_f, by=spec[\"by\"], value=spec[\"value\"], agg=spec[\"agg\"])\n",
    "        export(res, cfg[\"out_dir\"], name=spec[\"name\"], formats=cfg[\"export\"][\"formats\"])\n",
    "        outputs[spec[\"name\"]] = res\n",
    "\n",
    "    log(\"Pipeline complete.\")\n",
    "    return {\"data\": df_f, \"outputs\": outputs}\n",
    "\n",
    "# Run once to demonstrate\n",
    "artifacts = run_pipeline(PARAMS)\n",
    "list(artifacts[\"outputs\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peek_outputs",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üëÄ Quick Peek at Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peek_tables",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, df_out in artifacts[\"outputs\"].items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    display(df_out.head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tests",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ‚úÖ Lightweight Tests\n",
    "A few assertions catch common regressions. Add more for your project‚Äôs rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tests_code",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_clean = artifacts[\"data\"]\n",
    "assert (df_clean[\"amount\"] >= 0).all(), \"Found negative amounts after cleaning!\"\n",
    "assert df_clean[\"date\"].dtype.kind == 'M', \"Date column must be datetime.\"\n",
    "for name, df_out in artifacts[\"outputs\"].items():\n",
    "    assert not df_out.empty, f\"Summary {name} is empty.\"\n",
    "print(\"All sanity checks passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "param_runs",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üß™ Parameterised Runs (Examples)\n",
    "Change filters/date ranges without touching pipeline internals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "param_examples",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params_alt = PARAMS.copy()\n",
    "params_alt[\"out_dir\"] = \"artifacts/5_2_automation_subset\"\n",
    "params_alt[\"filters\"] = {\"Nutrient_in\": [\"Iron\", \"Vitamin_D\"]}\n",
    "params_alt[\"date_range\"] = {\"start\": None, \"end\": None}\n",
    "\n",
    "artifacts_alt = run_pipeline(params_alt)\n",
    "print(\"Done with parameterised run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cli",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üñ•Ô∏è (Optional) CLI Script Template\n",
    "Copy this to a file (e.g., `run_pipeline.py`) if you want to run it outside notebooks.\n",
    "\n",
    "<details><summary>Show script</summary>\n",
    "\n",
    "```python\n",
    "import json, argparse\n",
    "from your_module import run_pipeline  # import functions from a module/package\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument('--params', type=str, required=True, help='Path to params JSON file')\n",
    "    args = ap.parse_args()\n",
    "    with open(args.params) as f:\n",
    "        params = json.load(f)\n",
    "    run_pipeline(params)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üß© Exercises\n",
    "1. **New Summary:** Add a third summary: total `Amount` by `ID √ó Nutrient` and export it.\n",
    "2. **Unit Standardisation:** Assume some `Amount` values are grams (g) and others mg; add a conversion step so everything is mg.\n",
    "3. **Robustness:** Add a validation rule that flags days where total `Amount` exceeds a threshold per nutrient.\n",
    "4. **Performance:** If your input grows to millions of rows, refactor `read_csv_safe` to stream in chunks and aggregate incrementally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrap",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ‚úÖ Wrap-up\n",
    "You now have a small but robust **automation pipeline**: parameterised, validated, and exporting tidy artefacts other notebooks can consume.\n",
    "\n",
    "<details><summary>Further reading</summary>\n",
    "- Pandas GroupBy: https://pandas.pydata.org/docs/user_guide/groupby.html\n",
    "- Arrow/Parquet: https://arrow.apache.org/\n",
    "- tqdm (progress bars): https://tqdm.github.io/\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}