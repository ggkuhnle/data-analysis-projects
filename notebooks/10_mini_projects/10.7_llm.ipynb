{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e22e338",
   "metadata": {},
   "source": [
    "# üåü Using LLM APIs for Nutrition & Food Science: A Tasty AI Adventure! ü•ë\n",
    "\n",
    "Welcome to this scrumptious Jupyter Notebook on harnessing **Large Language Model (LLM) APIs** for nutrition and food science research! Whether you're nibbling at home üçé or cooking up ideas in a classroom, this guide will whisk you through using APIs from LLMs like **Grok**, **ChatGPT**, **Manus**, and more to tackle tasks like **parsing food diaries**, **reviewing literature**, **analyzing supply chains**, and **sensory analysis**! üç¥\n",
    "\n",
    "We‚Äôll use Python with a **free API** (Hugging Face) and explore other LLM APIs, comparing their strengths and weaknesses. Expect code, exercises, and hidden treats (click the \"Details\" to reveal them)! Let‚Äôs dive into the AI kitchen! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc2c188",
   "metadata": {},
   "source": [
    "## 1. Introduction to LLM APIs in Nutrition & Food Science üìä\n",
    "\n",
    "Nutrition and food science are like a buffet of data üçΩÔ∏è‚Äîfood diaries, research papers, supply chain logs, and sensory descriptions. LLM APIs let us tap into powerful language models to:\n",
    "\n",
    "- **Parse food diaries**: Extract nutrients or dietary patterns.\n",
    "- **Review literature**: Summarize nutrition studies.\n",
    "- **Analyze supply chains**: Optimize logistics or detect issues.\n",
    "- **Perform sensory analysis**: Interpret taste and texture descriptions.\n",
    "\n",
    "We‚Äôll use Python with `requests`, `pandas`, and a **free Hugging Face API** (plus others if you have access). No master chef skills needed‚Äîjust curiosity! üòÑ\n",
    "\n",
    "**Exercise 1**: Why might LLM APIs be ideal for parsing unstructured nutrition data (e.g., food diaries)? Jot down your thoughts (no code needed).\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Consider LLMs‚Äô ability to understand context, handle varied text formats, and extract structured information from messy data.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b3ab46",
   "metadata": {},
   "source": [
    "Let's load the required libraries first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d3472",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup for Google Colab: Fetch datasets automatically or manually\n",
    "%run ../../bootstrap.py    # installs requirements + editable package\n",
    "\n",
    "import fns_toolkit as fns\n",
    "\n",
    "# Import libraries\n",
    "\n",
    "%pip install huggingface_hub\n",
    "%pip install transformers\n",
    "%pip install torch\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import huggingface_hub\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import f\n",
    "import numpy.linalg as la\n",
    "\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "import textwrap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cbf3af",
   "metadata": {},
   "source": [
    "## 2. Overview of LLMs and Their APIs üß†\n",
    "\n",
    "Here‚Äôs a rundown of popular LLMs, their APIs, and their strengths and weaknesses for nutrition and food science tasks. *Note*: All LLMs may exhibit biases from training data (e.g., cultural or dietary biases), which can affect applications like food diary parsing or sensory analysis.\n",
    "\n",
    "| **LLM** | **Provider** | **API Availability** | **Free Tier?** | **Strengths** | **Weaknesses** |\n",
    "|---------|--------------|---------------------|----------------|--------------|---------------|\n",
    "| **Grok** | xAI | Yes (Grok API) | Limited (beta for X Premium+ users) | Real-time X integration, witty responses, strong reasoning | Limited free access, proprietary, less conversational depth |\n",
    "| **ChatGPT** | OpenAI | Yes (OpenAI API) | No (paid, starts ~$20/month) | Conversational fluency, broad NLP tasks (e.g., text summarization) | No free tier, lacks real-time data |\n",
    "| **Manus** | Manus AI | Yes (limited beta) | No (proprietary, waitlist) | Enterprise automation, multi-model architecture, deep reasoning | Slow response times, less conversational, academic tone |\n",
    "| **Llama** | Meta AI | No (open-source, not API-hosted) | Free (local use) | Open-source, medical/nutrition applications | High computational needs, no hosted API |\n",
    "| **DeepSeek** | DeepSeek | Yes (open-source API) | Yes (free for public use) | Efficient, open-source, structured problem-solving | Less conversational, limited multimodal support |\n",
    "| **Hugging Face Models** | Hugging Face | Yes (Inference API) | Yes (free tier) | Free, diverse models (e.g., BART for summarization), customizable | Limited free quota, less advanced than GPT-4 |\n",
    "\n",
    "**Why Hugging Face?** We‚Äôll use Hugging Face‚Äôs free Inference API for its accessibility and robust NLP capabilities, ideal for tasks like summarization and text analysis in nutrition research.\n",
    "\n",
    "**Exercise 2**: Which LLM might be best for real-time supply chain analysis? Why? (No code needed).\n",
    "\n",
    "<details>\n",
    "<summary>üí° Solution</summary>\n",
    "Grok‚Äôs real-time X integration makes it ideal for supply chain tasks needing current data (e.g., logistics updates).\n",
    "</details>\n",
    "\n",
    "**Learn More**: Explore [Hugging Face](https://huggingface.co/docs/api-inference), [OpenAI](https://openai.com/api/), or [xAI‚Äôs API](https://x.ai/api) for details! üìö"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b06b1e",
   "metadata": {},
   "source": [
    "## 3. Parsing Food Diaries with Hugging Face API üìù\n",
    "\n",
    "Food diaries are unstructured text (e.g., ‚ÄúAte oatmeal with berries and coffee‚Äù). LLMs can extract nutrients or dietary patterns. Let‚Äôs use Hugging Face‚Äôs free API to summarize a food diary entry.\n",
    "\n",
    "### 3.1 Summarizing a Food Diary\n",
    "\n",
    "We‚Äôll use the `facebook/bart-large-cnn` model to summarize a diary entry. Get a free API key from [Hugging Face](https://huggingface.co/).\n",
    "\n",
    "**Note**: Replace `YOUR_API_KEY` with your Hugging Face API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ead1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Hugging Face API\n",
    "API_URL = \"https://api-inference.huggingface.co/models/facebook/bart-large-cnn\"\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")  # Ensure HF_TOKEN is set in your environment\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"Hugging Face API token not found. Set HF_TOKEN environment variable.\")\n",
    "headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "\n",
    "\n",
    "# Sample food diary entry\n",
    "diary_entry = \"\"\"\n",
    "Breakfast: Oatmeal with blueberries, almond milk, and a drizzle of honey. Coffee with a splash of cream.\n",
    "Lunch: Grilled chicken salad with spinach, tomatoes, cucumber, and olive oil dressing. Sparkling water.\n",
    "Dinner: Baked salmon, quinoa, steamed broccoli, and a glass of red wine.\n",
    "Snack: Greek yogurt with a handful of almonds.\n",
    "\"\"\"\n",
    "\n",
    "# Function to summarize text\n",
    "def summarize_text(text, max_length=100, min_length=30):\n",
    "    payload = {\"inputs\": text, \"parameters\": {\"max_length\": max_length, \"min_length\": min_length}}\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[0]['summary_text']\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}\"\n",
    "\n",
    "# Summarize the diary\n",
    "summary = summarize_text(diary_entry)\n",
    "\n",
    "# Wrap summary to avoid long lines\n",
    "wrapper = textwrap.TextWrapper(width=80, subsequent_indent=\"  \")  # Wrap at 80 characters\n",
    "wrapped_summary = wrapper.fill(f\"- {summary}\")\n",
    "\n",
    "# Format the output for readability\n",
    "original_length = len(diary_entry.strip())\n",
    "summary_length = len(summary.strip())\n",
    "\n",
    "# Create structured output\n",
    "output = f\"\"\"\n",
    "{'=' * 36}\n",
    "Food Diary Summary\n",
    "{'=' * 36}\n",
    "Model: facebook/bart-large-cnn\n",
    "Original Length: {original_length} characters\n",
    "Summary Length: {summary_length} characters\n",
    "\n",
    "Summary:\n",
    "- {wrapped_summary}\n",
    "\n",
    "ü•£\n",
    "{'=' * 36}\n",
    "\"\"\"\n",
    "\n",
    "# Print formatted output\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa736e13",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **Hugging Face API**: Uses `facebook/bart-large-cnn` for summarization, ideal for condensing food diary text.\n",
    "- **requests.post**: Sends the diary entry to the API and retrieves the summary.\n",
    "- **payload**: Controls summary length for concise output.\n",
    "\n",
    "**Exercise 3**: Modify `max_length` to 50 in `summarize_text`. Is the summary more concise? Compare outputs.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Solution</summary>\n",
    "Change the function call to:\n",
    "```python\n",
    "summary = summarize_text(diary_entry, max_length=50)\n",
    "```\n",
    "A shorter `max_length` produces a more concise summary, potentially omitting details like specific foods.\n",
    "</details>\n",
    "\n",
    "**Learn More**: Try [Hugging Face‚Äôs NER models](https://huggingface.co/models) to extract specific nutrients from diaries! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f017c80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up NER model for food entity extraction\n",
    "ner_model = \"Dizex/InstaFoodRoBERTa-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ner_model)\n",
    "model = AutoModelForTokenClassification.from_pretrained(ner_model)\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Sample food diary entry\n",
    "diary_entry = \"\"\"\n",
    "Breakfast: Oatmeal with blueberries, almond milk, and a drizzle of honey. Coffee with a splash of cream.\n",
    "Lunch: Grilled chicken salad with spinach, tomatoes, cucumber, and olive oil dressing. Sparkling water.\n",
    "Dinner: Baked salmon, quinoa, steamed broccoli, and a glass of red wine.\n",
    "Snack: Greek yogurt with a handful of almonds.\n",
    "\"\"\"\n",
    "\n",
    "# Function to summarize text using Hugging Face Inference API\n",
    "def summarize_text(text, max_length=80, min_length=30):\n",
    "    # Prepare payload with tightened parameters for shorter summary\n",
    "    payload = {\n",
    "        \"inputs\": text,\n",
    "        \"parameters\": {\n",
    "            \"max_length\": max_length,\n",
    "            \"min_length\": min_length,\n",
    "            \"truncation\": True\n",
    "        }\n",
    "    }\n",
    "    # Send POST request to the Inference API\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    # Check response status\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[0]['summary_text']\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} - {response.text}\"\n",
    "\n",
    "# Function to extract food entities using NER\n",
    "def extract_foods(text):\n",
    "    # Normalize text to avoid spacing issues\n",
    "    text = \" \".join(text.split())\n",
    "    # Run NER pipeline and extract FOOD entities\n",
    "    ner_results = ner_pipeline(text)\n",
    "    # Filter valid food entities (exclude single letters or fragments)\n",
    "    foods = [result['word'] for result in ner_results if result['entity_group'] == 'FOOD' and len(result['word']) > 1]\n",
    "    # Post-process to merge common multi-word foods\n",
    "    merged_foods = []\n",
    "    i = 0\n",
    "    while i < len(foods):\n",
    "        # Check for known multi-word foods\n",
    "        if i < len(foods) - 1 and foods[i].lower() + \" \" + foods[i+1].lower() in nutrient_map:\n",
    "            merged_foods.append(foods[i] + \" \" + foods[i+1])\n",
    "            i += 2\n",
    "        else:\n",
    "            merged_foods.append(foods[i])\n",
    "            i += 1\n",
    "    return merged_foods\n",
    "\n",
    "# Extended dictionary to map foods to key nutrients (case-insensitive)\n",
    "nutrient_map = {\n",
    "    \"blueberries\": \"Antioxidants, Vitamin C, Fiber\",\n",
    "    \"almonds\": \"Vitamin E, Magnesium, Healthy Fats\",\n",
    "    \"salmon\": \"Omega-3 Fatty Acids, Vitamin D, Protein\",\n",
    "    \"spinach\": \"Iron, Vitamin K, Folate\",\n",
    "    \"olive oil\": \"Monounsaturated Fats, Vitamin E, Antioxidants\",\n",
    "    \"broccoli\": \"Vitamin C, Vitamin K, Fiber\",\n",
    "    \"quinoa\": \"Protein, Magnesium, Fiber\",\n",
    "    \"greek yogurt\": \"Protein, Probiotics, Calcium\",\n",
    "    \"oatmeal\": \"Fiber, Iron, Magnesium\",\n",
    "    \"chicken\": \"Protein, Vitamin B6, Niacin\",\n",
    "    \"tomatoes\": \"Vitamin C, Lycopene, Potassium\",\n",
    "    \"cucumber\": \"Hydration, Vitamin K, Antioxidants\",\n",
    "    \"honey\": \"Antioxidants, Natural Sugars\",\n",
    "    \"cream\": \"Calcium, Vitamin A, Saturated Fats\",\n",
    "    \"red wine\": \"Resveratrol, Antioxidants\",\n",
    "    \"almond milk\": \"Calcium, Vitamin E, Low Calories\",\n",
    "    \"coffee\": \"Caffeine, Antioxidants\",\n",
    "    \"sparkling water\": \"Hydration, No Calories\",\n",
    "    \"chicken salad\": \"Protein, Vitamins A and C\",\n",
    "    \"olive oil dressing\": \"Monounsaturated Fats, Vitamin E\",\n",
    "}\n",
    "\n",
    "# Summarize the diary\n",
    "summary = summarize_text(diary_entry)\n",
    "\n",
    "# Extract food entities\n",
    "foods = extract_foods(diary_entry)\n",
    "\n",
    "# Map foods to nutrients (case-insensitive)\n",
    "nutrients = {food: nutrient_map.get(food.lower(), \"Unknown nutrients\") for food in foods}\n",
    "\n",
    "# Pre-compute food/nutrient list with wrapped lines\n",
    "wrapper = textwrap.TextWrapper(width=80, subsequent_indent=\"  \")  # Wrap at 80 characters\n",
    "food_nutrient_lines = [wrapper.fill(f\"- {food}: {nutrients[food]}\") for food in sorted(nutrients)]\n",
    "food_nutrient_text = \"\\n\".join(food_nutrient_lines)\n",
    "\n",
    "# Format the output for readability\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "original_length = len(diary_entry.strip())\n",
    "summary_length = len(summary.strip())\n",
    "\n",
    "# Wrap summary to avoid long lines\n",
    "wrapped_summary = wrapper.fill(f\"- {summary}\")\n",
    "\n",
    "output = f\"\"\"\n",
    "{'=' * 40}\n",
    "Food Diary Analysis\n",
    "{'=' * 40}\n",
    "Summarization Model: facebook/bart-large-cnn\n",
    "NER Model: Dizex/InstaFoodRoBERTa-NER\n",
    "Timestamp: {current_time}\n",
    "Original Length: {original_length} characters\n",
    "Summary Length: {summary_length} characters\n",
    "Food Count: {len(foods)} items\n",
    "\n",
    "Summary:\n",
    "{wrapped_summary}\n",
    "\n",
    "Extracted Foods and Nutrients:\n",
    "{food_nutrient_text}\n",
    "\n",
    "ü•£\n",
    "{'=' * 40}\n",
    "\"\"\"\n",
    "\n",
    "# Print formatted output\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15d7dcd",
   "metadata": {},
   "source": [
    "## 4. Literature Review with Hugging Face API üìö\n",
    "\n",
    "LLM APIs can summarize nutrition research papers for literature reviews. Let‚Äôs use the same Hugging Face API to summarize a study abstract.\n",
    "\n",
    "### 4.1 Summarizing a Nutrition Study\n",
    "\n",
    "We‚Äôll summarize a sample abstract to extract key findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89094a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample nutrition study abstract\n",
    "abstract = \"\"\"\n",
    "The Mediterranean diet, characterized by high intake of fruits, vegetables, whole grains, and olive oil, has been associated with reduced cardiovascular risk and improved cognitive function. This study examined the impact of adherence to the Mediterranean diet on heart disease outcomes in 500 participants over five years. Results showed a 30% reduction in cardiovascular events among high-adherence groups compared to low-adherence groups. Challenges include cultural barriers and cost of fresh produce.\n",
    "\"\"\"\n",
    "\n",
    "# Summarize the abstract\n",
    "lit_summary = summarize_text(abstract)\n",
    "print(f'Literature Summary: {lit_summary} üìù')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51a8390",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **summarize_text**: Reuses the Hugging Face API to condense the abstract into key points.\n",
    "- **Output**: Highlights main findings (e.g., cardiovascular benefits) for quick review.\n",
    "\n",
    "**Exercise 4**: Change `min_length` to 50 in the `summarize_text` call for the abstract. Does it include more details? Why?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Solution</summary>\n",
    "Update the call to:\n",
    "```python\n",
    "lit_summary = summarize_text(abstract, max_length=100, min_length=50)\n",
    "```\n",
    "A higher `min_length` ensures a longer summary, capturing more details like study size or challenges.\n",
    "</details>\n",
    "\n",
    "**Learn More**: For advanced literature reviews, try [OpenAI‚Äôs API](https://openai.com/api/) for deeper contextual analysis (paid)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0cfad2",
   "metadata": {},
   "source": [
    "## 5. Supply Chain Analysis with a Mock LLM API üöö\n",
    "\n",
    "LLM APIs can analyze supply chain logs (e.g., delivery reports) to detect issues or optimize logistics. Since real APIs like OpenAI or xAI are paid, we‚Äôll use a mock API to simulate extracting insights from a supply chain log.\n",
    "\n",
    "### 5.1 Mock Supply Chain Analysis\n",
    "\n",
    "We‚Äôll simulate an LLM extracting key issues from a supply chain log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563beca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock function to simulate an LLM API for supply chain analysis\n",
    "def mock_supply_chain_analysis(log_text):\n",
    "    # Simulate LLM extracting issues (in reality, use Grok or ChatGPT API)\n",
    "    import random\n",
    "    issues = ['Delay in delivery', 'Contamination risk', 'Inventory shortage', 'Transport cost overrun']\n",
    "    confidence = random.uniform(0.7, 0.95)\n",
    "    detected_issue = random.choice(issues)\n",
    "    return {'issue': detected_issue, 'confidence': confidence}\n",
    "\n",
    "# Sample supply chain log\n",
    "supply_log = \"\"\"\n",
    "Delivery of fresh produce delayed by 2 days due to truck breakdown. Spinach showed signs of wilting upon arrival. Inventory levels for tomatoes are critically low. Fuel costs for transport increased by 15% this month.\n",
    "\"\"\"\n",
    "\n",
    "# Analyze the log\n",
    "result = mock_supply_chain_analysis(supply_log)\n",
    "print(f'Supply Chain Issue: {result[\"issue\"]} (Confidence: {result[\"confidence\"]:.2f}) üöõ')\n",
    "\n",
    "# Save result to file\n",
    "with open('supply_chain_result.json', 'w') as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b3326",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **mock_supply_chain_analysis**: Simulates an LLM identifying issues (e.g., delays, contamination) from a log.\n",
    "- **Real Alternative**: Grok‚Äôs API with X integration could provide real-time supply chain insights.\n",
    "\n",
    "**Exercise 5**: Add a new issue (e.g., ‚ÄòSupplier miscommunication‚Äô) to the `issues` list in `mock_supply_chain_analysis`. Test it and check the output.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Solution</summary>\n",
    "Update the issues list:\n",
    "```python\n",
    "issues = ['Delay in delivery', 'Contamination risk', 'Inventory shortage', 'Transport cost overrun', 'Supplier miscommunication']\n",
    "```\n",
    "The function now includes the new issue in its random selection.\n",
    "</details>\n",
    "\n",
    "**Learn More**: For real supply chain analysis, explore [xAI‚Äôs API](https://x.ai/api) for Grok‚Äôs real-time capabilities (paid)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f27e5",
   "metadata": {},
   "source": [
    "## 6. Sensory Analysis with Hugging Face API üç´\n",
    "\n",
    "Sensory analysis involves interpreting taste, texture, or aroma descriptions (e.g., ‚Äúcreamy, nutty chocolate‚Äù). LLMs can classify or summarize sensory data. Let‚Äôs use Hugging Face to summarize a sensory description.\n",
    "\n",
    "### 6.1 Summarizing Sensory Data\n",
    "\n",
    "We‚Äôll summarize a sensory description of a food product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f0b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sensory description\n",
    "sensory_text = \"\"\"\n",
    "The dark chocolate bar has a rich, velvety texture with a deep cocoa flavor. It offers subtle hints of roasted nuts and a slight fruity tang. The finish is smooth with a mild bitterness that lingers pleasantly.\n",
    "\"\"\"\n",
    "\n",
    "# Summarize the sensory description\n",
    "sensory_summary = summarize_text(sensory_text)\n",
    "print(f'Sensory Summary: {sensory_summary} üç´')\n",
    "\n",
    "# Save summary to file\n",
    "with open('sensory_summary.txt', 'w') as f:\n",
    "    f.write(sensory_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995d1822",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **summarize_text**: Uses Hugging Face to condense sensory text into key descriptors (e.g., ‚Äúrich, nutty‚Äù).\n",
    "- **Application**: Useful for product development or consumer feedback analysis.\n",
    "\n",
    "**Exercise 6**: Try a different sensory description (e.g., for a cheese or beverage). Does the summary capture the main flavors? Why or why not?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Solution</summary>\n",
    "Create a new `sensory_text` (e.g., ‚ÄúThe cheddar is sharp, creamy, with a nutty aftertaste‚Äù). The summary should capture key descriptors if the input is clear, but vague inputs may lead to less precise summaries.\n",
    "</details>\n",
    "\n",
    "**Learn More**: For advanced sensory analysis, try [ChatGPT‚Äôs API](https://openai.com/api/) for sentiment or emotion detection (paid)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0395b8e",
   "metadata": {},
   "source": [
    "## 7. Combining Applications: A Nutrition & Food Science Pipeline üõ†Ô∏è\n",
    "\n",
    "Let‚Äôs combine our LLM API calls into a pipeline to process a food diary, summarize a study, analyze a supply chain, and evaluate sensory data.\n",
    "\n",
    "### 7.1 Building the Pipeline\n",
    "\n",
    "We‚Äôll create a function to run all tasks and save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6287e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline function\n",
    "def nutrition_food_science_pipeline(diary, abstract, supply_log, sensory_text):\n",
    "    results = {}\n",
    "    \n",
    "    # Step 1: Summarize food diary\n",
    "    results['diary_summary'] = summarize_text(diary)\n",
    "    \n",
    "    # Step 2: Summarize literature\n",
    "    results['literature_summary'] = summarize_text(abstract)\n",
    "    \n",
    "    # Step 3: Analyze supply chain\n",
    "    results['supply_chain_issue'] = mock_supply_chain_analysis(supply_log)\n",
    "    \n",
    "    # Step 4: Summarize sensory data\n",
    "    results['sensory_summary'] = summarize_text(sensory_text)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run pipeline\n",
    "pipeline_results = nutrition_food_science_pipeline(diary_entry, abstract, supply_log, sensory_text)\n",
    "\n",
    "# Print results\n",
    "print('Pipeline Results:')\n",
    "print(f'Food Diary Summary: {pipeline_results[\"diary_summary\"]} ü•£')\n",
    "print(f'Literature Summary: {pipeline_results[\"literature_summary\"]} üìù')\n",
    "print(f'Supply Chain Issue: {pipeline_results[\"supply_chain_issue\"]} üöõ')\n",
    "print(f'Sensory Summary: {pipeline_results[\"sensory_summary\"]} üç´')\n",
    "\n",
    "# Save pipeline results\n",
    "with open('pipeline_results.json', 'w') as f:\n",
    "    json.dump(pipeline_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f64a5f",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **nutrition_food_science_pipeline**: Combines diary parsing, literature summarization, supply chain analysis, and sensory summarization.\n",
    "- **results**: A dictionary storing outputs from each step.\n",
    "\n",
    "**Exercise 7**: Add a step to the pipeline to count the number of meals in the `diary_summary` (hint: split by punctuation). Print the count in the results.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Solution</summary>\n",
    "Add to the pipeline:\n",
    "```python\n",
    "results['meal_count'] = len(results['diary_summary'].split('.')) - 1  # Subtract 1 for trailing period\n",
    "```\n",
    "Then print:\n",
    "```python\n",
    "print(f'Meal Count: {pipeline_results[\"meal_count\"]}')\n",
    "```\n",
    "This counts sentences in the summary, approximating meal mentions.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0edcc83",
   "metadata": {},
   "source": [
    "## 8. Summary: Your LLM API Toolkit for Nutrition & Food Science üß∞\n",
    "\n",
    "Here‚Äôs what you‚Äôve mastered:\n",
    "\n",
    "- **Food Diary Parsing** ü•£: Summarize dietary logs with Hugging Face.\n",
    "- **Literature Reviews** üìö: Condense research papers for quick insights.\n",
    "- **Supply Chain Analysis** üöö: Detect issues in logistics (mock API).\n",
    "- **Sensory Analysis** üç´: Summarize taste and texture descriptions.\n",
    "- **Pipelines** üõ†Ô∏è: Combine LLM tasks for efficient workflows.\n",
    "\n",
    "**Final Exercise**: Find a real nutrition dataset or paper (e.g., from [USDA FoodData Central](https://fdc.nal.usda.gov/)) and apply one of these LLM API approaches. Share your findings in a short paragraph!\n",
    "\n",
    "**What‚Äôs Next?** Experiment with paid APIs like [Grok](https://x.ai/api) or [ChatGPT](https://openai.com/api/) for advanced tasks, or explore open-source models like DeepSeek for cost-free options. Keep tasting the future of AI in food science! üòÑ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
