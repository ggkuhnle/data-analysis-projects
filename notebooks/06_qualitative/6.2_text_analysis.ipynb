{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "\n",
        "# üìù 6.2 Text Analysis for Qualitative Research\n",
        "\n",
        "This notebook introduces text analysis techniques for qualitative nutrition research, focusing on processing survey responses.\n",
        "\n",
        "**Objectives**:\n",
        "\n",
        "- Preprocess text data using tokenisation and stopword removal.\n",
        "- Perform word frequency analysis and visualisation.\n",
        "- Apply techniques to `food_preferences.txt` to uncover hippo dietary preferences.\n",
        "\n",
        "**Context**: Qualitative analysis of survey data, like hippo food preferences, reveals insights into dietary behaviours, complementing quantitative methods. ü¶õ\n",
        "\n",
        "<details><summary>Fun Fact</summary>\n",
        "Hippos love to express their food preferences, and text analysis helps us decode their crunchy cravings! ü¶õ\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "colab_setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup for Google Colab: Fetch datasets automatically or manually\n",
        "%run ../../bootstrap.py    # installs requirements + editable package\n",
        "\n",
        "import fns_toolkit as fns\n",
        "\n",
        "import pandas as pd\n",
        "import nltk  # For natural language processing\n",
        "import pandas as pd  # For data manipulation\n",
        "from nltk.tokenize import word_tokenize  # For splitting text into words\n",
        "from nltk.corpus import stopwords  # For removing common words\n",
        "from collections import Counter  # For counting word frequencies\n",
        "import matplotlib.pyplot as plt  # For visualization\n",
        "\n",
        "print('Python environment ready.')\n",
        "print('Python environment ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# üìù 6.2 Text Analysis for Qualitative Research\n",
        "\n",
        "This notebook introduces text analysis techniques for qualitative nutrition research, focusing on processing survey responses.\n",
        "\n",
        "**Objectives**:\n",
        "- Preprocess text data using tokenization and stopword removal.\n",
        "- Perform word frequency analysis and visualization.\n",
        "- Apply techniques to `food_preferences.txt` to uncover hippo dietary preferences.\n",
        "\n",
        "**Context**: Qualitative analysis of survey data, like hippo food preferences, reveals insights into dietary behaviours, complementing quantitative methods.\n",
        "\n",
        "<details><summary>Fun Fact</summary>\n",
        "Hippos love to express their food preferences, and text analysis helps us decode their crunchy cravings! ü¶õ\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text analysis environment ready.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt_tab')  # Tokenizer\n",
        "nltk.download('stopwords')  # Stopwords list\n",
        "print('Text analysis environment ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load_data",
      "metadata": {},
      "source": [
        "## Data Preparation\n",
        "\n",
        "Load `food_preferences.txt`, containing 50 hippo survey responses, and preprocess the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of responses: 50\n",
            "Sample response: Hippo H1: I enjoy crunchy carrots.\n"
          ]
        }
      ],
      "source": [
        "# Load survey responses\n",
        "file_path = fns.get_data_path(\"food_preferences\")\n",
        "\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    responses = f.readlines()\n",
        "print(responses[:2])\n",
        "\n",
        "print(f'Number of responses: {len(responses)}')  # Display total responses\n",
        "print(f'Sample response: {responses[0]}')  # Show first response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "preprocess",
      "metadata": {},
      "source": [
        "## Text Preprocessing\n",
        "\n",
        "Tokenize responses, convert to lowercase, and remove stopwords and punctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "preprocess",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample tokens from first response: ['enjoy', 'crunchy', 'carrots']\n"
          ]
        }
      ],
      "source": [
        "# Initialize stopwords\n",
        "stop_words = set(stopwords.words('english')).union({':', '.', 'hippo', 'i'})\n",
        "\n",
        "# Tokenize and clean responses\n",
        "tokens = []\n",
        "for response in responses:\n",
        "    words = word_tokenize(response.lower())  # Convert to lowercase and tokenize\n",
        "    clean_words = [word for word in words if word.isalpha() and word not in stop_words]\n",
        "    tokens.extend(clean_words)\n",
        "\n",
        "print(f'Sample tokens from first response: {tokens[:3]}')  # Show first few tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "frequency",
      "metadata": {},
      "source": [
        "## Word Frequency Analysis\n",
        "\n",
        "Count and visualize the most common words in the responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "frequency",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 words: [('carrots', 15), ('crunchy', 12), ('sweet', 10), ('enjoy', 8), ('greens', 7)]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Count word frequencies\n",
        "word_freq = Counter(tokens)\n",
        "top_words = word_freq.most_common(5)\n",
        "print(f'Top 5 words: {top_words}')\n",
        "\n",
        "# Visualize word frequencies\n",
        "words, counts = zip(*top_words)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(words, counts)\n",
        "plt.title('Top 5 Words in Hippo Food Preferences')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()  # Display bar plot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exercise",
      "metadata": {},
      "source": [
        "## Exercise: Analyze Adjectives\n",
        "\n",
        "Modify the preprocessing to extract only adjectives (e.g., 'crunchy', 'sweet') and count their frequencies. Visualize the top 3 adjectives in a bar plot.\n",
        "\n",
        "**Guidance**:\n",
        "- Use NLTK‚Äôs part-of-speech tagging (`nltk.pos_tag`) with `nltk.download('averaged_perceptron_tagger')`.\n",
        "- Filter for adjectives (POS tag 'JJ').\n",
        "- Create a bar plot of the top 3 adjectives."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "your_answer",
      "metadata": {},
      "source": [
        "**Answer**:\n",
        "\n",
        "My adjective analysis code and results are as follows:\n",
        "\n",
        "```python\n",
        "# Your code here\n",
        "```\n",
        "\n",
        "**Top 3 Adjectives**:\n",
        "\n",
        "- [Adjective 1]: [Count]\n",
        "- [Adjective 2]: [Count]\n",
        "- [Adjective 3]: [Count]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wrap",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "You‚Äôve applied text analysis to uncover dietary preferences from hippo survey responses, revealing key themes like 'crunchy carrots'.\n",
        "\n",
        "**Next Steps**: Apply these skills to your own qualitative datasets or revisit earlier modules for quantitative analysis.\n",
        "\n",
        "**Resources**:\n",
        "- [NLTK Documentation](https://www.nltk.org/)\n",
        "- [Text Analysis Tutorial](https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk)\n",
        "- Repository: [github.com/ggkuhnle/data-analysis-toolkit-FNS](https://github.com/ggkuhnle/data-analysis-toolkit-FNS)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
