{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Frequentist vs Bayesian in Nutrition & Food Science ü•óüìä\n",
    "\n",
    "> **Thesis:** In real nutrition science‚Äîsmall n, messy measurements, meta-analytic prior knowledge, sequential looks at data, multiple outcomes‚Äîthe Bayesian workflow is not just philosophically nicer. It is **operationally superior**: clearer questions, honest uncertainty, principled sequential monitoring, and decisions framed by utility. Frequentist NHST is fine for idealised, single-shot experiments with large samples and fixed protocols. That‚Äôs rarely our world.\n",
    "\n",
    "### What we‚Äôll do\n",
    "1) Set the stage: what Frequentist and Bayesian actually compute.\n",
    "2) Show classic Frequentist traps: dichotomised p-values, misread CIs, optional stopping, multiplicity, and the garden of forking paths.\n",
    "3) Run a small **clinical supplement** example both ways.\n",
    "4) Do the **Bayesian things Frequentists can‚Äôt** (or won‚Äôt): posterior probability of clinically meaningful benefit, ROPE (practical equivalence), decision analysis, prior sensitivity, and sequential monitoring.\n",
    "\n",
    "You‚Äôll leave with code patterns you can drop into real nutrition trials, cohort analyses, and metabolomics experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8690d06b",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## 1) What the two camps actually compute\n",
    "\n",
    "**Frequentist**: Assume a fixed, unknown parameter; reason about hypothetical repetitions of the experiment. A *p*-value is \\(P(\\text{data or more extreme} \\mid H_0)\\). A 95% CI is a random interval that *would* contain the true value in 95% of infinite repetitions. **It is not** the probability the parameter lies in that interval. NHST then dichotomises evidence at an arbitrary threshold.\n",
    "\n",
    "**Bayesian**: Treat the parameter as uncertain; update beliefs with data. Compute a **posterior** \\(p(\\theta\\mid\\text{data})\\). A 95% **credible interval** *is* the set that contains the parameter with 95% probability. You can ask the question you actually care about: \\(P(\\text{effect} < -0.3\\,\\text{mmol/L} \\mid \\text{data})\\), or expected utility.\n",
    "\n",
    "**Nutrition reality check**: Small n (pilot trials), multiple endpoints (biomarkers), prior literature (meta-analyses), interim looks (DSMBs), heterogeneity (subgroups). Which paradigm fits this reality better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3785680b",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## 2) Frequentist failure modes (with receipts)\n",
    "\n",
    "1) **Dichotomisation**: ‚Äúp<0.05 = works, p>0.05 = nothing there.‚Äù The world is graded; decisions aren‚Äôt binary.\n",
    "2) **CI misinterpretation**: 95% CI is *not* a 95% probability statement. Students, reviewers, clinicians routinely misread it that way.\n",
    "3) **Optional stopping**: Peeking at data until p<0.05 **inflates false positives** unless special sequential methods are pre-registered. And almost nobody does that rigorously in small nutrition studies.\n",
    "4) **Multiplicity**: Many outcomes, time points, subgroups ‚Üí p-values explode without careful correction; power collapses. People cherry-pick.\n",
    "5) **No mechanism to use prior knowledge**: Meta-analytic effect sizes? Mechanistic priors? With NHST, they stay on the sidelines.\n",
    "\n",
    "Bayesian analysis **solves all five** in a single, coherent calculus: posterior probabilities about quantities of interest, with built-in sequential updating and explicit priors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36505ca8",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr />\n",
    "## 3) A simple nutrition RCT (supplement ‚Üí fasting glucose change)\n",
    "Two arms, 25 per arm.\n",
    "- Control mean change: 0 (SD 0.3)\n",
    "- Treatment mean change: ‚àí0.4 mmol/L (SD 0.3)\n",
    "- Prior knowledge: meta-analysis suggests ‚âà ‚àí0.5 ¬± 0.2 mmol/L benefit.\n",
    "\n",
    "We‚Äôll run both **Frequentist t-test** and a **Bayesian model**; then we‚Äôll ask the actually useful questions: *What‚Äôs the probability the effect is at least ‚àí0.3? What is the expected utility if we treat everyone?* We‚Äôll also do a **prior sensitivity** check and a quick **sequential monitoring** demo that shows why peeking doesn‚Äôt break Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d199ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from scipy.stats import ttest_ind, norm\n",
    "import pymc as pm, arviz as az\n",
    "sns.set_style('whitegrid'); np.random.seed(11088)\n",
    "\n",
    "# Simulate trial\n",
    "n = 25\n",
    "control = np.random.normal(0.0, 0.3, size=n)\n",
    "treat   = np.random.normal(-0.4, 0.3, size=n)\n",
    "df = pd.DataFrame({\n",
    "    'group': ['Control']*n + ['Treatment']*n,\n",
    "    'delta_glucose': np.r_[control, treat]\n",
    "})\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(data=df, x='group', y='delta_glucose'); plt.title('Œî Fasting Glucose by Group')\n",
    "plt.xlabel(''); plt.ylabel('mmol/L'); plt.tight_layout(); plt.show()\n",
    "\n",
    "diff = treat.mean() - control.mean()\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28077f5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Frequentist t-test and CI\n",
    "Note how we must **contort** interpretation to avoid saying what everyone wants to say (‚Äúthere‚Äôs a 95% chance the true effect is in the interval‚Ä¶‚Äù). That sentence is **Bayesian**; it is **not** a valid Frequentist statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc547c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Two-sample t-test\n",
    "t_stat, p = ttest_ind(treat, control, equal_var=True)\n",
    "m_t, m_c = treat.mean(), control.mean()\n",
    "se = np.sqrt(np.var(treat, ddof=1)/n + np.var(control, ddof=1)/n)\n",
    "ci = ( (m_t - m_c) - 1.96*se, (m_t - m_c) + 1.96*se )\n",
    "print(f\"t={t_stat:.2f}, p={p:.4f}\\nMean diff (T‚àíC)={m_t-m_c:.2f} mmol/L\\n95% CI: [{ci[0]:.2f}, {ci[1]:.2f}] mmol/L\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972ed9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2 Bayesian model (with prior), and questions that matter\n",
    "- Priors: \\(\\mu_C \\sim N(0, 0.5)\\), \\(\\mu_T \\sim N(-0.5, 0.2)\\), common \\(\\sigma\\sim\\text{HalfNormal}(0.5)\\)\n",
    "- Posterior of \\(\\Delta=\\mu_T-\\mu_C\\).\n",
    "- Ask: \\(P(\\Delta < -0.3\\,|\\,\\text{data})\\) (clinically meaningful benefit) and **ROPE**: \\(P(-0.1 < \\Delta < 0.1)\\) (practical equivalence to no effect)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0ac970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pm.Model() as m:\n",
    "    mu_c = pm.Normal('mu_c', 0, 0.5)\n",
    "    mu_t = pm.Normal('mu_t', -0.5, 0.2)\n",
    "    sigma = pm.HalfNormal('sigma', 0.5)\n",
    "    pm.Normal('y_c', mu_c, sigma, observed=control)\n",
    "    pm.Normal('y_t', mu_t, sigma, observed=treat)\n",
    "    delta = pm.Deterministic('delta', mu_t - mu_c)\n",
    "    idata = pm.sample(1500, tune=1500, target_accept=0.9, chains=4, random_seed=11088, return_inferencedata=True)\n",
    "\n",
    "az.plot_posterior(idata, var_names=['delta'], ref_val=0.0)\n",
    "plt.title('Posterior of Treatment Effect (Œî mmol/L)'); plt.tight_layout(); plt.show()\n",
    "\n",
    "post = idata.posterior['delta'].values.reshape(-1)\n",
    "p_benefit = (post < -0.3).mean()\n",
    "p_equiv = ((post>-0.1) & (post<0.1)).mean()\n",
    "print(f\"P(Œî < -0.3 | data) = {p_benefit:.3f}\")\n",
    "print(f\"P(|Œî| < 0.1 | data) (ROPE) = {p_equiv:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d4a106",
   "metadata": {
    "tags": []
   },
   "source": [
    "**This is the point:** you just computed the probability your effect is clinically meaningful, and the probability it‚Äôs practically negligible. No contortions. No hypotheticals about infinite repetitions. Exactly the question clinicians ask.\n",
    "\n",
    "#### Prior sensitivity (responsible Bayes)\n",
    "Critics say priors are subjective. We agree‚Äîso show robustness. Refit with a **skeptical** prior \\(\\mu_T\\sim N(0,0.5)\\) and a **weakly informative** prior \\(\\mu_T\\sim N(-0.5,0.5)\\), and compare \\(P(\\Delta<-0.3)\\). If conclusions are stable, you‚Äôve earned credibility. If not, you‚Äôve learned that the data don‚Äôt dominate (useful in itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab13e3d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def posterior_prob_delta(treat, control, prior_mu_t, prior_sd_t):\n",
    "    with pm.Model() as m2:\n",
    "        mu_c = pm.Normal('mu_c', 0, 0.5)\n",
    "        mu_t = pm.Normal('mu_t', prior_mu_t, prior_sd_t)\n",
    "        sigma = pm.HalfNormal('sigma', 0.5)\n",
    "        pm.Normal('y_c', mu_c, sigma, observed=control)\n",
    "        pm.Normal('y_t', mu_t, sigma, observed=treat)\n",
    "        delta = pm.Deterministic('delta', mu_t - mu_c)\n",
    "        id_ = pm.sample(1200, tune=1200, chains=4, target_accept=0.9, random_seed=11088, return_inferencedata=True)\n",
    "    post = id_.posterior['delta'].values.reshape(-1)\n",
    "    return (post < -0.3).mean()\n",
    "\n",
    "for name, mu, sd in [\n",
    "    ('Informed', -0.5, 0.2),\n",
    "    ('Weakly inf', -0.5, 0.5),\n",
    "    ('Skeptical',  0.0, 0.5)\n",
    "]:\n",
    "    print(name, posterior_prob_delta(treat, control, mu, sd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d033aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr />\n",
    "## 4) Optional stopping: why Bayes stays sane and NHST doesn‚Äôt\n",
    "Optional stopping (peek every 5 participants, stop when p<0.05) **inflates false positives** for standard t-tests. In contrast, **Bayesian updating** does not require correction: your posterior is your posterior, given the data, regardless of when you looked‚Äî*provided* your model is correct and you report the full posterior (don‚Äôt cherry-pick stopping rules to hack a decision threshold without a decision model).\n",
    "\n",
    "Let‚Äôs simulate under **no true effect** (Œî=0) and see how often we falsely declare success with optional stopping (Frequentist) vs. a Bayesian decision rule based on \\(P(\\Delta<‚àí0.3)>0.95\\). (Small simulation to keep runtime reasonable.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b00f5c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def frequentist_optional_stopping(alpha=0.05, max_n=80, step=5, reps=300):\n",
    "    fp=0\n",
    "    for _ in range(reps):\n",
    "        c = np.random.normal(0,0.3,size=max_n)\n",
    "        t = np.random.normal(0,0.3,size=max_n)  # no true effect\n",
    "        decided=False\n",
    "        for n in range(step, max_n+1, step):\n",
    "            p = ttest_ind(t[:n], c[:n], equal_var=True).pvalue\n",
    "            if p<alpha:\n",
    "                fp+=1; decided=True; break\n",
    "        # if never crossed, no FP counted\n",
    "    return fp/reps\n",
    "\n",
    "def bayes_sequential(rule_p=0.95, thresh=-0.3, max_n=80, step=5, reps=150):\n",
    "    fp=0\n",
    "    for _ in range(reps):\n",
    "        c_full = np.random.normal(0,0.3,size=max_n)\n",
    "        t_full = np.random.normal(0,0.3,size=max_n)\n",
    "        decided=False\n",
    "        for n in range(step, max_n+1, step):\n",
    "            c = c_full[:n]; t = t_full[:n]\n",
    "            with pm.Model() as m:\n",
    "                mu_c = pm.Normal('mu_c', 0, 1)\n",
    "                mu_t = pm.Normal('mu_t', 0, 1)\n",
    "                sigma = pm.HalfNormal('sigma', 1)\n",
    "                pm.Normal('y_c', mu_c, sigma, observed=c)\n",
    "                pm.Normal('y_t', mu_t, sigma, observed=t)\n",
    "                delta = pm.Deterministic('delta', mu_t-mu_c)\n",
    "                id_ = pm.sample(600, tune=600, chains=2, target_accept=0.9, progressbar=False, random_seed=123)\n",
    "            post = id_.posterior['delta'].values.reshape(-1)\n",
    "            if (post < thresh).mean()>rule_p:\n",
    "                # Declared benefit under null -> false positive\n",
    "                fp+=1; decided=True; break\n",
    "        # if no decision, no FP counted\n",
    "    return fp/reps\n",
    "\n",
    "print('Frequentist FP rate (optional stopping):', frequentist_optional_stopping())\n",
    "print('Bayesian FP rate (sequential rule):     ', bayes_sequential())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e81dd5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "> Expect the NHST false positive rate to **inflate above 0.05** with optional stopping. The Bayesian sequential rule stays near its nominal behaviour because decisions are based on posterior probabilities rather than a procedure tuned to a single look.\n",
    "\n",
    "**Important nuance**: If you turn posterior thresholds into rigid stop/go rules without a pre-specified utility, you‚Äôre doing decision theory casually. The solution is not to abandon Bayes; it‚Äôs to **add** explicit utilities (see next)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792d2ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr />\n",
    "## 5) Decision analysis: expected utility beats p-values\n",
    "Suppose:\n",
    "- Benefit: a clinically meaningful reduction is \\(< -0.3\\) mmol/L, worth +10 utility units if true.\n",
    "- Cost: adopting an ineffective supplement costs ‚àí3 units (money, side-effects, opportunity).\n",
    "\n",
    "Bayesian decision: compute expected utility \\(EU = 10\\cdot P(\\Delta<-0.3) - 3\\cdot (1-P(\\Delta<-0.3))\\). Treat if \\(EU>0\\). Try that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9862159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_benefit = (post < -0.3).mean()\n",
    "EU = 10*p_benefit - 3*(1-p_benefit)\n",
    "print(f\"P(benefit)={p_benefit:.3f} ‚Üí Expected utility = {EU:.2f} (treat if >0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d370bab",
   "metadata": {
    "tags": []
   },
   "source": [
    "Frequentist NHST has **no native place to put utility**. People try with power calculations and Type I/II trade-offs, but it‚Äôs a blunt instrument compared to explicit expected loss minimisation on the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e86419c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr />\n",
    "## 6) Multiplicity & the garden of forking paths (mini demo)\n",
    "If you measure 10 biomarkers and test them all at 0.05, you expect **false positives**. If you also try alternative exclusions, transforms, or subgroups, the path you ended up reporting is one of many that were tried. NHST gives you a *p*-value for the chosen path, not the path-finding process.\n",
    "\n",
    "Bayesian route:\n",
    "- Use **hierarchical models** to share strength across outcomes and shrink noisy ones towards the group mean (partial pooling).\n",
    "- Or build a multivariate model with a shared prior. You get calibrated uncertainty *after* considering multiplicity.\n",
    "\n",
    "Below: simulate 10 null biomarkers; count how often any *p*<0.05 (no correction) vs. a simple Bayesian hierarchical model‚Äôs posterior shrinks effects towards 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361b3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "B, n = 10, 30\n",
    "Y = [np.random.normal(0,1, size=n) for _ in range(B)]  # 10 null endpoints\n",
    "pvals = [ttest_ind(Y[i][:n//2], Y[i][n//2:], equal_var=True).pvalue for i in range(B)]\n",
    "any_sig = (np.array(pvals) < 0.05).any()\n",
    "print('Any nominally significant p<0.05 among 10 null endpoints?', any_sig)\n",
    "\n",
    "# Simple hierarchical shrinkage: each endpoint mean theta_i ~ N(mu, tau), observations ~ N(theta_i, sigma)\n",
    "ybar = np.array([np.mean(y[:n//2]) - np.mean(y[n//2:]) for y in Y])  # naive diffs\n",
    "with pm.Model() as hm:\n",
    "    mu = pm.Normal('mu', 0, 1)\n",
    "    tau = pm.HalfNormal('tau', 1)\n",
    "    theta = pm.Normal('theta', mu, tau, shape=B)\n",
    "    sigma = pm.HalfNormal('sigma', 1)\n",
    "    pm.Normal('obs', theta, sigma, observed=ybar)\n",
    "    idh = pm.sample(1500, tune=1500, chains=4, target_accept=0.9, random_seed=11088, return_inferencedata=True)\n",
    "shrunken = idh.posterior['theta'].mean(dim=['chain','draw']).values\n",
    "pd.DataFrame({'raw_diff':ybar, 'shrunken':shrunken})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6ac5d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "Hierarchical Bayes **shrinks** noisy effects back toward the grand mean, taming multiplicity by modelling it‚Äîno Bonferroni carpet-bombing, no underpowered chaos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504df718",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr />\n",
    "## 7) What about Bayes factors?\n",
    "You can report Bayes factors (evidence ratios for H‚ÇÅ vs H‚ÇÄ). They can be sensitive to prior width on the effect under H‚ÇÅ, so they‚Äôre best used with **pre-registered** priors. In clinical nutrition, **posterior probabilities of clinically meaningful effect** plus **decision analysis** usually communicate better than a naked evidence ratio. Use what your audience understands and rewards.\n",
    "\n",
    "## 8) What to put in your paper (or SOP)\n",
    "- **Model**: likelihood, priors (with sensitivity), and rationale grounded in prior literature.\n",
    "- **Posterior**: means, credible intervals, and probabilities of clinically relevant regions (e.g., Œî<‚àí0.3).\n",
    "- **Decision**: expected utility or cost‚Äìbenefit threshold.\n",
    "- **Multiplicity**: hierarchical or multivariate structure.\n",
    "- **Sequential**: if you peeked, say so‚Äîand show the posterior after each look (no correction needed).\n",
    "- **Code & seeds**: reproducibility.\n",
    "\n",
    "## 9) Bottom line\n",
    "Frequentist NHST was designed for a world without prior knowledge, with single-shot fixed designs and large n. Nutrition science is the opposite. Bayesian analysis aligns with how we actually think, decide, and iterate.\n",
    "\n",
    "**If you must use NHST** (journal policy, legacy reasons):\n",
    "- Report effect sizes and intervals (avoid p-value worship).\n",
    "- Pre-register endpoints and analysis plan; correct for multiplicity.\n",
    "- Don‚Äôt peek‚Äîor use proper group-sequential methods.\n",
    "\n",
    "**If you can use Bayes** (do it):\n",
    "- Use informative/weakly informative priors from meta-analysis when possible.\n",
    "- Report posterior probabilities of clinically meaningful effects.\n",
    "- Include ROPE, sensitivity, and a simple expected-utility decision.\n",
    "\n",
    "That‚Äôs science you can defend."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
