{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# ðŸ”„ 3.4 Data Transformation\n",
        "\n",
        "In this notebook youâ€™ll learn how to **transform** data so itâ€™s easy to analyse and model. Weâ€™ll work with `hippo_nutrients.csv` and cover filtering, grouping, reshaping (wide/long), pivoting, and numeric transformations such as **log** and **z-score** standardisation. Weâ€™ll also add **density plots** to visualise how transformations change the distribution.\n",
        "\n",
        "## ðŸŽ¯ Objectives\n",
        "- Filter rows and select columns with expressive, reproducible code.\n",
        "- **Aggregate** groups (what it means, when to use it) using `.groupby(...).agg(...)` with clear syntax.\n",
        "- Reshape between **wide** and **long** formats (melt / pivot).\n",
        "- Pivot-tabulate values for quick comparisons.\n",
        "- Create derived variables and apply **log**, **z-score**, **minâ€“max** scaling.\n",
        "- (Bonus) Group-wise standardisation and rolling transforms over time.\n",
        "- Use **density plots** to see how transformations change distributions.\n",
        "\n",
        "## ðŸ“Œ Context\n",
        "Transformation is the ladder from raw data to insight. In nutrition datasets, youâ€™ll often transform by **group** (e.g., nutrient, sex, year) and reshape between **wide** (one row per ID with many columns) and **long** (one row per measurement).\n",
        "\n",
        "> **Wide vs Long**\n",
        "- **Wide**: Each variable has its own column (e.g., `Iron`, `Calcium`, `Vitamin_D`). Good for humans and some models.\n",
        "- **Long (tidy)**: One row per observation; variables are in *columns*, not headers (e.g., a `Nutrient` column and a `Value` column). Preferred for plotting, grouping, modelling.\n",
        "\n",
        "<details><summary>Fun Fact</summary>\n",
        "Transforming data is like a hippo rearranging its snacks â€” same nutrients, better view! ðŸ¦›\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "colab_setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup for Google Colab: Fetch datasets automatically or manually",
        "import os",
        "from google.colab import files",
        "",
        "MODULE = '03_data_handling'",
        "DATASET = 'hippo_nutrients.csv'",
        "BASE_PATH = '/content/data-analysis-projects'",
        "MODULE_PATH = os.path.join(BASE_PATH, 'notebooks', MODULE)",
        "DATASET_PATH = os.path.join('data', DATASET)",
        "",
        "try:",
        "    print('Attempting to clone repository...')",
        "    if not os.path.exists(BASE_PATH):",
        "        !git clone https://github.com/ggkuhnle/data-analysis-projects.git",
        "    print('Setting working directory...')",
        "    os.chdir(MODULE_PATH)",
        "    if os.path.exists(DATASET_PATH):",
        "        print(f'Dataset found: {DATASET_PATH} âœ…')",
        "    else:",
        "        raise FileNotFoundError('Dataset missing after clone.')",
        "except Exception as e:",
        "    print(f'Cloning failed: {e}')",
        "    print('Falling back to manual upload...')",
        "    os.makedirs('data', exist_ok=True)",
        "    uploaded = files.upload()",
        "    if DATASET in uploaded:",
        "        with open(DATASET_PATH, 'wb') as f:",
        "            f.write(uploaded[DATASET])",
        "        print(f'Successfully uploaded {DATASET} âœ…')",
        "    else:",
        "        raise FileNotFoundError(f'Upload failed. Please upload {DATASET}.')",
        "",
        "print('Python environment ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "libs",
      "metadata": {},
      "source": [
        "Add required libraries (pandas, numpy, matplotlib for density plots)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install_libs",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q pandas numpy matplotlib",
        "import pandas as pd, numpy as np",
        "import matplotlib.pyplot as plt",
        "pd.set_option('display.max_columns', 30)",
        "print('Libraries ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load_data",
      "metadata": {},
      "source": [
        "## 1) Load and Inspect\n",
        "Letâ€™s load `hippo_nutrients.csv` and inspect the first rows and data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/hippo_nutrients.csv')",
        "print('Shape:', df.shape)",
        "print('Dtypes:\\n', df.dtypes)",
        "display(df.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "filter_select",
      "metadata": {},
      "source": [
        "## 2) Filtering Rows and Selecting Columns\n",
        "Use **boolean masks** to keep the rows you want, and bracket notation to select columns. This is bread-and-butter transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "filter_select_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: female hippos, Iron only, years â‰¥ 2024",
        "mask = (df['Sex'] == 'F') & (df['Nutrient'] == 'Iron') & (df['Year'] >= 2024)",
        "cols = ['ID', 'Year', 'Nutrient', 'Value', 'Age', 'Sex']",
        "df_female_iron = df.loc[mask, cols].sort_values(['ID', 'Year']).reset_index(drop=True)",
        "display(df_female_iron.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "agg_concept",
      "metadata": {},
      "source": [
        "## 3) Aggregation â€” Concept and `.agg` Syntax\n",
        "**Aggregation** means reducing many rows in a group to **summary values** (e.g., mean iron intake by sex). In pandas, you usually do this with:\n",
        "\n",
        "```python\n",
        "df.groupby(<group-cols>)[<value-col>].agg(<how>)\n",
        "```\n",
        "\n",
        "### What can `<how>` be?\n",
        "1) A **single function**: `'mean'`, `'median'`, `'count'`, `'std'`, or a custom function:\n",
        "```python\n",
        "df.groupby('Nutrient')['Value'].agg('mean')\n",
        "```\n",
        "\n",
        "2) A **list of functions**: returns multiple columns with each statistic:\n",
        "```python\n",
        "df.groupby('Nutrient')['Value'].agg(['mean','median','count'])\n",
        "```\n",
        "\n",
        "3) A **dictionary** mapping column â†’ function(s):\n",
        "```python\n",
        "df.groupby(['Nutrient','Sex']).agg({\n",
        "  'Value': ['mean','median','count'],\n",
        "  'Age': 'mean'  # optional extra summaries\n",
        "})\n",
        "```\n",
        "\n",
        "4) **Named aggregations** (cleaner column names):\n",
        "```python\n",
        "df.groupby(['Nutrient','Sex']).agg(\n",
        "  Mean_Value = ('Value', 'mean'),\n",
        "  Median_Value = ('Value', 'median'),\n",
        "  N = ('Value', 'size')\n",
        ")\n",
        "```\n",
        "\n",
        "### Why aggregate?\n",
        "- To **summarise** groups (e.g., mean intake per nutrient, per sex).\n",
        "- To **compare** across categories (e.g., male vs female, 2024 vs 2025).\n",
        "- To **reduce noise** and **prepare** for plotting or reporting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "groupby",
      "metadata": {},
      "source": [
        "### 3.1 Basic aggregations (mean, median, count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "groupby_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mean Value by Nutrient",
        "mean_by_nutrient = df.groupby('Nutrient')['Value'].mean().sort_values(ascending=False)",
        "display(mean_by_nutrient)",
        "",
        "# Multiple aggregations by Nutrient and Sex (named aggregations for tidy column names)",
        "agg_ns = (",
        "    df.groupby(['Nutrient','Sex'])",
        "      .agg(Mean_Value=('Value','mean'),",
        "           Median_Value=('Value','median'),",
        "           N=('Value','size'))",
        "      .reset_index()",
        ")",
        "display(agg_ns.head(8))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wide_long_intro",
      "metadata": {},
      "source": [
        "## 4) Reshaping Between Wide and Long\n",
        "Many real datasets arrive in **wide** form; most analyses and plots like **long (tidy)** form.\n",
        "\n",
        "### Long (tidy) â†’ one row per observation\n",
        "- Columns: `ID`, `Year`, `Age`, `Sex`, `Nutrient`, `Value`\n",
        "\n",
        "### Wide â†’ columns are variables (e.g., one column per nutrient)\n",
        "- Columns: `ID`, `Year`, `Age`, `Sex`, `Iron`, `Calcium`, `Vitamin_D`, ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wide_from_long_explain",
      "metadata": {},
      "source": [
        "### 4.1 Long â†’ Wide (pivot wider)\n",
        "Use `pivot_table` with `index` = identifier columns and `columns` = the variable you want as new columns (`Nutrient`). Values come from `Value`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pivot_wider",
      "metadata": {},
      "outputs": [],
      "source": [
        "id_cols = ['ID','Year','Age','Sex']",
        "wide = df.pivot_table(index=id_cols, columns='Nutrient', values='Value', aggfunc='mean')",
        "wide = wide.reset_index()",
        "display(wide.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "long_from_wide_explain",
      "metadata": {},
      "source": [
        "### 4.2 Wide â†’ Long (melt)\n",
        "Use `melt` to gather nutrient columns back into two columns: `Nutrient` (variable name) and `Value` (measurement). This is the canonical *tidy* structure for plotting, modelling, and faceting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "melt_long",
      "metadata": {},
      "outputs": [],
      "source": [
        "value_vars = [c for c in wide.columns if c not in id_cols]",
        "long_again = wide.melt(id_vars=id_cols, value_vars=value_vars,",
        "                      var_name='Nutrient', value_name='Value')",
        "display(long_again.head(6))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pivot_tables",
      "metadata": {},
      "source": [
        "## 5) Pivot Tables for Comparisons\n",
        "Cross-tabulate means by two dimensions (e.g., `Nutrient` Ã— `Year`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pivot_table_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "pt = df.pivot_table(values='Value', index='Nutrient', columns='Year', aggfunc='mean')",
        "display(pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "derived_vars",
      "metadata": {},
      "source": [
        "## 6) Derived Variables and Numeric Transformations\n",
        "Real analyses often need **transformed** variables to meet model assumptions or to compare across scales.\n",
        "\n",
        "### 6.1 Log transform\n",
        "Use when data are **right-skewed** (e.g., highly variable intakes). Add a small `Îµ` to avoid `log(0)`.\n",
        "\n",
        "### 6.2 z-score standardisation\n",
        "Convert values to standard units: `(x - mean) / std`. Helpful when combining variables on different scales.\n",
        "\n",
        "### 6.3 Minâ€“max scaling (0â€“1)\n",
        "Maps the minimum to 0 and maximum to 1. Useful for comparability in dashboards.\n",
        "\n",
        "### 6.4 Group-wise transforms\n",
        "Standardise **within** groups (e.g., within each `Nutrient`) so each nutrientâ€™s distribution is centred and scaled separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "numeric_transforms",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_trans = df.copy()",
        "",
        "# 6.1 Log transform: add epsilon to avoid log(0) if present",
        "eps = 1e-6",
        "df_trans['Value_log'] = np.log(df_trans['Value'] + eps)",
        "",
        "# 6.2 Global z-score",
        "mu = df_trans['Value'].mean()",
        "sd = df_trans['Value'].std(ddof=0)",
        "df_trans['Value_z'] = (df_trans['Value'] - mu) / (sd if sd else 1.0)",
        "",
        "# 6.3 Minâ€“max scaling",
        "vmin, vmax = df_trans['Value'].min(), df_trans['Value'].max()",
        "rng = vmax - vmin if vmax > vmin else 1.0",
        "df_trans['Value_minmax'] = (df_trans['Value'] - vmin) / rng",
        "",
        "# 6.4 Group-wise z-score by Nutrient",
        "df_trans['Value_z_by_nutrient'] = (",
        "    df_trans.groupby('Nutrient')['Value']",
        "           .transform(lambda s: (s - s.mean()) / (s.std(ddof=0) if s.std(ddof=0) else 1.0))",
        ")",
        "",
        "display(df_trans.head(8))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "density_plots_explain",
      "metadata": {},
      "source": [
        "## 7) Density Plots â€” Visualise Transformations\n",
        "Letâ€™s visualise how the distribution changes after transformations. Weâ€™ll use **histograms with density=True** (a simple density estimate) for one nutrient (e.g., Iron). You should see that the **log transform** often makes a right-skewed distribution more symmetric, and **z-scores** centre the data at 0 with unit variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "density_plots",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose one nutrient to illustrate (fallback to first if Iron absent)",
        "nutrient_to_plot = 'Iron' if 'Iron' in df['Nutrient'].unique() else df['Nutrient'].unique()[0]",
        "sub = df_trans[df_trans['Nutrient'] == nutrient_to_plot].copy()",
        "",
        "print(f'Plotting distributions for nutrient: {nutrient_to_plot}')",
        "",
        "# Original scale density (histogram)",
        "plt.figure()",
        "sub['Value'].plot(kind='hist', bins=20, density=True, edgecolor='black')",
        "plt.title(f'{nutrient_to_plot}: Original Value (density)')",
        "plt.xlabel('Value')",
        "plt.ylabel('Density')",
        "plt.show()",
        "",
        "# Log-transformed density",
        "plt.figure()",
        "sub['Value_log'].plot(kind='hist', bins=20, density=True, edgecolor='black')",
        "plt.title(f'{nutrient_to_plot}: Log(Value) (density)')",
        "plt.xlabel('log(Value + Îµ)')",
        "plt.ylabel('Density')",
        "plt.show()",
        "",
        "# Z-score density",
        "plt.figure()",
        "sub['Value_z'].plot(kind='hist', bins=20, density=True, edgecolor='black')",
        "plt.title(f'{nutrient_to_plot}: Z-score (density)')",
        "plt.xlabel('Z-score')",
        "plt.ylabel('Density')",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rolling",
      "metadata": {},
      "source": [
        "## 8) (Bonus) Time-based and Rolling Transforms\n",
        "For longitudinal data (e.g., the same `ID` measured across `Year`), compute rolling means or deltas. This is illustrative if your dataset has multiple years per ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rolling_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_roll = (",
        "    df.sort_values(['ID','Nutrient','Year'])",
        "      .groupby(['ID','Nutrient'], as_index=False)",
        "      .apply(lambda g: g.assign(",
        "          Value_roll_mean=g['Value'].rolling(window=2, min_periods=1).mean(),",
        "          Value_delta=g['Value'].diff()",
        "      ))",
        ")",
        "display(df_roll.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exercise_1",
      "metadata": {},
      "source": [
        "## ðŸ§ª Exercise 1 â€” Grouped Summary\n",
        "Filter to `Nutrient == 'Vitamin_D'`, then compute **median** `Value` by `Sex` and `Year`. Present the result sorted by `Year` and then `Sex`.\n",
        "\n",
        "**Hints**:\n",
        "- Filter: `df[df['Nutrient'] == 'Vitamin_D']`\n",
        "- Group and aggregate: `.groupby(['Sex','Year'])['Value'].median().reset_index()`\n",
        "- Sort: `.sort_values(['Year','Sex'])`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exercise_1_answer_here",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exercise_2",
      "metadata": {},
      "source": [
        "## ðŸ§ª Exercise 2 â€” Wide â†” Long\n",
        "1) Create a **wide** table with index `[ID, Year]` and columns as nutrients (values = mean `Value`).  \n",
        "2) Convert that wide table **back to long** using `melt` with `Nutrient` and `Value` columns.\n",
        "\n",
        "**Hints**:\n",
        "- `pivot_table(index=['ID','Year'], columns='Nutrient', values='Value', aggfunc='mean')`\n",
        "- `melt(id_vars=['ID','Year'], var_name='Nutrient', value_name='Value')`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exercise_2_answer_here",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exercise_3",
      "metadata": {},
      "source": [
        "## ðŸ§ª Exercise 3 â€” Transformations for Modelling\n",
        "Create a dataframe with these new columns on the **Iron** subset only:\n",
        "- `Value_log_iron` â€” log-transformed value with epsilon.\n",
        "- `Value_z_iron_by_sex` â€” z-score **within Sex**.\n",
        "- `Value_minmax_iron_by_year` â€” minâ€“max scaling **within Year**.\n",
        "\n",
        "**Hints**:\n",
        "- Filter: `(df['Nutrient'] == 'Iron')`\n",
        "- Group-wise transforms with `.groupby(...).transform(...)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exercise_3_answer_here",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wrap",
      "metadata": {},
      "source": [
        "## âœ… Conclusion & Next Steps\n",
        "Youâ€™ve practised the core transformation tools:\n",
        "- Filtering and selecting for clear subsets.\n",
        "- **Aggregation** with `.groupby(...).agg(...)`, including named aggregations and why aggregation matters.\n",
        "- Reshaping between **wide** and **long** for tidy analysis.\n",
        "- Pivot tables for quick comparisons.\n",
        "- Numeric transformations: **log**, **z-score**, **minâ€“max**, and **group-wise** standardisation â€” plus **density plots** to see the changes.\n",
        "\n",
        "ðŸ‘‰ Next: **3.5 Data Aggregation** â€” robust summaries, grouped statistics, and combining pipelines for analysis-ready datasets.\n",
        "\n",
        "**Resources**:\n",
        "- Pandas GroupBy: https://pandas.pydata.org/docs/user_guide/groupby.html\n",
        "- Reshaping (melt/pivot): https://pandas.pydata.org/docs/user_guide/reshaping.html\n",
        "- Working with dtypes: https://pandas.pydata.org/docs/user_guide/basics.html#basics-dtypes"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
