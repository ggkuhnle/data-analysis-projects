{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {
    "tags": []
   },
   "source": [
    "# üß† 5.3 Advanced Bayesian Modelling\n",
    "\n",
    "In this notebook we go beyond basic Bayes and build **hierarchical (multilevel) models** for nutrition data. You‚Äôll see how **partial pooling** stabilises estimates across groups, how **robust likelihoods** handle outliers, and how to do **prior/posterior predictive checks**, **diagnostics**, and **model comparison**.\n",
    "\n",
    "**You will:**\n",
    "- Fit hierarchical models with varying intercepts (and optional slopes).\n",
    "- Compare Normal vs **Student-t** likelihoods for robustness.\n",
    "- Run **prior predictive** and **posterior predictive** checks.\n",
    "- Inspect diagnostics (**R-hat**, **ESS**) and compare models via **WAIC/LOO**.\n",
    "- Use **non-centred** parameterisation for better sampling.\n",
    "\n",
    "<details><summary>When to use hierarchical Bayes?</summary>\n",
    "When you have repeated measures, sites/years/sexes/diets, or any natural grouping where you want to share strength across groups without assuming they‚Äôre identical.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab_setup",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Colab setup: clone repo and locate data\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "MODULE = '05_advanced'\n",
    "BASE_PATH = '/content/data-analysis-projects'\n",
    "MODULE_PATH = os.path.join(BASE_PATH, 'notebooks', MODULE)\n",
    "\n",
    "# We'll read hippo nutrients from the data-handling module\n",
    "DATA_MODULE = '03_data_handling'\n",
    "NUTRIENTS_PATH = os.path.join(BASE_PATH, 'notebooks', DATA_MODULE, 'data', 'hippo_nutrients.csv')\n",
    "\n",
    "try:\n",
    "    print('Attempting to clone repository...')\n",
    "    if not os.path.exists(BASE_PATH):\n",
    "        !git clone https://github.com/ggkuhnle/data-analysis-projects.git\n",
    "    print('Setting working directory...')\n",
    "    os.chdir(MODULE_PATH)\n",
    "    if os.path.exists(NUTRIENTS_PATH):\n",
    "        print(f'Dataset found: {NUTRIENTS_PATH} ‚úÖ')\n",
    "    else:\n",
    "        raise FileNotFoundError('hippo_nutrients.csv missing after clone.')\n",
    "except Exception as e:\n",
    "    print(f'Cloning failed: {e}')\n",
    "    print('You can upload hippo_nutrients.csv manually to notebooks/03_data_handling/data/.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_libs",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -q pandas numpy pymc arviz matplotlib\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "import pymc as pm, arviz as az\n",
    "pd.set_option('display.max_columns', 40)\n",
    "print('Bayesian environment ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üì• Load & Prepare Data\n",
    "We‚Äôll model **Iron** intake as a function of **Sex** (F/M) and **Year** (2024/2025), using partial pooling across groups *(Sex √ó Year)*. Feel free to switch to Calcium/Vitamin_D later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUTRIENTS_PATH = '../03_data_handling/data/hippo_nutrients.csv'  # relative to this notebook\n",
    "df = pd.read_csv(NUTRIENTS_PATH)\n",
    "df = df.dropna(subset=['Nutrient', 'Value', 'Sex', 'Year'])\n",
    "df_iron = df[df['Nutrient']=='Iron'].copy()\n",
    "df_iron['Sex'] = df_iron['Sex'].astype('category')\n",
    "df_iron['Year'] = df_iron['Year'].astype('category')\n",
    "df_iron['group'] = (df_iron['Sex'].astype(str) + '_' + df_iron['Year'].astype(str)).astype('category')\n",
    "print(df_iron[['ID','Nutrient','Year','Sex','Value']].head())\n",
    "print('\\nGroups:', df_iron['group'].cat.categories.tolist())\n",
    "print('Rows:', len(df_iron))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ppc_prior",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üéØ Prior Predictive Check\n",
    "Before seeing data, do our **priors** imply plausible values? We‚Äôll set weakly-informative priors and simulate from the prior predictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prior_pred",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_iron' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assumes df_iron exists with columns: 'Value' (numeric), 'group' (categorical)\u001b[39;00m\n",
      "\u001b[0;32m----> 2\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mdf_iron\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[1;32m      3\u001b[0m group_idx \u001b[38;5;241m=\u001b[39m df_iron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcat\u001b[38;5;241m.\u001b[39mcodes\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[1;32m      4\u001b[0m n_groups \u001b[38;5;241m=\u001b[39m df_iron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcat\u001b[38;5;241m.\u001b[39mcategories\u001b[38;5;241m.\u001b[39msize\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_iron' is not defined"
     ]
    }
   ],
   "source": [
    "# Assumes df_iron exists with columns: 'Value' (numeric), 'group' (categorical)\n",
    "y = df_iron['Value'].values\n",
    "group_idx = df_iron['group'].cat.codes.values\n",
    "n_groups = df_iron['group'].cat.categories.size\n",
    "\n",
    "with pm.Model() as prior_model:\n",
    "    mu_global = pm.Normal('mu_global', mu=8, sigma=2)      # prior mean around ~8\n",
    "    tau_group = pm.HalfNormal('tau_group', sigma=1)         # group SD\n",
    "    mu_group  = pm.Normal('mu_group', mu=mu_global, sigma=tau_group, shape=n_groups)\n",
    "    sigma     = pm.HalfNormal('sigma', sigma=1)\n",
    "\n",
    "    # For prior predictive draws, DO NOT pass observed\n",
    "    y_like = pm.Normal('y_like', mu=mu_group[group_idx], sigma=sigma, shape=y.shape[0])\n",
    "\n",
    "    prior_pred = pm.sample_prior_predictive(\n",
    "        samples=1000,\n",
    "        random_seed=42,\n",
    "        var_names=[\"y_like\"],\n",
    "        return_inferencedata=False,  # return a plain dict for simple access\n",
    "    )\n",
    "\n",
    "# prior_pred[\"y_like\"] has shape (samples, n_obs); flatten for a pooled prior predictive density\n",
    "ypp = np.asarray(prior_pred[\"y_like\"]).ravel()\n",
    "\n",
    "az.plot_dist(ypp, kind='kde')\n",
    "plt.title('Prior Predictive: Iron (Value)')\n",
    "plt.xlabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hier_normal",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üèóÔ∏è Hierarchical Model (Normal likelihood)\n",
    "Partial pooling of **group means** (Sex√óYear). This stabilises small groups by sharing information.\n",
    "\n",
    "- Global mean `mu_global`\n",
    "- Group deviations `mu_group ~ Normal(mu_global, tau_group)`\n",
    "- Observation noise `sigma`\n",
    "\n",
    "<details><summary>Tip: non-centred parameterisation</summary>\n",
    "We‚Äôll use a **non-centred** form to help sampling when groups are weakly informed: `mu_group = mu_global + z * tau_group`, `z ~ Normal(0,1)`.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit_normal",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_raw = df_iron['Value'].values\n",
    "y_mean, y_std = y_raw.mean(), y_raw.std()\n",
    "y = (y_raw - y_mean) / y_std\n",
    "\n",
    "group_idx = df_iron['group'].cat.codes.values\n",
    "coords = {'group': df_iron['group'].cat.categories.tolist()}\n",
    "\n",
    "with pm.Model(coords=coords) as hier_normal:\n",
    "    pm.Data('group_idx', group_idx)\n",
    "\n",
    "    # On standardised scale:\n",
    "    mu_global = pm.Normal('mu_global', mu=0.0, sigma=1.0)      # prior mean near 0 on z-scale\n",
    "    tau_group = pm.HalfNormal('tau_group', sigma=0.5)           # between-group SD (z-scale)\n",
    "    z = pm.Normal('z', mu=0.0, sigma=1.0, dims='group')\n",
    "    mu_group = pm.Deterministic('mu_group', mu_global + z * tau_group, dims='group')\n",
    "\n",
    "    sigma = pm.HalfNormal('sigma', sigma=0.5)                   # within-group SD (z-scale)\n",
    "\n",
    "    y_obs = pm.Normal('y_obs', mu=mu_group[group_idx], sigma=sigma, observed=y)\n",
    "\n",
    "    idata_normal = pm.sample(\n",
    "        draws=2000,\n",
    "        tune=3000,\n",
    "        target_accept=0.95,   # smaller steps; reduce divergences\n",
    "        chains=4,\n",
    "        random_seed=42,\n",
    "        return_inferencedata=True\n",
    "    )\n",
    "\n",
    "    ppc_normal = pm.sample_posterior_predictive(idata_normal, random_seed=42, var_names=['y_obs','mu_group'])\n",
    "\n",
    "# --- Diagnostics ---\n",
    "print(\"Total divergences:\", int(idata_normal.sample_stats['diverging'].sum()))\n",
    "az.plot_energy(idata_normal); plt.show()\n",
    "az.plot_rank(idata_normal, var_names=['mu_global','tau_group','sigma']); plt.show()\n",
    "\n",
    "# --- Summary on z-scale ---\n",
    "display(az.summary(idata_normal, var_names=['mu_global','tau_group','sigma','mu_group'], round_to=2))\n",
    "\n",
    "# --- Optional: back-transform key parameters to original units ---\n",
    "# mu_global_back = mu_global_z * y_std + y_mean\n",
    "mu_global_samples = idata_normal.posterior['mu_global'].values * y_std + y_mean\n",
    "sigma_samples     = idata_normal.posterior['sigma'].values * y_std\n",
    "tau_group_samples = idata_normal.posterior['tau_group'].values * y_std\n",
    "mu_group_samples  = idata_normal.posterior['mu_group'].values * y_std + y_mean\n",
    "\n",
    "print(\"Posterior mean (original units):\")\n",
    "print(\"  mu_global ‚âà\", mu_global_samples.mean())\n",
    "print(\"  sigma     ‚âà\", sigma_samples.mean())\n",
    "print(\"  tau_group ‚âà\", tau_group_samples.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ppc_normal_plot",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Posterior Predictive Check (Normal)\n",
    "Compare simulated `y_rep` to observed `y`. Look for systematic mismatches (spread, tails, skew)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ppc_normal",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hier_normal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mhier_normal\u001b[49m:\n",
      "\u001b[1;32m      2\u001b[0m     ppc_normal \u001b[38;5;241m=\u001b[39m pm\u001b[38;5;241m.\u001b[39msample_posterior_predictive(\n",
      "\u001b[1;32m      3\u001b[0m         idata_normal,\n",
      "\u001b[1;32m      4\u001b[0m         var_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_obs\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n",
      "\u001b[1;32m      5\u001b[0m         random_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n",
      "\u001b[1;32m      6\u001b[0m         return_inferencedata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "\u001b[1;32m      7\u001b[0m     )\n",
      "\u001b[1;32m      9\u001b[0m az\u001b[38;5;241m.\u001b[39mplot_ppc(ppc_normal, data_pairs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_obs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_obs\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hier_normal' is not defined"
     ]
    }
   ],
   "source": [
    "with hier_normal:\n",
    "    ppc_normal = pm.sample_posterior_predictive(\n",
    "        idata_normal,\n",
    "        var_names=[\"y_obs\"],\n",
    "        random_seed=42,\n",
    "        return_inferencedata=True,\n",
    "    )\n",
    "\n",
    "az.plot_ppc(ppc_normal, data_pairs={\"y_obs\": \"y_obs\"})\n",
    "plt.title(\"PPC: Hierarchical Normal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hier_student",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üß± Robust Hierarchical Model (Student-t likelihood)\n",
    "Outliers/severe tails? Use **Student-t** with unknown degrees of freedom `ŒΩ`.\n",
    "\n",
    "- `y_obs ~ StudentT(ŒΩ, mu_group[group_idx], sigma)`\n",
    "- `ŒΩ` with weakly informative prior (e.g., Exponential) to allow heavy tails when needed.\n",
    "\n",
    "<details><summary>Why t?</summary>\n",
    "It down-weights extreme observations relative to Normal, often yielding more stable inferences in messy nutrition data.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit_t",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pm.Model(coords=coords) as hier_t:\n",
    "    pm.Data('group_idx', group_idx)\n",
    "\n",
    "    mu_global = pm.Normal('mu_global', mu=8, sigma=2)\n",
    "    tau_group = pm.HalfNormal('tau_group', sigma=1)\n",
    "    z = pm.Normal('z', mu=0, sigma=1, dims='group')\n",
    "    mu_group = pm.Deterministic('mu_group', mu_global + z * tau_group, dims='group')\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    nu = pm.Exponential('nu', lam=1/10)  # mean 10, reasonably heavy tails allowed\n",
    "\n",
    "    y_obs = pm.StudentT('y_obs', nu=nu, mu=mu_group[group_idx], sigma=sigma, observed=y)\n",
    "\n",
    "    idata_t = pm.sample(1000, tune=1000, target_accept=0.9, chains=4, random_seed=42, return_inferencedata=True)\n",
    "    ppc_t = pm.sample_posterior_predictive(idata_t, random_seed=42)\n",
    "\n",
    "az.summary(idata_t, var_names=['mu_global','tau_group','sigma','nu','mu_group'], round_to=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ppc_t",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with hier_t:\n",
    "    ppc_normal = pm.sample_posterior_predictive(\n",
    "        idata_normal,\n",
    "        var_names=[\"y_obs\"],\n",
    "        random_seed=42,\n",
    "        return_inferencedata=True,\n",
    "    )\n",
    "\n",
    "az.plot_ppc(ppc_normal, data_pairs={\"y_obs\": \"y_obs\"})\n",
    "plt.title(\"PPC: Hierarchical Normal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostics",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üß™ Diagnostics & Model Comparison\n",
    "Check **R-hat** (~1.00), **ESS** (large), and compare models by **WAIC/LOO**. Lower is better (penalises complexity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diag_compare",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- helper: ensure log_likelihood is attached (PyMC‚â•5 API) ---\n",
    "def ensure_loglik(idata, model):\n",
    "    if hasattr(idata, \"log_likelihood\"):\n",
    "        return idata\n",
    "    with model:\n",
    "        ll = pm.compute_log_likelihood(idata)\n",
    "    out = idata.copy()\n",
    "    out.extend(ll)\n",
    "    return out\n",
    "\n",
    "# Attach log-likelihoods if needed\n",
    "idata_normal_ll = ensure_loglik(idata_normal, hier_normal)\n",
    "idata_t_ll      = ensure_loglik(idata_t,      hier_t)\n",
    "\n",
    "# Choose variables to report\n",
    "vars_normal = [\"mu_global\", \"tau_group\", \"sigma\", \"mu_group\"]\n",
    "vars_t      = [\"mu_global\", \"tau_group\", \"sigma\", \"nu\", \"mu_group\"]\n",
    "\n",
    "# --- R-hat & ESS tables ---\n",
    "print(\"R-hat & ESS ‚Äî Normal model\")\n",
    "summ_norm = az.summary(idata_normal_ll, var_names=vars_normal, round_to=2)\n",
    "display(summ_norm[[\"r_hat\", \"ess_bulk\", \"ess_tail\"]])\n",
    "\n",
    "print(\"\\nR-hat & ESS ‚Äî Student-t model\")\n",
    "summ_t = az.summary(idata_t_ll, var_names=vars_t, round_to=2)\n",
    "display(summ_t[[\"r_hat\", \"ess_bulk\", \"ess_tail\"]])\n",
    "\n",
    "# If you want arrays directly:\n",
    "# rhat_norm = az.rhat(idata_normal_ll)\n",
    "# essb_norm = az.ess(idata_normal_ll, method=\"bulk\")\n",
    "# esst_norm = az.ess(idata_normal_ll, method=\"tail\")\n",
    "\n",
    "# --- Model comparison (lower is better) ---\n",
    "print(\"\\nModel comparison (WAIC, BB-pseudo-BMA)\")\n",
    "cmp_waic = az.compare(\n",
    "    {\"normal\": idata_normal_ll, \"student_t\": idata_t_ll},\n",
    "    ic=\"waic\",\n",
    "    method=\"BB-pseudo-BMA\",\n",
    ")\n",
    "display(cmp_waic)\n",
    "\n",
    "print(\"\\nModel comparison (LOO, BB-pseudo-BMA)\")\n",
    "cmp_loo = az.compare(\n",
    "    {\"normal\": idata_normal_ll, \"student_t\": idata_t_ll},\n",
    "    ic=\"loo\",\n",
    "    method=\"BB-pseudo-BMA\",\n",
    ")\n",
    "display(cmp_loo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrasts",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üéØ Group Contrasts (Posterior Differences)\n",
    "You often want contrasts like **Treatment vs Control** or **F_2025 ‚àí F_2024**. We‚Äôll compute differences between selected `mu_group` levels from the robust model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrast_code",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "groups = df_iron['group'].cat.categories.tolist()\n",
    "print('Groups:', groups)\n",
    "gmap = {g:i for i,g in enumerate(groups)}\n",
    "\n",
    "mu_post = idata_t.posterior['mu_group']  # dims: chain, draw, group\n",
    "\n",
    "# Example contrasts: (F_2025 - F_2024) and (M_2025 - M_2024) if present\n",
    "def posterior_diff(mu, g1, g0):\n",
    "    return (mu.sel(group=g1) - mu.sel(group=g0)).stack(sample=('chain','draw')).values\n",
    "\n",
    "contrast_results = {}\n",
    "if 'F_2025' in groups and 'F_2024' in groups:\n",
    "    d = posterior_diff(mu_post, 'F_2025', 'F_2024')\n",
    "    contrast_results['F_2025 - F_2024'] = d\n",
    "if 'M_2025' in groups and 'M_2024' in groups:\n",
    "    d = posterior_diff(mu_post, 'M_2025', 'M_2024')\n",
    "    contrast_results['M_2025 - M_2024'] = d\n",
    "\n",
    "for name, d in contrast_results.items():\n",
    "    az.plot_posterior(d, ref_val=0)\n",
    "    plt.title(f'Posterior Contrast: {name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying_slope",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üìà (Optional) Varying Intercepts & Slopes\n",
    "If you have a continuous covariate (e.g., **Age** or **BodyWeight_kg**), allow group-specific slopes:\n",
    "\n",
    "```\n",
    "mu_group = mu_global + z_inter[group]*tau_inter\n",
    "beta_group = beta_global + z_slope[group]*tau_slope\n",
    "y ~ Normal(mu_group[group_idx] + beta_group[group_idx]*x, sigma)\n",
    "```\n",
    "\n",
    "<details><summary>Learn more</summary>\n",
    "- PyMC docs: https://www.pymc.io/\n",
    "- ArviZ model comparison: https://python.arviz.org/\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üß© Exercises\n",
    "1. **Switch nutrient**: Fit the same hierarchical models for **Calcium**. Compare WAIC/LOO to decide between Normal vs Student-t.\n",
    "2. **Add covariate**: If your dataset has `Age` or `BodyWeight_kg`, build a varying-slopes model. Does partial pooling shrink extreme slopes?\n",
    "3. **Prior sensitivity**: Widen/narrow `tau_group` prior and re-run. How do group means change?\n",
    "4. **Predict new group**: Add a new year with few observations and inspect how partial pooling stabilises its estimate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrap",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ‚úÖ Wrap-up\n",
    "You built hierarchical models, used robust likelihoods, checked priors/posteriors, validated diagnostics, and compared models. These tools generalise well to multi-site nutrition studies, repeated measures, and small-sample subgroups.\n",
    "\n",
    "<details><summary>Further reading</summary>\n",
    "- Gelman et al., *Bayesian Data Analysis*\n",
    "- PyMC docs: https://www.pymc.io/\n",
    "- ArviZ: https://python.arviz.org/\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}