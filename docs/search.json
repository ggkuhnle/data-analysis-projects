[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis Toolkit for Food and Nutrition Sciences",
    "section": "",
    "text": "Data Analysis Toolkit for Food and Nutrition Sciences 🦛\nWelcome to the Data Analysis Toolkit for Food and Nutrition Sciences, a comprehensive resource for MSc students mastering data analysis in nutrition research. This toolkit features 27 Jupyter notebooks across six modules, rendered as interactive HTML tutorials using Quarto and hosted on GitHub Pages. Run the notebooks in Google Colab with one click or explore the rendered tutorials below. With hippo-themed datasets (🦛), it covers Python basics, data handling, statistical analysis, advanced methods, and qualitative research.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Analysis Toolkit for Food and Nutrition Sciences</span>"
    ]
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "Data Analysis Toolkit for Food and Nutrition Sciences",
    "section": "Quick Start 🚀",
    "text": "Quick Start 🚀\n\nView the Site: Browse tutorials at https://ggkuhnle.github.io/data-analysis-toolkit-FNS/.\nRun in Colab: Use the “Open in Colab” badges below to run notebooks in the cloud.\nSyllabus: See the full course outline at syllabus.html.\nGitHub: Clone or fork the repo at github.com/ggkuhnle/data-analysis-toolkit-FNS.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Analysis Toolkit for Food and Nutrition Sciences</span>"
    ]
  },
  {
    "objectID": "index.html#modules",
    "href": "index.html#modules",
    "title": "Data Analysis Toolkit for Food and Nutrition Sciences",
    "section": "Modules 📚",
    "text": "Modules 📚\nExplore the six modules, each with notebooks you can view as HTML or run in Colab:\n\nInfrastructure\nSet up Python, Jupyter, and Quarto for data analysis.\nExplore Module\nProgramming Python\nMaster Python syntax and programming basics.\nExplore Module\nData Handling\nImport, clean, and transform nutrition datasets.\nExplore Module\nData Analysis\nVisualise data and build regression models.\nExplore Module\nAdvanced Topics\nDive into Bayesian methods, SQL, and dashboards. Explore Module\nQualitative Research\nAnalyse text data for nutrition studies.\nExplore Module\nMini projects Some mini projects, e.g. RCT Explore Module",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Analysis Toolkit for Food and Nutrition Sciences</span>"
    ]
  },
  {
    "objectID": "index.html#get-involved",
    "href": "index.html#get-involved",
    "title": "Data Analysis Toolkit for Food and Nutrition Sciences",
    "section": "Get Involved 🧑‍💻",
    "text": "Get Involved 🧑‍💻\n\nClone the Repo:\ngit clone https://github.com/ggkuhnle/data-analysis-toolkit-FNS.git\ncd data-analysis-toolkit-FNS\nSet Up Locally:\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\npip install -r requirements.txt\nInstall Quarto: quarto.org.\nRender Locally:\nrm -rf _site/\nquarto render",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Analysis Toolkit for Food and Nutrition Sciences</span>"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Data Analysis Toolkit for Food and Nutrition Sciences",
    "section": "License 📝",
    "text": "License 📝\nCreated by Gunter Kuhnle. Licensed under MIT.\n\nHappy analysing! 🚀",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Analysis Toolkit for Food and Nutrition Sciences</span>"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus: Data Analysis Toolkit for Food and Nutrition Sciences 🦛",
    "section": "",
    "text": "Overview 📚\nThis syllabus outlines the Data Analysis Toolkit for Food and Nutrition Sciences for MSc students. It includes 27 Jupyter notebooks across six modules, rendered as HTML tutorials in _site/notebooks/. Hippo-themed datasets (🦛) teach practical data analysis skills.\nNote: The _site/ directory contains outdated HTMLs. Delete _site/ and run quarto render to generate fresh tutorials.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus: Data Analysis Toolkit for Food and Nutrition Sciences 🦛</span>"
    ]
  },
  {
    "objectID": "syllabus.html#modules-and-notebooks",
    "href": "syllabus.html#modules-and-notebooks",
    "title": "Syllabus: Data Analysis Toolkit for Food and Nutrition Sciences 🦛",
    "section": "🧮 Modules and Notebooks",
    "text": "🧮 Modules and Notebooks\nBelow are the modules, notebooks, expected HTML outputs (after rendering), datasets, and objectives.\n\n1. Infrastructure (5 Notebooks)\nPath: notebooks/01_infrastructure/\nHTMLs: _site/notebooks/infrastructure/\nDataset: notebooks/01_infrastructure/data/hippo_diets.csv (50 rows)\n\n\n\n\n\n\n\n\nNotebook\nExpected HTML\nObjectives\n\n\n\n\n0_getting_started.ipynb\ngetting_started.html\nSet up Python, Jupyter, and Colab.\n\n\n1.1_what_is_data_science_env.ipynb\nwhat_is_data_science_env.html\nUnderstand data science environments.\n\n\n1.2_python_vs_r.ipynb\npython_vs_r.html\nCompare Python and R.\n\n\n1.3_intro_to_git.ipynb\nintro_to_git.html\nLearn Git for version control.\n\n\n1.4_quarto_basics.ipynb\nquarto_basics.html\nCreate reproducible documents.\n\n\n\n\n\n2. Programming Basics (5 Notebooks)\nPath: notebooks/02_programming_basics/\nHTMLs: _site/notebooks/programming_basics/\nDataset: None\n\n\n\n\n\n\n\n\nNotebook\nExpected HTML\nObjectives\n\n\n\n\n2.1_syntax_variables_comments.ipynb\nsyntax_variables_comments.html\nMaster Python syntax.\n\n\n2.2_data_types_conversion.ipynb\ndata_types_conversion.html\nUnderstand data types.\n\n\n2.3_functions_loops.ipynb\nfunctions_loops.html\nUse functions and loops.\n\n\n2.4_data_structures.ipynb\ndata_structures.html\nWork with lists and DataFrames.\n\n\n2.5_oop_basics.ipynb\noop_basics.html\nApply object-oriented programming.\n\n\n\n\n\n3. Data Handling (5 Notebooks)\nPath: notebooks/03_data_handling/\nHTMLs: _site/notebooks/data_handling/\nDataset: notebooks/03_data_handling/data/hippo_nutrients.csv (100 rows)\n\n\n\n\n\n\n\n\nNotebook\nExpected HTML\nObjectives\n\n\n\n\n3.1_what_is_data.ipynb\nwhat_is_data.html\nUnderstand tidy data.\n\n\n3.2_importing_data.ipynb\nimporting_data.html\nImport CSV/Excel files.\n\n\n3.3_data_cleaning.ipynb\ndata_cleaning.html\nHandle missing values.\n\n\n3.4_data_transformation.ipynb\ndata_transformation.html\nFilter and pivot data.\n\n\n3.5_data_aggregation.ipynb\ndata_aggregation.html\nSummarise and join datasets.\n\n\n\n\n\n4. Data Analysis (6 Notebooks)\nPath: notebooks/04_data_analysis/\nHTMLs: _site/notebooks/data_analysis/\nDataset: notebooks/04_data_analysis/data/vitamin_trial.csv (200 rows)\n\n\n\n\n\n\n\n\nNotebook\nExpected HTML\nObjectives\n\n\n\n\n4.1_distributions_visualisation.ipynb\ndistributions_visualisation.html\nVisualise distributions.\n\n\n4.2_exploratory_data_analysis.ipynb\nexploratory_data_analysis.html\nPerform EDA.\n\n\n3_correlation_analysis.ipynb\ncorrelation_analysis.html\nAnalyse correlations.\n\n\n4.4_statistical_testing.ipynb\nstatistical_testing.html\nConduct t-tests and ANOVA.\n\n\n4.5_regression_modelling.ipynb\nregression_modelling.html\nBuild regression models.\n\n\n4.6_logistic_survival.ipynb\n04_logistic_and_survival.html\nApply logistic regression and survival analysis.\n\n\n\n\n\n5. Advanced Topics (5 Notebooks)\nPath: notebooks/05_advanced/\nHTMLs: _site/notebooks/advanced/\nDatasets: notebooks/05_advanced/data/large_food_log.csv (500 rows), notebooks/03_data_handling/data/hippo_nutrients.csv\n\n\n\n\n\n\n\n\nNotebook\nExpected HTML\nObjectives\n\n\n\n\n5.1_bayesian_methods.ipynb\nbayesian_methods.html\nApply Bayesian methods.\n\n\n5.2_workflow_automation.ipynb\nworkflow_automation.html\nAutomate workflows.\n\n\n5.3_advanced_bayesian.ipynb\nadvanced_bayesian.html\nBuild hierarchical models.\n\n\n5.4_databases_sql.ipynb\ndatabases_sql.html\nQuery databases with SQL.\n\n\n5.5_dashboards.ipynb\ndashboards.html\nCreate dashboards.\n\n\n\n\n\n6. Qualitative Research (2 Notebooks)\nPath: notebooks/06_qualitative/\nHTMLs: _site/notebooks/qualitative/\nDataset: notebooks/06_qualitative/data/food_preferences.txt (50 lines)\n\n\n\n\n\n\n\n\nNotebook\nExpected HTML\nObjectives\n\n\n\n\n6.1_intro_qualitative_research.ipynb\nintro_qualitative_research.html\nUnderstand qualitative methods.\n\n\n6.2_text_analysis.ipynb\ntext_analysis.html\nPerform text analysis.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus: Data Analysis Toolkit for Food and Nutrition Sciences 🦛</span>"
    ]
  },
  {
    "objectID": "syllabus.html#learning-path",
    "href": "syllabus.html#learning-path",
    "title": "Syllabus: Data Analysis Toolkit for Food and Nutrition Sciences 🦛",
    "section": "📊 Learning Path",
    "text": "📊 Learning Path\n\nInfrastructure: Set up tools (Weeks 1–2).\nProgramming Basics: Learn Python (Weeks 3–4).\nData Handling: Process data (Weeks 5–6).\nData Analysis: Analyse data (Weeks 7–9).\nAdvanced Topics: Explore advanced methods (Weeks 10–11).\nQualitative Research: Analyse text (Week 12).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus: Data Analysis Toolkit for Food and Nutrition Sciences 🦛</span>"
    ]
  },
  {
    "objectID": "syllabus.html#setup-instructions",
    "href": "syllabus.html#setup-instructions",
    "title": "Syllabus: Data Analysis Toolkit for Food and Nutrition Sciences 🦛",
    "section": "🚀 Setup Instructions",
    "text": "🚀 Setup Instructions\n\nClone the Repository:\ngit clone https://github.com/ggkuhnle/data-analysis-toolkit-FNS.git\nInstall Dependencies:\npip install -r requirements.txt\nInstall Quarto: quarto.org.\nRun Notebooks:\njupyter notebook notebooks/01_infrastructure/0_getting_started.ipynb\nRender the Site:\nrm -rf _site/\nquarto render",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus: Data Analysis Toolkit for Food and Nutrition Sciences 🦛</span>"
    ]
  },
  {
    "objectID": "syllabus.html#why-hippos",
    "href": "syllabus.html#why-hippos",
    "title": "Syllabus: Data Analysis Toolkit for Food and Nutrition Sciences 🦛",
    "section": "🦛 Why Hippos?",
    "text": "🦛 Why Hippos?\nHippo-themed datasets make learning fun and relevant to nutrition science.\n\nCreated by [Your Name]. Licensed under MIT.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus: Data Analysis Toolkit for Food and Nutrition Sciences 🦛</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/index.html",
    "href": "notebooks/01_infrastructure/index.html",
    "title": "Infrastructure",
    "section": "",
    "text": "Infrastructure 🛠️\nThis module guides you through setting up the tools needed for data analysis in nutrition science, using hippo-themed examples (🦛).",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Infrastructure</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/index.html#notebooks",
    "href": "notebooks/01_infrastructure/index.html#notebooks",
    "title": "Infrastructure",
    "section": "Notebooks",
    "text": "Notebooks\n\nGetting Started\nIntroduction to the toolkit and environment setup.\nView HTML | \nWhat is Data Science Environment\nExplore data science environments for nutrition research.\nView HTML | \nPython vs R\nCompare Python and R for data analysis.\nView HTML | \nIntroduction to Git\nLearn Git for version control.\nView HTML | \nQuarto Basics\nSet up Quarto for rendering tutorials.\nView HTML |",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Infrastructure</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/index.html#datasets",
    "href": "notebooks/01_infrastructure/index.html#datasets",
    "title": "Infrastructure",
    "section": "Datasets",
    "text": "Datasets\n\ndata/hippo_diets.csv: Example dataset for setup exercises.",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Infrastructure</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/0_getting_started.html",
    "href": "notebooks/01_infrastructure/0_getting_started.html",
    "title": "👩‍💻 Notebook 0 – Getting Started",
    "section": "",
    "text": "🎯 Objectives\nWelcome to the Data Analysis Toolkit for Food and Nutrition Sciences!\nBefore we dive into exciting topics like nutrient analysis and clinical trials, it’s essential to set up a working environment you can rely on.\nBy the end of this notebook, you will:",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>👩‍💻 Notebook 0 – Getting Started</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/0_getting_started.html#objectives",
    "href": "notebooks/01_infrastructure/0_getting_started.html#objectives",
    "title": "👩‍💻 Notebook 0 – Getting Started",
    "section": "",
    "text": "Understand the different ways you can run Python (locally or online)\nSet up and verify your Python environment\nInstall the required libraries\nTest everything with a simple example",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>👩‍💻 Notebook 0 – Getting Started</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/0_getting_started.html#running-python-your-options",
    "href": "notebooks/01_infrastructure/0_getting_started.html#running-python-your-options",
    "title": "👩‍💻 Notebook 0 – Getting Started",
    "section": "🌍 Running Python: Your Options",
    "text": "🌍 Running Python: Your Options\nThere are two common ways to run Python for data analysis:\n\n🖥️ 1. Local installation\nYou install Python, Jupyter, and other libraries directly on your computer. This gives you full control and works well for advanced users.\n\n\n☁️ 2. Google Colab (Recommended for beginners)\nThis free, web-based platform runs Python in the cloud — no installation required.\n\nIt’s ideal for beginners or anyone working on shared machines (e.g. university PCs).\nAll you need is a Google account.\n\nWe’ll design this toolkit to work seamlessly in Google Colab, but you can also download and run it locally if you prefer.",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>👩‍💻 Notebook 0 – Getting Started</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/0_getting_started.html#what-is-a-python-environment",
    "href": "notebooks/01_infrastructure/0_getting_started.html#what-is-a-python-environment",
    "title": "👩‍💻 Notebook 0 – Getting Started",
    "section": "🔍 What is a Python “environment”?",
    "text": "🔍 What is a Python “environment”?\nA Python environment is a collection of installed tools and packages. Think of it as your lab bench:\n\nPython is the bench itself.\nPackages like pandas, numpy, and matplotlib are your tools.\nYou can create custom environments to keep tools separate for different projects.\n\nColab already provides a pre-configured environment — we’ll just add a few extra tools.\n\n\n🦛\n\nJust like a hippo needs the right waterhole to cool off, you need the right environment to analyse your data comfortably. Let’s get you set up!",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>👩‍💻 Notebook 0 – Getting Started</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/0_getting_started.html#loading-data-different-ways-to-do-it",
    "href": "notebooks/01_infrastructure/0_getting_started.html#loading-data-different-ways-to-do-it",
    "title": "👩‍💻 Notebook 0 – Getting Started",
    "section": "📂 Loading Data: Different Ways to Do It",
    "text": "📂 Loading Data: Different Ways to Do It\nBefore we begin analysing data, we need to load it into our Python environment.\nThere are a few common ways to do this in Colab or Jupyter:\n\n🧳 Option 1: Load from the Internet (Recommended)\nIf your data is stored in a GitHub repository (like this project), you can automatically download and use it in Google Colab.\nThis is great because:\n\nYou don’t need to upload files manually\nEveryone in your group sees the same structure\n\nWe’ll start by trying to clone the whole GitHub repository, just like downloading a suitcase full of datasets and notebooks.\n\n\n📁 Option 2: Upload Manually\nIf cloning fails or you’re using your own file, you can upload it manually from your computer.\nThis is helpful if:\n\nYou’re working with private data\nYou’re just testing out a quick idea\n\n\n\n🦛\n\nThink of it like this:\n\nThe GitHub repository is your shared hippo pantry\nUploading a file manually is like bringing your own snacks\n\n\nWe’ll now run a code cell that first tries the automatic method, and falls back to manual upload if needed.\nDon’t worry — it explains everything along the way!\n\n# Setup for Google Colab: Fetch datasets automatically or manually\n%run ../../bootstrap.py    # installs requirements + editable package\n\nimport fns_toolkit as fns\n\n🔧 Installing requirements…\nRequirement already satisfied: anyio==4.9.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 1)) (4.9.0)\nRequirement already satisfied: appnope==0.1.4 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 2)) (0.1.4)\nRequirement already satisfied: argon2-cffi==23.1.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 3)) (23.1.0)\nRequirement already satisfied: argon2-cffi-bindings==21.2.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 4)) (21.2.0)\nRequirement already satisfied: arrow==1.3.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 5)) (1.3.0)\nRequirement already satisfied: arviz==0.16.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 6)) (0.16.1)\nRequirement already satisfied: asttokens==3.0.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 7)) (3.0.0)\nRequirement already satisfied: async-lru==2.0.5 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 8)) (2.0.5)\nRequirement already satisfied: attrs==25.3.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 9)) (25.3.0)\nRequirement already satisfied: autograd==1.7.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 10)) (1.7.0)\nRequirement already satisfied: autograd-gamma==0.5.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 11)) (0.5.0)\nRequirement already satisfied: babel==2.17.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 12)) (2.17.0)\nRequirement already satisfied: beautifulsoup4==4.13.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 13)) (4.13.3)\nRequirement already satisfied: bleach==6.2.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 14)) (6.2.0)\nRequirement already satisfied: cachetools==5.5.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 15)) (5.5.2)\nRequirement already satisfied: certifi==2025.1.31 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 16)) (2025.1.31)\nRequirement already satisfied: cffi==1.17.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 17)) (1.17.1)\nRequirement already satisfied: charset-normalizer==3.4.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 18)) (3.4.1)\nRequirement already satisfied: cloudpickle==3.1.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 19)) (3.1.1)\nRequirement already satisfied: comm==0.2.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 20)) (0.2.2)\nRequirement already satisfied: cons==0.4.6 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 21)) (0.4.6)\nRequirement already satisfied: contourpy==1.3.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 22)) (1.3.0)\nRequirement already satisfied: cycler==0.12.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 23)) (0.12.1)\nRequirement already satisfied: debugpy==1.8.13 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 24)) (1.8.13)\nRequirement already satisfied: decorator==5.2.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 25)) (5.2.1)\nRequirement already satisfied: defusedxml==0.7.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 26)) (0.7.1)\nRequirement already satisfied: et_xmlfile==2.0.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 27)) (2.0.0)\nRequirement already satisfied: etuples==0.3.9 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 28)) (0.3.9)\nRequirement already satisfied: exceptiongroup==1.2.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 29)) (1.2.2)\nRequirement already satisfied: executing==2.2.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 30)) (2.2.0)\nRequirement already satisfied: fastjsonschema==2.21.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 31)) (2.21.1)\nRequirement already satisfied: fastprogress==1.0.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 32)) (1.0.3)\nRequirement already satisfied: filelock==3.18.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 33)) (3.18.0)\nRequirement already satisfied: fonttools==4.57.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 34)) (4.57.0)\nRequirement already satisfied: formulaic==1.1.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 35)) (1.1.1)\nRequirement already satisfied: fqdn==1.5.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 36)) (1.5.1)\nRequirement already satisfied: h11==0.14.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 37)) (0.14.0)\nRequirement already satisfied: h5netcdf==1.6.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 38)) (1.6.1)\nRequirement already satisfied: h5py==3.13.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 39)) (3.13.0)\nRequirement already satisfied: httpcore==1.0.7 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 40)) (1.0.7)\nRequirement already satisfied: httpx==0.28.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 41)) (0.28.1)\nRequirement already satisfied: idna==3.10 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 42)) (3.10)\nRequirement already satisfied: importlib_metadata==8.6.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 43)) (8.6.1)\nRequirement already satisfied: importlib_resources==6.5.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 44)) (6.5.2)\nRequirement already satisfied: interface-meta==1.3.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 45)) (1.3.0)\nRequirement already satisfied: ipykernel==6.29.5 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 46)) (6.29.5)\nRequirement already satisfied: ipython==8.18.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 47)) (8.18.1)\nRequirement already satisfied: ipywidgets==8.1.6 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 48)) (8.1.6)\nRequirement already satisfied: isoduration==20.11.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 49)) (20.11.0)\nRequirement already satisfied: jedi==0.19.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 50)) (0.19.2)\nRequirement already satisfied: Jinja2==3.1.6 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 51)) (3.1.6)\nRequirement already satisfied: json5==0.12.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 52)) (0.12.0)\nRequirement already satisfied: jsonpointer==3.0.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 53)) (3.0.0)\nRequirement already satisfied: jsonschema==4.23.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 54)) (4.23.0)\nRequirement already satisfied: jsonschema-specifications==2024.10.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 55)) (2024.10.1)\nRequirement already satisfied: jupyter==1.1.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 56)) (1.1.1)\nRequirement already satisfied: jupyter-console==6.6.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 57)) (6.6.3)\nRequirement already satisfied: jupyter-events==0.12.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 58)) (0.12.0)\nRequirement already satisfied: jupyter-lsp==2.2.5 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 59)) (2.2.5)\nRequirement already satisfied: jupyter_client==8.6.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 60)) (8.6.3)\nRequirement already satisfied: jupyter_core==5.7.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 61)) (5.7.2)\nRequirement already satisfied: jupyter_server==2.15.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 62)) (2.15.0)\nRequirement already satisfied: jupyter_server_terminals==0.5.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 63)) (0.5.3)\nRequirement already satisfied: jupyterlab==4.4.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 64)) (4.4.0)\nRequirement already satisfied: jupyterlab_pygments==0.3.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 65)) (0.3.0)\nRequirement already satisfied: jupyterlab_server==2.27.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 66)) (2.27.3)\nRequirement already satisfied: jupyterlab_widgets==3.0.14 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 67)) (3.0.14)\nRequirement already satisfied: kiwisolver==1.4.7 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 68)) (1.4.7)\nRequirement already satisfied: lifelines==0.30.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 69)) (0.30.0)\nRequirement already satisfied: logical-unification==0.4.6 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 70)) (0.4.6)\nRequirement already satisfied: MarkupSafe==3.0.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 71)) (3.0.2)\nRequirement already satisfied: matplotlib==3.9.4 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 72)) (3.9.4)\nRequirement already satisfied: matplotlib-inline==0.1.7 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 73)) (0.1.7)\nRequirement already satisfied: miniKanren==1.0.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 74)) (1.0.3)\nRequirement already satisfied: mistune==3.1.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 75)) (3.1.3)\nRequirement already satisfied: multipledispatch==1.0.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 76)) (1.0.0)\nRequirement already satisfied: nbclient==0.10.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 77)) (0.10.2)\nRequirement already satisfied: nbconvert==7.16.6 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 78)) (7.16.6)\nRequirement already satisfied: nbformat==5.10.4 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 79)) (5.10.4)\nRequirement already satisfied: nest-asyncio==1.6.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 80)) (1.6.0)\nRequirement already satisfied: notebook==7.4.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 81)) (7.4.0)\nRequirement already satisfied: notebook_shim==0.2.4 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 82)) (0.2.4)\nRequirement already satisfied: numpy==1.26.4 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 83)) (1.26.4)\nRequirement already satisfied: openpyxl==3.1.5 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 84)) (3.1.5)\nRequirement already satisfied: overrides==7.7.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 85)) (7.7.0)\nRequirement already satisfied: packaging==24.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 86)) (24.2)\nRequirement already satisfied: pandas==2.2.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 87)) (2.2.3)\nRequirement already satisfied: pandocfilters==1.5.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 88)) (1.5.1)\nRequirement already satisfied: parso==0.8.4 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 89)) (0.8.4)\nRequirement already satisfied: patsy==1.0.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 90)) (1.0.1)\nRequirement already satisfied: pexpect==4.9.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 91)) (4.9.0)\nRequirement already satisfied: pillow==11.1.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 92)) (11.1.0)\nRequirement already satisfied: platformdirs==4.3.7 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 93)) (4.3.7)\nRequirement already satisfied: prometheus_client==0.21.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 94)) (0.21.1)\nRequirement already satisfied: prompt_toolkit==3.0.50 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 95)) (3.0.50)\nRequirement already satisfied: psutil==7.0.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 96)) (7.0.0)\nRequirement already satisfied: ptyprocess==0.7.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 97)) (0.7.0)\nRequirement already satisfied: pure_eval==0.2.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 98)) (0.2.3)\nRequirement already satisfied: pycparser==2.22 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 99)) (2.22)\nRequirement already satisfied: Pygments==2.19.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 100)) (2.19.1)\nRequirement already satisfied: pymc==5.9.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 101)) (5.9.0)\nRequirement already satisfied: pyparsing==3.2.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 102)) (3.2.3)\nRequirement already satisfied: pytensor==2.17.4 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 103)) (2.17.4)\nRequirement already satisfied: python-dateutil==2.9.0.post0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 104)) (2.9.0.post0)\nRequirement already satisfied: python-json-logger==3.3.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 105)) (3.3.0)\nRequirement already satisfied: pytz==2025.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 106)) (2025.2)\nRequirement already satisfied: PyYAML==6.0.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 107)) (6.0.2)\nRequirement already satisfied: pyzmq==26.4.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 108)) (26.4.0)\nRequirement already satisfied: referencing==0.36.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 109)) (0.36.2)\nRequirement already satisfied: requests==2.32.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 110)) (2.32.3)\nRequirement already satisfied: rfc3339-validator==0.1.4 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 111)) (0.1.4)\nRequirement already satisfied: rfc3986-validator==0.1.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 112)) (0.1.1)\nRequirement already satisfied: rpds-py==0.24.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 113)) (0.24.0)\nRequirement already satisfied: scipy==1.11.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 114)) (1.11.3)\nRequirement already satisfied: seaborn==0.13.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 115)) (0.13.2)\nRequirement already satisfied: Send2Trash==1.8.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 116)) (1.8.3)\nRequirement already satisfied: six==1.17.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 117)) (1.17.0)\nRequirement already satisfied: sniffio==1.3.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 118)) (1.3.1)\nRequirement already satisfied: soupsieve==2.6 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 119)) (2.6)\nRequirement already satisfied: stack-data==0.6.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 120)) (0.6.3)\nRequirement already satisfied: statsmodels==0.14.4 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 121)) (0.14.4)\nRequirement already satisfied: tableone==0.9.4 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 122)) (0.9.4)\nRequirement already satisfied: tabulate==0.9.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 123)) (0.9.0)\nRequirement already satisfied: terminado==0.18.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 124)) (0.18.1)\nRequirement already satisfied: tinycss2==1.4.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 125)) (1.4.0)\nRequirement already satisfied: tomli==2.2.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 126)) (2.2.1)\nRequirement already satisfied: toolz==1.0.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 127)) (1.0.0)\nRequirement already satisfied: tornado==6.4.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 128)) (6.4.2)\nRequirement already satisfied: traitlets==5.14.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 129)) (5.14.3)\nRequirement already satisfied: types-python-dateutil==2.9.0.20241206 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 130)) (2.9.0.20241206)\nRequirement already satisfied: typing_extensions==4.13.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 131)) (4.13.1)\nRequirement already satisfied: tzdata==2025.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 132)) (2025.2)\nRequirement already satisfied: uri-template==1.3.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 133)) (1.3.0)\nRequirement already satisfied: urllib3==2.3.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 134)) (2.3.0)\nRequirement already satisfied: wcwidth==0.2.13 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 135)) (0.2.13)\nRequirement already satisfied: webcolors==24.11.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 136)) (24.11.1)\nRequirement already satisfied: webencodings==0.5.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 137)) (0.5.1)\nRequirement already satisfied: websocket-client==1.8.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 138)) (1.8.0)\nRequirement already satisfied: widgetsnbextension==4.0.14 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 139)) (4.0.14)\nRequirement already satisfied: wrapt==1.17.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 140)) (1.17.2)\nRequirement already satisfied: xarray==2024.7.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 141)) (2024.7.0)\nRequirement already satisfied: xarray-einstats==0.7.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 142)) (0.7.0)\nRequirement already satisfied: zipp==3.21.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from -r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 143)) (3.21.0)\nRequirement already satisfied: setuptools&gt;=60.0.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from arviz==0.16.1-&gt;-r /Users/gunter/Documents/data-analysis-toolkit-FNS/requirements.txt (line 6)) (79.0.1)\n🔧 Installing toolkit editable…\nObtaining file:///Users/gunter/Documents/data-analysis-toolkit-FNS\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Checking if build backend supports build_editable: started\n  Checking if build backend supports build_editable: finished with status 'done'\n  Getting requirements to build editable: started\n  Getting requirements to build editable: finished with status 'done'\n  Preparing editable metadata (pyproject.toml): started\n  Preparing editable metadata (pyproject.toml): finished with status 'done'\nRequirement already satisfied: anyio==4.9.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (4.9.0)\nRequirement already satisfied: appnope==0.1.4 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.1.4)\nRequirement already satisfied: argon2-cffi==23.1.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (23.1.0)\nRequirement already satisfied: argon2-cffi-bindings==21.2.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (21.2.0)\nRequirement already satisfied: arrow==1.3.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.3.0)\nRequirement already satisfied: arviz==0.16.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.16.1)\nRequirement already satisfied: asttokens==3.0.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (3.0.0)\nRequirement already satisfied: async-lru==2.0.5 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2.0.5)\nRequirement already satisfied: attrs==25.3.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (25.3.0)\nRequirement already satisfied: autograd==1.7.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.7.0)\nRequirement already satisfied: autograd-gamma==0.5.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.5.0)\nRequirement already satisfied: babel==2.17.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2.17.0)\nRequirement already satisfied: beautifulsoup4==4.13.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (4.13.3)\nRequirement already satisfied: bleach==6.2.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (6.2.0)\nRequirement already satisfied: cachetools==5.5.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (5.5.2)\nRequirement already satisfied: certifi==2025.1.31 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2025.1.31)\nRequirement already satisfied: cffi==1.17.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.17.1)\nRequirement already satisfied: charset-normalizer==3.4.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (3.4.1)\nRequirement already satisfied: cloudpickle==3.1.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (3.1.1)\nRequirement already satisfied: comm==0.2.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.2.2)\nRequirement already satisfied: cons==0.4.6 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.4.6)\nRequirement already satisfied: contourpy==1.3.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.3.0)\nRequirement already satisfied: cycler==0.12.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.12.1)\nRequirement already satisfied: debugpy==1.8.13 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.8.13)\nRequirement already satisfied: decorator==5.2.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (5.2.1)\nRequirement already satisfied: defusedxml==0.7.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.7.1)\nRequirement already satisfied: et_xmlfile==2.0.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2.0.0)\nRequirement already satisfied: etuples==0.3.9 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.3.9)\nRequirement already satisfied: exceptiongroup==1.2.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.2.2)\nRequirement already satisfied: executing==2.2.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2.2.0)\nRequirement already satisfied: fastjsonschema==2.21.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2.21.1)\nRequirement already satisfied: fastprogress==1.0.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.0.3)\nRequirement already satisfied: filelock==3.18.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (3.18.0)\nRequirement already satisfied: fonttools==4.57.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (4.57.0)\nRequirement already satisfied: formulaic==1.1.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.1.1)\nRequirement already satisfied: fqdn==1.5.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.5.1)\nRequirement already satisfied: h11==0.14.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.14.0)\nRequirement already satisfied: h5netcdf==1.6.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.6.1)\nRequirement already satisfied: h5py==3.13.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (3.13.0)\nRequirement already satisfied: httpcore==1.0.7 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.0.7)\nRequirement already satisfied: httpx==0.28.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.28.1)\nRequirement already satisfied: idna==3.10 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (3.10)\nRequirement already satisfied: importlib_metadata==8.6.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (8.6.1)\nRequirement already satisfied: importlib_resources==6.5.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (6.5.2)\nRequirement already satisfied: interface-meta==1.3.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.3.0)\nRequirement already satisfied: ipykernel==6.29.5 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (6.29.5)\nRequirement already satisfied: ipython==8.18.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (8.18.1)\nRequirement already satisfied: ipywidgets==8.1.6 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (8.1.6)\nRequirement already satisfied: isoduration==20.11.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (20.11.0)\nRequirement already satisfied: jedi==0.19.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.19.2)\nRequirement already satisfied: Jinja2==3.1.6 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (3.1.6)\nRequirement already satisfied: json5==0.12.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.12.0)\nRequirement already satisfied: jsonpointer==3.0.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (3.0.0)\nRequirement already satisfied: jsonschema==4.23.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (4.23.0)\nRequirement already satisfied: jsonschema-specifications==2024.10.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2024.10.1)\nRequirement already satisfied: jupyter==1.1.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.1.1)\nRequirement already satisfied: jupyter-console==6.6.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (6.6.3)\nRequirement already satisfied: jupyter-events==0.12.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.12.0)\nRequirement already satisfied: jupyter-lsp==2.2.5 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2.2.5)\nRequirement already satisfied: jupyter_client==8.6.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (8.6.3)\nRequirement already satisfied: jupyter_core==5.7.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (5.7.2)\nRequirement already satisfied: jupyter_server==2.15.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2.15.0)\nRequirement already satisfied: jupyter_server_terminals==0.5.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.5.3)\nRequirement already satisfied: jupyterlab==4.4.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (4.4.0)\nRequirement already satisfied: jupyterlab_pygments==0.3.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.3.0)\nRequirement already satisfied: jupyterlab_server==2.27.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2.27.3)\nRequirement already satisfied: jupyterlab_widgets==3.0.14 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (3.0.14)\nRequirement already satisfied: kiwisolver==1.4.7 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.4.7)\nRequirement already satisfied: lifelines==0.30.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.30.0)\nRequirement already satisfied: logical-unification==0.4.6 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.4.6)\nRequirement already satisfied: MarkupSafe==3.0.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (3.0.2)\nRequirement already satisfied: matplotlib==3.9.4 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (3.9.4)\nRequirement already satisfied: matplotlib-inline==0.1.7 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.1.7)\nRequirement already satisfied: miniKanren==1.0.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.0.3)\nRequirement already satisfied: mistune==3.1.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (3.1.3)\nRequirement already satisfied: multipledispatch==1.0.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.0.0)\nRequirement already satisfied: nbclient==0.10.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.10.2)\nRequirement already satisfied: nbconvert==7.16.6 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (7.16.6)\nRequirement already satisfied: nbformat==5.10.4 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (5.10.4)\nRequirement already satisfied: nest-asyncio==1.6.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.6.0)\nRequirement already satisfied: notebook==7.4.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (7.4.0)\nRequirement already satisfied: notebook_shim==0.2.4 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.2.4)\nRequirement already satisfied: numpy==1.26.4 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.26.4)\nRequirement already satisfied: openpyxl==3.1.5 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (3.1.5)\nRequirement already satisfied: overrides==7.7.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (7.7.0)\nRequirement already satisfied: packaging==24.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (24.2)\nRequirement already satisfied: pandas==2.2.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2.2.3)\nRequirement already satisfied: pandocfilters==1.5.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.5.1)\nRequirement already satisfied: parso==0.8.4 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.8.4)\nRequirement already satisfied: patsy==1.0.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.0.1)\nRequirement already satisfied: pexpect==4.9.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (4.9.0)\nRequirement already satisfied: pillow==11.1.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (11.1.0)\nRequirement already satisfied: platformdirs==4.3.7 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (4.3.7)\nRequirement already satisfied: prometheus_client==0.21.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.21.1)\nRequirement already satisfied: prompt_toolkit==3.0.50 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (3.0.50)\nRequirement already satisfied: psutil==7.0.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (7.0.0)\nRequirement already satisfied: ptyprocess==0.7.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.7.0)\nRequirement already satisfied: pure_eval==0.2.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.2.3)\nRequirement already satisfied: pycparser==2.22 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2.22)\nRequirement already satisfied: Pygments==2.19.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2.19.1)\nRequirement already satisfied: pymc==5.9.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (5.9.0)\nRequirement already satisfied: pyparsing==3.2.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (3.2.3)\nRequirement already satisfied: pytensor==2.17.4 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2.17.4)\nRequirement already satisfied: python-dateutil==2.9.0.post0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2.9.0.post0)\nRequirement already satisfied: python-json-logger==3.3.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (3.3.0)\nRequirement already satisfied: pytz==2025.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2025.2)\nRequirement already satisfied: PyYAML==6.0.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (6.0.2)\nRequirement already satisfied: pyzmq==26.4.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (26.4.0)\nRequirement already satisfied: referencing==0.36.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.36.2)\nRequirement already satisfied: requests==2.32.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2.32.3)\nRequirement already satisfied: rfc3339-validator==0.1.4 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.1.4)\nRequirement already satisfied: rfc3986-validator==0.1.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.1.1)\nRequirement already satisfied: rpds-py==0.24.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.24.0)\nRequirement already satisfied: scipy==1.11.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.11.3)\nRequirement already satisfied: seaborn==0.13.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.13.2)\nRequirement already satisfied: Send2Trash==1.8.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.8.3)\nRequirement already satisfied: six==1.17.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.17.0)\nRequirement already satisfied: sniffio==1.3.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.3.1)\nRequirement already satisfied: soupsieve==2.6 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2.6)\nRequirement already satisfied: stack-data==0.6.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.6.3)\nRequirement already satisfied: statsmodels==0.14.4 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.14.4)\nRequirement already satisfied: tableone==0.9.4 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.9.4)\nRequirement already satisfied: tabulate==0.9.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.9.0)\nRequirement already satisfied: terminado==0.18.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.18.1)\nRequirement already satisfied: tinycss2==1.4.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.4.0)\nRequirement already satisfied: tomli==2.2.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2.2.1)\nRequirement already satisfied: toolz==1.0.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.0.0)\nRequirement already satisfied: tornado==6.4.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (6.4.2)\nRequirement already satisfied: traitlets==5.14.3 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (5.14.3)\nRequirement already satisfied: types-python-dateutil==2.9.0.20241206 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2.9.0.20241206)\nRequirement already satisfied: typing_extensions==4.13.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (4.13.1)\nRequirement already satisfied: tzdata==2025.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2025.2)\nRequirement already satisfied: uri-template==1.3.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.3.0)\nRequirement already satisfied: urllib3==2.3.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2.3.0)\nRequirement already satisfied: wcwidth==0.2.13 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.2.13)\nRequirement already satisfied: webcolors==24.11.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (24.11.1)\nRequirement already satisfied: webencodings==0.5.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.5.1)\nRequirement already satisfied: websocket-client==1.8.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.8.0)\nRequirement already satisfied: widgetsnbextension==4.0.14 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (4.0.14)\nRequirement already satisfied: wrapt==1.17.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (1.17.2)\nRequirement already satisfied: xarray==2024.7.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (2024.7.0)\nRequirement already satisfied: xarray-einstats==0.7.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (0.7.0)\nRequirement already satisfied: zipp==3.21.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from data-analysis-toolkit-fns==2.0.0) (3.21.0)\nRequirement already satisfied: setuptools&gt;=60.0.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from arviz==0.16.1-&gt;data-analysis-toolkit-fns==2.0.0) (79.0.1)\nBuilding wheels for collected packages: data-analysis-toolkit-fns\n  Building editable for data-analysis-toolkit-fns (pyproject.toml): started\n  Building editable for data-analysis-toolkit-fns (pyproject.toml): finished with status 'done'\n  Created wheel for data-analysis-toolkit-fns: filename=data_analysis_toolkit_fns-2.0.0-0.editable-py3-none-any.whl size=6134 sha256=e6f7582bed6685dc425a7bd62f4e38e0fba383877033c13dcd565dedc82f50c4\n  Stored in directory: /private/var/folders/jr/p60s3gd574d_f62sc7_0bzfm0000gq/T/pip-ephem-wheel-cache-bj30ofov/wheels/d9/61/00/78f2734b520b566ba6cc685a244c16337951b4a6f9f27b2c3d\nSuccessfully built data-analysis-toolkit-fns\nInstalling collected packages: data-analysis-toolkit-fns\n  Attempting uninstall: data-analysis-toolkit-fns\n    Found existing installation: data-analysis-toolkit-fns 2.0.0\n    Uninstalling data-analysis-toolkit-fns-2.0.0:\n      Successfully uninstalled data-analysis-toolkit-fns-2.0.0\nSuccessfully installed data-analysis-toolkit-fns-2.0.0\n✅ Environment ready!",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>👩‍💻 Notebook 0 – Getting Started</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/0_getting_started.html#installing-and-using-python-packages",
    "href": "notebooks/01_infrastructure/0_getting_started.html#installing-and-using-python-packages",
    "title": "👩‍💻 Notebook 0 – Getting Started",
    "section": "📦 Installing and Using Python Packages",
    "text": "📦 Installing and Using Python Packages\nPython is powerful, but it doesn’t come with everything built-in.\nThat’s where packages come in — they’re like apps you install to give Python superpowers!\n\n\n🛠️ What is a package?\nA package is a collection of code written by others that you can reuse in your own projects.\nThink of them as:\n\n🧰 Specialised tools you add to your data analysis workbench\n📚 Cheat codes that help you do complex things with just a few lines\n\n\n\n\n📦 In this notebook, we’ll install and use:\n\npandas – Makes working with data tables easy, like using a spreadsheet in Python\nnumpy – Adds powerful maths and statistics tools (great for calculations!)\nmatplotlib – Lets you create simple graphs and plots\n\nWe’ll use %pip install to make sure these are available in your current environment.\n(In Google Colab, %pip works just like the normal terminal command pip but runs inside the notebook.)\n\n\n\n🦛\n\nEven hippos appreciate the right tools for the job — let’s load ours!\n\n\n# Install core packages\n%pip install pandas numpy matplotlib  # For Colab users\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nprint('Your data analysis environment is ready!')\n\nRequirement already satisfied: pandas in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (2.2.3)\nRequirement already satisfied: numpy in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (1.26.4)\nRequirement already satisfied: matplotlib in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (3.9.4)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from pandas) (2025.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from matplotlib) (1.3.0)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from matplotlib) (4.57.0)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from matplotlib) (1.4.7)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from matplotlib) (24.2)\nRequirement already satisfied: pillow&gt;=8 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from matplotlib) (11.1.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from matplotlib) (3.2.3)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from matplotlib) (6.5.2)\nRequirement already satisfied: zipp&gt;=3.1.0 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib) (3.21.0)\nRequirement already satisfied: six&gt;=1.5 in /Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.17.0)\nNote: you may need to restart the kernel to use updated packages.\nYour data analysis environment is ready!",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>👩‍💻 Notebook 0 – Getting Started</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/0_getting_started.html#test-your-setup",
    "href": "notebooks/01_infrastructure/0_getting_started.html#test-your-setup",
    "title": "👩‍💻 Notebook 0 – Getting Started",
    "section": "✅ Test Your Setup",
    "text": "✅ Test Your Setup\nLet’s check that:\n\nYour Python environment works\nRequired packages are installed\nYou can read a file and make a plot\n\nWe’ll use a small dataset called hippo_diets.csv, which contains sample records from a fictional hippo nutrition study. 🦛\n\n\n📚 What will this code do?\n\nRead a CSV file\nWe use pd.read_csv() to load the dataset into a DataFrame — a special table-like structure in Python that’s great for analysis.\nPrint the first row\nThis lets us quickly check that the data loaded correctly.\nMake a scatter plot\nWe’ll plot Calories vs Protein to get a feel for the data.\n\n\nfrom fns_toolkit import get_dataset\n\ndf = get_dataset('hippo_diets.csv')\nprint(df.head(1))\nplt.scatter(df['Calories'], df['Protein'])\nplt.xlabel('Calories')\nplt.ylabel('Protein (g)')\nplt.title('Sample Hippo Diet Data')\nplt.show()\n\n   ID  Calories  Protein        Date\n0  H1      2525     77.3  2024-01-01",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>👩‍💻 Notebook 0 – Getting Started</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/0_getting_started.html#conclusion",
    "href": "notebooks/01_infrastructure/0_getting_started.html#conclusion",
    "title": "👩‍💻 Notebook 0 – Getting Started",
    "section": "✅ Conclusion",
    "text": "✅ Conclusion\n🎉 Success! You’ve verified that your Python environment is working properly:\n\n✅ You’ve installed key packages\n\n✅ Loaded your first dataset\n\n✅ Created a simple visualisation\n\nYou’re now fully set up and ready to begin exploring data in the exciting world of food and nutrition science. 🦛\n\n\n🚀 What’s Next?\nHead to Notebook 1.1 to begin your journey into data science environments — you’ll learn how to think like a data scientist and explore how we actually work with data.\n\n\n\n📚 Helpful Resources\n\n🔧 Install Anaconda (for local setup)\n\n☁️ Google Colab Documentation\n\n📦 Course Repository on GitHub",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>👩‍💻 Notebook 0 – Getting Started</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.1_what_is_data_science_env.html",
    "href": "notebooks/01_infrastructure/1.1_what_is_data_science_env.html",
    "title": "📊 1.1 Introduction to Data Science Environments",
    "section": "",
    "text": "🎯 Objectives\nBefore we dive into data analysis, it’s important to understand the environment where our code runs.\nIn nutrition and food science, we often work with sensitive or large datasets. Having a reliable, reproducible setup — whether local or in the cloud — makes our work easier to share, debug, and scale.\nContext: A robust environment is essential for analysing datasets like NDNS or hippo-themed nutrient logs. 🦛",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>📊 1.1 Introduction to Data Science Environments</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.1_what_is_data_science_env.html#what-is-data-science",
    "href": "notebooks/01_infrastructure/1.1_what_is_data_science_env.html#what-is-data-science",
    "title": "📊 1.1 Introduction to Data Science Environments",
    "section": "🧠 What is Data Science?",
    "text": "🧠 What is Data Science?\nData science is the process of transforming raw data into insights and decisions.\nIt blends statistics, computer science, and domain knowledge — in our case, nutrition and health.\n\n\n🔧 Core Components of Data Science\n\n\n\n\n\n\n\n\nComponent\nWhat It Does\nExample in Nutrition\n\n\n\n\nData\nCollect, clean, and organise information\nSurvey results, NDNS, 24hr recalls\n\n\nStatistics\nSummarise, model, and test relationships\nAre protein levels linked to age or BMI?\n\n\nProgramming\nAutomate tasks, build tools, repeat analyses\nClean hundreds of food logs efficiently\n\n\nVisualisation\nReveal patterns and communicate findings\nPlot sugar trends across age groups\n\n\nCommunication\nTell a story with data\nMake a compelling case for public policy\n\n\nReproducibility\nMake sure others (or future you!) can repeat your work\nShare code + data = reliable science\n\n\n\n\n\n\n🧪 What Makes Data Science Special?\n\nIt’s iterative: You don’t start with all the answers.\nIt’s exploratory: You poke around to understand what’s going on.\nIt’s interdisciplinary: You bring together tools, evidence, and context.\nIt’s powerful: It helps uncover things that no single experiment ever could.\n\n\n\n\n🦛 Hippo Example\nLet’s say you have data from 100 hippos about their diets, weights, and energy intake.\nAs a data scientist, you might ask:\n\nAre heavier hippos eating more calories?\nWhich foods are linked to healthier weights?\nIs there a seasonal pattern to energy intake?\n\nWith data science, you can find answers, visualise trends, and share insights — all in a way that’s reproducible, testable, and scalable.\n\n\n\n✅ Why Start With Environments?\nJust like you need the right kitchen to bake a cake, you need the right environment to do data science.\nThe rest of this notebook helps you create that space — so everything runs smoothly from here on.",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>📊 1.1 Introduction to Data Science Environments</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.1_what_is_data_science_env.html#how-do-we-access-data",
    "href": "notebooks/01_infrastructure/1.1_what_is_data_science_env.html#how-do-we-access-data",
    "title": "📊 1.1 Introduction to Data Science Environments",
    "section": "📁 How Do We Access Data?",
    "text": "📁 How Do We Access Data?\nWe typically load data in one of two ways:\n\n🧳 From the internet – using GitHub to pull the same files used across the course\n\n# From GitHub\n!git clone https://github.com/ggkuhnle/data-analysis-toolkit-FNS.git\n\n📂 Manual upload – useful if you’re testing something private or custom\n\n# Manual upload\nfrom google.colab import files\nuploaded = files.upload()\nThis next step tries to clone the full toolkit repository. If it fails, you’ll be asked to upload the file manually.\n\n# Setup for Google Colab: Fetch datasets automatically or manually\n%run ../../bootstrap.py    # installs requirements + editable package\n\nimport fns_toolkit as fns\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint('Environment ready.')",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>📊 1.1 Introduction to Data Science Environments</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.1_what_is_data_science_env.html#testing-with-hippo-data",
    "href": "notebooks/01_infrastructure/1.1_what_is_data_science_env.html#testing-with-hippo-data",
    "title": "📊 1.1 Introduction to Data Science Environments",
    "section": "Testing with Hippo Data",
    "text": "Testing with Hippo Data\nLoad hippo_diets.csv to verify your setup and create a simple visualization.\n\ndf = fns.get_dataset('hippo_diets.csv')\n\nplt.scatter(df['Calories'], df['Protein'])\nplt.xlabel('Calories')\nplt.ylabel('Protein (g)')\nplt.title('Hippo Diet: Calories vs. Protein')\nplt.show()",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>📊 1.1 Introduction to Data Science Environments</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.1_what_is_data_science_env.html#exercise-1-explore-hippo-diets-visually",
    "href": "notebooks/01_infrastructure/1.1_what_is_data_science_env.html#exercise-1-explore-hippo-diets-visually",
    "title": "📊 1.1 Introduction to Data Science Environments",
    "section": "🧪 Exercise 1: Explore Hippo Diets Visually",
    "text": "🧪 Exercise 1: Explore Hippo Diets Visually\nLet’s go one step further and learn something useful about the data!\nTry one of the following (or both):\n\n\n🔴 Option 1: Colour by Protein Intake\nChange the colour of the points in the scatter plot based on protein levels:\n\nRed for high protein (e.g. &gt; 80g)\nBlue for low protein\n\nThis teaches you how to use conditional logic to customise plots based on data.\n\n\n\n📊 Option 2: Plot a Histogram\nCreate a histogram showing how many hippos eat at different calorie levels.\nThis helps you understand the distribution of a single variable.\n\n\n\n✅ How to Do This in Colab?\nYou don’t need to install anything! Just write code in the cell below and run it with Shift + Enter.\nNeed a reminder?\n\nUse df['Protein'] &gt; 80 to filter high-protein hippos\nUse plt.hist() for histograms\nUse plt.scatter(x, y, c=colours) for colour-coded scatter plots\n\nAnswer:\nI changed the marker color by…\n\n# OPTION 1: Colour points based on protein levels\ncolours = np.where(df['Protein'] &gt; 80, 'red', 'blue')  # True = red, False = blue\nplt.scatter(df['Calories'], df['Protein'], c=colours)\nplt.xlabel('Calories')\nplt.ylabel('Protein (g)')\nplt.title('Hippo Diets by Protein Level')\nplt.show()\n\n# OPTION 2: Plot histogram of Calories\nplt.hist(df['Calories'], bins=10, edgecolor='black')\nplt.xlabel('Calories')\nplt.ylabel('Number of Hippos')\nplt.title('Distribution of Hippo Calorie Intake')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🌟 Bonus: add a trend‐line (np.polyfit) to the scatter plot.\n\n# 1. Grab your raw arrays\nx = df['Calories'].values\ny = df['Protein'].values\n\n# 2. Mask out any NaNs or infs\nmask = np.isfinite(x) & np.isfinite(y)\nx_clean = x[mask]\ny_clean = y[mask]\n\n# 3. Fit the line on the clean data\ncoeffs = np.polyfit(x_clean, y_clean, deg=1)    # [slope, intercept]\npoly   = np.poly1d(coeffs)\n\n# 4. Prepare a smooth x-vector for plotting the trend-line\nx_line = np.linspace(x_clean.min(), x_clean.max(), 100)\n\n# 5. Plot!\ncolors = np.where(y_clean &gt; 80, 'red', 'blue')\nplt.scatter(x_clean, y_clean, c=colors, alpha=0.7)\nplt.plot(x_line, poly(x_line), lw=2,\n         label=f'y = {coeffs[0]:.2f}x + {coeffs[1]:.1f}')\nplt.xlabel('Calories')\nplt.ylabel('Protein (g)')\nplt.title('Hippo Diets: Calories vs. Protein with Trend-Line 📈')\nplt.legend()\nplt.show()",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>📊 1.1 Introduction to Data Science Environments</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.1_what_is_data_science_env.html#conclusion",
    "href": "notebooks/01_infrastructure/1.1_what_is_data_science_env.html#conclusion",
    "title": "📊 1.1 Introduction to Data Science Environments",
    "section": "✅ Conclusion",
    "text": "✅ Conclusion\n🎉 You’ve successfully set up a working Python environment in the cloud and tested your first dataset.\nThis setup ensures that:\n\nYour tools work correctly\nYou can access shared data\nYou’re ready to explore real questions in nutrition science\n\n\n\n📘 Next Steps\nProceed to Notebook 1.2 to compare Python vs. R and understand why this toolkit focuses on Python.\n\n\n\n📚 Additional Resources\n\nJupyter Documentation\nGoogle Colab Docs\nCourse Repository on GitHub",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>📊 1.1 Introduction to Data Science Environments</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.2_python_vs_r.html",
    "href": "notebooks/01_infrastructure/1.2_python_vs_r.html",
    "title": "🧮 1.2 Python vs R vs Other Tools for Nutrition Data Analysis",
    "section": "",
    "text": "🧰 What Do We Mean by “Tools”?\nThis notebook compares Python, R, SPSS, and XLStat as tools for data analysis in food and nutrition sciences, helping MSc students choose the right tool for their research.\nObjectives:\nContext: Nutrition studies often require robust data analysis tools. Python offers flexibility, R excels in statistical packages, SPSS provides a user-friendly interface, and XLStat integrates with Excel for sensory analysis. Choosing the right tool depends on your project needs!\nIn data science, a tool is a program or environment that helps you: - Load and clean data - Perform calculations and analysis - Visualise or report results\nYou’ve probably used some already — Excel, SPSS, R, or Python.\nEach tool has different strengths depending on the kind of problem and how much flexibility or automation you need.\nChoosing a tool is part of being a thoughtful data analyst — like choosing the right lab equipment!",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>🧮 1.2 Python vs R vs Other Tools for Nutrition Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.2_python_vs_r.html#tool-comparison",
    "href": "notebooks/01_infrastructure/1.2_python_vs_r.html#tool-comparison",
    "title": "🧮 1.2 Python vs R vs Other Tools for Nutrition Data Analysis",
    "section": "💡 Tool Comparison",
    "text": "💡 Tool Comparison\nHere’s a detailed comparison of Python, R, SPSS, and XLStat, focusing on their use in nutrition data analysis:\n\n\n\n\n\n\n\n\n\nTool\nStrengths\nLimitations\nBest For\n\n\n\n\nPython\nOpen-source, general-purpose, readable, supports automation, machine learning, and large datasets\nRequires coding knowledge, slightly more setup\nPipelines, dashboards, reproducible workflows, NDNS analysis\n\n\nR\nExcellent statistical packages (e.g., ggplot2, dplyr), built-in stats functions, visualisation\nLess general-purpose, steeper learning curve for non-stats tasks\nStatistical analysis, visualisation, epidemiology studies\n\n\nSPSS\nUser-friendly GUI, menu-driven stats, widely used in social sciences and nutrition\nLimited flexibility, expensive, poor reproducibility (manual steps)\nQuick statistical tests, small studies, users preferring GUI\n\n\nXLStat\nIntegrates with Excel, excellent for sensory analysis and multivariate stats, user-friendly\nExpensive, closed environment, limited scripting, tied to Excel’s limitations\nSensory evaluation, Excel-based workflows, quick multivariate analysis\n\n\n\nDetailed Differences:\n\n🔧 Usability:\n\nPython and R require coding but offer flexibility and reproducibility. Python’s syntax (e.g., pandas) is often more intuitive for beginners, while R’s statistical functions (e.g., summary()) are straightforward for stats tasks.\nSPSS uses a graphical interface, making it accessible for non-coders, but manual steps (e.g., clicking menus) hinder automation. For example, running an ANOVA in SPSS involves selecting options through menus, which isn’t logged as code.\nXLStat also uses a GUI within Excel, ideal for users comfortable with spreadsheets. It simplifies tasks like principal component analysis (PCA) for sensory data but lacks the depth of Python or R for custom analyses.\n\n\n\n🔄 Reproducibility:\n\nPython and R excel here—code can be shared and rerun exactly (e.g., this notebook!). Python’s pandas and R’s dplyr allow scripted workflows.\nSPSS and XLStat struggle with reproducibility. SPSS can generate syntax, but it’s often an afterthought, and XLStat’s Excel integration means steps are buried in spreadsheet operations, making them error-prone and hard to replicate.\n\n\n\n💷 Cost:\n\nPython and R are free and open-source, making them accessible for students and researchers.\nSPSS and XLStat are commercial. SPSS requires a costly licence, often prohibitive for individuals, though universities may provide access. XLStat also requires a paid licence, adding to Excel’s cost, which can be a barrier for small research groups.\n\n\n\n🤸 Flexibility:\n\nPython is the most flexible, supporting everything from data cleaning (pandas) to machine learning (scikit-learn) and dashboards (Dash). It’s ideal for building end-to-end pipelines in nutrition research.\nR is strong for statistical modelling and visualisation but less suited for non-stats tasks like web apps.\nSPSS is rigid—its GUI limits customisation, and scripting is clunky. It’s best for standard statistical tests (e.g., t-tests, ANOVA).\nXLStat offers flexibility within Excel’s ecosystem, with modules for sensory analysis, but it’s constrained by Excel’s limitations (e.g., handling large datasets, automation).\n\nWhy Python for This Toolkit?\nPython is our choice because it’s free, supports reproducible workflows, and can handle everything from small scripts to large nutrition data pipelines (e.g., NDNS analysis). It’s also widely used in modern research, making it a valuable skill for MSc students!",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>🧮 1.2 Python vs R vs Other Tools for Nutrition Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.2_python_vs_r.html#common-use-cases-in-food-nutrition-sciences",
    "href": "notebooks/01_infrastructure/1.2_python_vs_r.html#common-use-cases-in-food-nutrition-sciences",
    "title": "🧮 1.2 Python vs R vs Other Tools for Nutrition Data Analysis",
    "section": "🍽️ Common Use Cases in Food & Nutrition Sciences",
    "text": "🍽️ Common Use Cases in Food & Nutrition Sciences\n\n\n\nScenario\nBest Tool\n\n\n\n\nAnalysing NDNS dietary intake data\n🐍 Python, 📊 R\n\n\nQuick t-test for nutrient intervention\n📋 SPSS\n\n\nMultivariate PCA for sensory scores\n📈 XLStat\n\n\nCreating automated plots and reports\n🐍 Python\n\n\nStatistical model for food safety risk\n📊 R",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>🧮 1.2 Python vs R vs Other Tools for Nutrition Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.2_python_vs_r.html#fun-analogy-if-statistics-tools-were-cars",
    "href": "notebooks/01_infrastructure/1.2_python_vs_r.html#fun-analogy-if-statistics-tools-were-cars",
    "title": "🧮 1.2 Python vs R vs Other Tools for Nutrition Data Analysis",
    "section": "🚗 Fun Analogy: If Statistics Tools Were Cars…",
    "text": "🚗 Fun Analogy: If Statistics Tools Were Cars…\nTo lighten things up, here’s a humorous take on these tools as cars, reflecting their characteristics in data analysis:\n\n\n\nIf statistics programs/languages were cars\n\n\n\nExcel: A broken-down car—familiar but unreliable for serious analysis due to errors and lack of reproducibility.\nSPSS: An old station wagon—functional for basic stats but outdated and slow for modern research needs.\nPython: A sleek Tesla—modern, versatile, and powerful, perfect for cutting-edge nutrition research.\nR: A Mad Max-style armoured vehicle—robust for stats but complex and intimidating for beginners.\nStata: A reliable SUV—practical and user-friendly, though not as flexible as Python or R.\nMinitab: A small hatchback—basic and limited, suitable for quick tasks but not heavy lifting.\nSAS: A classic luxury car—powerful and expensive, often overkill for most nutrition studies.\n\nNote: SPSS and XLStat aren’t shown in the image, but we’d imagine SPSS as a dependable but clunky minivan (user-friendly but not agile) and XLStat as a modified Excel car—handy for specific tasks but constrained by its base model (Excel)!",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>🧮 1.2 Python vs R vs Other Tools for Nutrition Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.2_python_vs_r.html#tool-selection-guide",
    "href": "notebooks/01_infrastructure/1.2_python_vs_r.html#tool-selection-guide",
    "title": "🧮 1.2 Python vs R vs Other Tools for Nutrition Data Analysis",
    "section": "🧭 Tool Selection Guide",
    "text": "🧭 Tool Selection Guide\nUse this quick decision tree to help choose a data analysis tool based on your preferences:\nAre you comfortable writing code?\n├── ❌ No\n│   ├── Prefer Excel?               → ✅ XLStat\n│   └── Prefer menu-based stats?   → ✅ SPSS\n└── ✅ Yes\n    ├── Focus on statistics/plots? → ✅ R\n    └── Need automation/flexibility? → ✅ Python",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>🧮 1.2 Python vs R vs Other Tools for Nutrition Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.2_python_vs_r.html#common-use-cases-in-food-nutrition-sciences-1",
    "href": "notebooks/01_infrastructure/1.2_python_vs_r.html#common-use-cases-in-food-nutrition-sciences-1",
    "title": "🧮 1.2 Python vs R vs Other Tools for Nutrition Data Analysis",
    "section": "🍽️ Common Use Cases in Food & Nutrition Sciences",
    "text": "🍽️ Common Use Cases in Food & Nutrition Sciences\n\n\n\nScenario\nBest Tool\n\n\n\n\nAnalysing NDNS dietary intake data\nPython, R\n\n\nQuick t-test for nutrient intervention\nSPSS\n\n\nMultivariate PCA for sensory scores\nXLStat\n\n\nCreating automated plots and reports\nPython\n\n\nStatistical model for food safety risk\nR\n\n\n\n\nTip: There’s no perfect tool — it depends on your project, dataset size, goals, and your own learning style!\n\n\n🦛\n\nHippo says: “Use the tool that gets you moving, not just the one with the flashiest horn.”\n\n\n# Setup for Google Colab: Fetch datasets automatically or manually\n%run ../../bootstrap.py    # installs requirements + editable package\n\nimport fns_toolkit as fns\n\nimport pandas as pd  # For data manipulation\nimport numpy as np  # For numerical operations",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>🧮 1.2 Python vs R vs Other Tools for Nutrition Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.2_python_vs_r.html#python-analysis-example",
    "href": "notebooks/01_infrastructure/1.2_python_vs_r.html#python-analysis-example",
    "title": "🧮 1.2 Python vs R vs Other Tools for Nutrition Data Analysis",
    "section": "Python Analysis Example",
    "text": "Python Analysis Example\nLet’s perform a simple analysis in Python using hippo_diets.csv to compute summary statistics for calories and protein. This mirrors what you might do in R, SPSS, or XLStat, but with Python’s reproducible code.\n\n\ndf = fns.get_dataset('hippo_diets')\n\n# Compute summary statistics for Calories and Protein columns\n# .describe() generates stats like count, mean, std, min, max\n# We select specific rows for clarity\nsummary = df[['Calories', 'Protein']].describe().loc[['count', 'mean', 'std', 'min', 'max']]\nprint(summary)  # Display the summary statistics",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>🧮 1.2 Python vs R vs Other Tools for Nutrition Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.2_python_vs_r.html#how-other-tools-would-do-this",
    "href": "notebooks/01_infrastructure/1.2_python_vs_r.html#how-other-tools-would-do-this",
    "title": "🧮 1.2 Python vs R vs Other Tools for Nutrition Data Analysis",
    "section": "How Other Tools Would Do This",
    "text": "How Other Tools Would Do This\nR: In R, you’d use summary() for similar stats:\ndf &lt;- read.csv('data/hippo_diets.csv')\nsummary(df[, c('Calories', 'Protein')])\nR’s output includes quartiles (e.g., 25%, 50%, 75%), which Python’s describe() also provides but we filtered out for brevity.\nSPSS: In SPSS, you’d: 1. Import hippo_diets.csv via File &gt; Open &gt; Data. 2. Go to Analyze &gt; Descriptive Statistics &gt; Frequencies. 3. Select Calories and Protein, then click Statistics to choose mean, std, min, max. 4. Run and view results in the Output window. This is user-friendly but manual—steps aren’t scripted, making it hard to reproduce.\nXLStat: In XLStat (within Excel): 1. Open hippo_diets.csv in Excel. 2. Go to XLStat &gt; Describing Data &gt; Descriptive Statistics. 3. Select Calories and Protein columns, choose stats (mean, std, etc.), and run. 4. Results appear in a new Excel sheet. This is quick for Excel users but limited by Excel’s scalability and lack of scripting.",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>🧮 1.2 Python vs R vs Other Tools for Nutrition Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.2_python_vs_r.html#exercises",
    "href": "notebooks/01_infrastructure/1.2_python_vs_r.html#exercises",
    "title": "🧮 1.2 Python vs R vs Other Tools for Nutrition Data Analysis",
    "section": "🧪 Exercises",
    "text": "🧪 Exercises\n\nCompare Outputs: Run the Python code above. How does Python’s describe() output compare to what you’d expect from R’s summary(), SPSS’s Frequencies, or XLStat’s Descriptive Statistics? Write your thoughts in a Markdown cell below. Consider ease of use, output format, and reproducibility.\nCalculate a Median: Write a Python script to calculate the median of [1, 2, 3, 4, 100]. How would you do this in SPSS or XLStat? (Hint: SPSS uses Analyze &gt; Descriptive Statistics; XLStat uses Describing Data &gt; Descriptive Statistics.)\nResearch Tools: Choose one tool (SPSS or XLStat) and Google “ vs Python for nutrition data analysis”. Summarise your findings in a Markdown cell. How does this influence your tool choice for a project like NDNS analysis?\n\nGuidance: Use the comparison table and car analogy to reflect on which tool suits your research needs!\nYour Answers:\nExercise 1: Compare Outputs\nPython’s describe() provides…\nExercise 2: Calculate Median\n[Write your Python code and compare with SPSS/XLStat]\nExercise 3: Research Tools\n[Summarise your findings here]",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>🧮 1.2 Python vs R vs Other Tools for Nutrition Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.2_python_vs_r.html#conclusion",
    "href": "notebooks/01_infrastructure/1.2_python_vs_r.html#conclusion",
    "title": "🧮 1.2 Python vs R vs Other Tools for Nutrition Data Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve compared Python, R, SPSS, and XLStat for nutrition data analysis and performed a Python analysis with hippo_diets.csv. Python’s flexibility and reproducibility make it ideal for this toolkit, but R, SPSS, and XLStat have their strengths—R for stats, SPSS for GUI users, and XLStat for Excel-based sensory analysis.\nNext Steps: Explore version control with Git in 1.3_intro_to_git.ipynb.\nResources: - Python Documentation - R Project - SPSS Overview - XLStat Features - Repository: github.com/ggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>🧮 1.2 Python vs R vs Other Tools for Nutrition Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.3_intro_to_git.html",
    "href": "notebooks/01_infrastructure/1.3_intro_to_git.html",
    "title": "📚 1.3 Introduction to Git 🦛",
    "section": "",
    "text": "🧰 What is Git?\nObjectives: - Understand Git’s purpose and core commands - Practise committing & tracking changes - Explore GitHub collaboration workflows\nGit is a version control system that lets you track changes in your code, collaborate with others, and backtrack if something goes wrong. Think of it as a very detailed “track changes” for your entire project.\nYou can: - Save snapshots of your project (commits) - Roll back to earlier versions - Collaborate via platforms like GitHub - See who changed what and when - Create isolated workstreams (branches) for new features",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>📚 1.3 Introduction to Git 🦛</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.3_intro_to_git.html#setting-up-git",
    "href": "notebooks/01_infrastructure/1.3_intro_to_git.html#setting-up-git",
    "title": "📚 1.3 Introduction to Git 🦛",
    "section": "⚙️ Setting Up Git",
    "text": "⚙️ Setting Up Git\n\nInstalling & Configuring\nRun in a terminal (not Python):\n# Check Git installation\ngit --version  # ensure Git is installed\n\n# Configure your identity (👉 use your own name/email!)\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n\n\nDay-to-day Commands\nUse these to interact with your repo:\ngit init              # Initialise a repository\ngit status            # Show changes\ngit add &lt;file&gt;        # Stage a file\ngit commit -m \"msg\"   # Commit staged changes\ngit log               # Show history\ngit diff              # Show differences\ngit clone &lt;URL&gt;       # Clone a remote repo\ngit pull              # Fetch and merge changes\ngit push              # Send changes to remote\n\n\n📘 Common Git Commands (expanded)\n\ngit branch                   # List or create branches\ngit checkout -b new-branch   # Create & switch to a branch\ngit merge &lt;branch&gt;           # Merge another branch into current\ngit remote add origin &lt;URL&gt;  # Link local repo to GitHub",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>📚 1.3 Introduction to Git 🦛</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.3_intro_to_git.html#undoing-changes",
    "href": "notebooks/01_infrastructure/1.3_intro_to_git.html#undoing-changes",
    "title": "📚 1.3 Introduction to Git 🦛",
    "section": "🔁 Undoing Changes",
    "text": "🔁 Undoing Changes\n\n\n\n\n\n\n\nSituation\nCommand\n\n\n\n\nUnstage file\ngit restore --staged &lt;file&gt;\n\n\nDiscard file changes\ngit restore &lt;file&gt;\n\n\nUndo last commit (keep code)\ngit reset --soft HEAD~1\n\n\nUndo last commit (drop code)\ngit reset --hard HEAD~1\n\n\nUndo pushed commit\nAvoid! Use git revert &lt;hash&gt; instead\n\n\n\n💡 Tip: Avoid --hard resets on shared branches—you can always git revert to undo publicly shared commits.",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>📚 1.3 Introduction to Git 🦛</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.3_intro_to_git.html#exercise-create-a-repository",
    "href": "notebooks/01_infrastructure/1.3_intro_to_git.html#exercise-create-a-repository",
    "title": "📚 1.3 Introduction to Git 🦛",
    "section": "🧪 Exercise: Create a Repository",
    "text": "🧪 Exercise: Create a Repository\nFollow these steps in a terminal: 1. 🗂️ mkdir my-nutrition-repo 2. 🔨 cd my-nutrition-repo 3. 🔨 git init 4. 📝 echo \"# Nutrition Notes\" &gt; README.md 5. ➕ git add README.md 6. 💾 git commit -m \"Initial commit\"\n🔥 Stretch (bonus): Push to GitHub:\ngit remote add origin git@github.com:YOUR-USERNAME/my-nutrition-repo.git\ngit push -u origin main",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>📚 1.3 Introduction to Git 🦛</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.3_intro_to_git.html#answer",
    "href": "notebooks/01_infrastructure/1.3_intro_to_git.html#answer",
    "title": "📚 1.3 Introduction to Git 🦛",
    "section": "Answer",
    "text": "Answer\nI set up my repo and here’s what I saw when I ran git log…",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>📚 1.3 Introduction to Git 🦛</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.3_intro_to_git.html#conclusion",
    "href": "notebooks/01_infrastructure/1.3_intro_to_git.html#conclusion",
    "title": "📚 1.3 Introduction to Git 🦛",
    "section": "✅ Conclusion",
    "text": "✅ Conclusion\nYou did it! - [x] Learned what Git is - [x] Configured your name/email - [x] Made your first commit - [ ] (Bonus) Pushed to GitHub\nNext Steps: - ▶️ 1.4 Quarto: reproducible documents\n\n\n📖 Docs\n\nGit Book\nGitHub Guides\n\n\n\n🔗 Toolkit Repo\n\nggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>📚 1.3 Introduction to Git 🦛</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.4_quarto_basics.html",
    "href": "notebooks/01_infrastructure/1.4_quarto_basics.html",
    "title": "📓 1.4 Quarto Basics",
    "section": "",
    "text": "📝 What is Quarto?\nThis notebook introduces Quarto, a tool for creating reproducible documents combining code, text, and visuals, ideal for nutrition research reports.\nObjectives: - Understand Quarto’s role in reproducible research. - Create a simple Quarto document with Python code. - Render the document to HTML.\nContext: Quarto is used in this toolkit (e.g., index.qmd) to share analyses, perfect for MSc projects.\nQuarto is an open-source scientific and technical publishing system that allows you to create fully reproducible documents that combine code, narrative text, and visuals. It’s designed for researchers, analysts, and data scientists who need to communicate their findings in a clear, flexible, and transparent way.",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>📓 1.4 Quarto Basics</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.4_quarto_basics.html#what-can-you-use-quarto-for",
    "href": "notebooks/01_infrastructure/1.4_quarto_basics.html#what-can-you-use-quarto-for",
    "title": "📓 1.4 Quarto Basics",
    "section": "📊 What Can You Use Quarto For?",
    "text": "📊 What Can You Use Quarto For?\n\n\n\n\n\n\n\nUse Case\nDescription\n\n\n\n\nData Reports\nGenerate clean, consistent research reports combining code, figures, analysis.\n\n\nLab Notebooks\nCreate digital, reproducible records of your work—great for collaborations.\n\n\nTeaching Materials\nBuild slides, handouts, tutorials from one source.\n\n\nWebsites\nBuild research/teaching sites with integrated results.\n\n\nDashboards\nWith extensions, create interactive dashboards.\n\n\n\n\n\n\n💡 Example: Nutrition Study Report\n\nImagine analysing polyphenol intake from the NDNS dataset. Using Quarto you could: - Load data with Python - Run models - Generate plots/tables - Write interpretations - Render a stand-alone HTML/PDF—all from one .qmd file!",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>📓 1.4 Quarto Basics</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.4_quarto_basics.html#how-does-it-work",
    "href": "notebooks/01_infrastructure/1.4_quarto_basics.html#how-does-it-work",
    "title": "📓 1.4 Quarto Basics",
    "section": "💾 How Does It Work?",
    "text": "💾 How Does It Work?\nA Quarto file uses the .qmd extension and typically includes:\n---\ntitle: \"Example Analysis\"\nformat: html\n---\n\n## Results\n\n```{python}\nimport pandas as pd\ndf = pd.read_csv(\"data.csv\")\ndf.describe()\nRunning `quarto render myfile.qmd` executes all code, embeds outputs, and produces the final HTML (or PDF) document.\n\n---\n\n📚 [Quarto Documentation →](https://quarto.org)  \n🎓 Perfect for MSc nutrition, food science, and sensory analysis students.\n\n## 🧪 From Concept to Practice\n\nLet’s try it with our `hippo_diets.csv` data. We’ll load the data, make a plot, and then you’ll draft a `.qmd` file to render yourself.\n\n::: {#setup .cell}\n``` {.python .cell-code}\n# Setup for Google Colab: Fetch datasets automatically or manually\n%run ../../bootstrap.py    # installs requirements + editable package\n\nimport fns_toolkit as fns\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint('Quarto environment ready.')\n\nQuarto environment ready.\n\n:::",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>📓 1.4 Quarto Basics</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.4_quarto_basics.html#creating-a-quarto-ready-plot",
    "href": "notebooks/01_infrastructure/1.4_quarto_basics.html#creating-a-quarto-ready-plot",
    "title": "📓 1.4 Quarto Basics",
    "section": "Creating a Quarto-ready Plot",
    "text": "Creating a Quarto-ready Plot\nLoad hippo_diets.csv and make a histogram for your Quarto report:\n\ndf = fns.get_dataset('hippo_diets')\nplt.hist(df['Calories'], bins=10, edgecolor='black')\nplt.xlabel('Calories (kcal)')\nplt.ylabel('Frequency')\nplt.title('Distribution of Hippo Calorie Intakes')\nplt.grid(True, alpha=0.3)\nplt.show()",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>📓 1.4 Quarto Basics</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.4_quarto_basics.html#exercise-1-draft-a-quarto-file",
    "href": "notebooks/01_infrastructure/1.4_quarto_basics.html#exercise-1-draft-a-quarto-file",
    "title": "📓 1.4 Quarto Basics",
    "section": "🧪 Exercise 1: Draft a Quarto File",
    "text": "🧪 Exercise 1: Draft a Quarto File\n\nCreate my_quarto.qmd with:\n---\ntitle: \"Hippo Diet Analysis\"\nformat: html\n---\n\n## Calorie Distribution\n\n```{python}\n# Paste your histogram code here\n```\nIn a terminal, run:\nquarto render my_quarto.qmd\nPaste any errors or your success message below.\n\nAnswer: I created and rendered the Quarto file by…",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>📓 1.4 Quarto Basics</span>"
    ]
  },
  {
    "objectID": "notebooks/01_infrastructure/1.4_quarto_basics.html#conclusion",
    "href": "notebooks/01_infrastructure/1.4_quarto_basics.html#conclusion",
    "title": "📓 1.4 Quarto Basics",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve learned the basics of Quarto for reproducible documents, enhancing your nutrition research outputs.\nNext Steps: Begin programming basics in 2.1 Python fundamentals.\nResources: - Quarto Documentation - Quarto on GitHub - Course Repo: ggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "01 Infrastructure",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>📓 1.4 Quarto Basics</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/index.html",
    "href": "notebooks/02_programming_basics/index.html",
    "title": "2 – Programming Basics",
    "section": "",
    "text": "Welcome to Programming Basics\nIn this module you’ll learn the foundations of writing code for data analysis in nutrition science: from understanding what programming is, all the way through object-oriented design.",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>2 – Programming Basics</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/index.html#notebooks",
    "href": "notebooks/02_programming_basics/index.html#notebooks",
    "title": "2 – Programming Basics",
    "section": "📑 Notebooks",
    "text": "📑 Notebooks\n\n2.0 Intro to Programming\nWhat is programming, why it matters for data analysis, and how Python fits into the story.\n❯ Open notebook\n2.1 Programming Fundamentals\nThe big picture: variables, functions, comments, formatting and error handling.\n❯ Open notebook\n2.2 Data Types & Conversion\nIntegers, floats, strings, dates, and strategies for type‐casting & missing values.\n❯ Open notebook\n2.3 Functions, Loops & Control Flow\nReusable functions, looping constructs, conditionals and best practices.\n❯ Open notebook\n2.4 Data Structures\nLists, dictionaries, NumPy arrays, and pandas DataFrames for organising your data.\n❯ Open notebook\n2.5 Input & Output\nConsole I/O, text files, CSV, and JSON—how to read external data and export results.\n❯ Open notebook\n2.6 OOP Basics\nClasses, objects, methods, inheritance and when to use object-oriented design.\n❯ Open notebook",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>2 – Programming Basics</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/index.html#next-steps",
    "href": "notebooks/02_programming_basics/index.html#next-steps",
    "title": "2 – Programming Basics",
    "section": "🛠️ Next Steps",
    "text": "🛠️ Next Steps\nOnce you’ve worked through these notebooks, move on to Module 3 for data cleaning and exploration techniques.",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>2 – Programming Basics</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.0_intro_to_programming.html",
    "href": "notebooks/02_programming_basics/2.0_intro_to_programming.html",
    "title": "🧠 2.0 Introduction to Programming for Data Analysis",
    "section": "",
    "text": "🧾 What is Programming?\nWelcome to the foundation of programming! This notebook introduces key programming concepts and explains what programming is, why it’s useful, and how Python fits into the world of data analysis—especially in nutrition, food, and sensory sciences.\nObjectives: - Understand what programming is and what it is used for. - Learn about different programming paradigms. - Recognise Python’s strengths and why it’s a good fit for this course.\nContext: Just as recipes guide a chef, code tells the computer what to do. Let’s explore how! 🦛\nProgramming is the process of writing instructions that a computer can follow to perform tasks.\nIn nutrition and food science, we use programs to: - Analyse large datasets (e.g., NDNS or lab results) - Perform calculations (e.g., nutrient intake, sensory scores) - Create visualisations (e.g., graphs of dietary intake) - Automate repetitive tasks\nThink of a program like a recipe: clear steps to achieve a tasty result. 🍲",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>🧠 2.0 Introduction to Programming for Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.0_intro_to_programming.html#programming-paradigms",
    "href": "notebooks/02_programming_basics/2.0_intro_to_programming.html#programming-paradigms",
    "title": "🧠 2.0 Introduction to Programming for Data Analysis",
    "section": "🧭 Programming Paradigms",
    "text": "🧭 Programming Paradigms\nParadigms are different styles of organising code. The main ones are:\n\nProcedural Programming (like a recipe): You write step-by-step instructions.\nObject-Oriented Programming (like describing ingredients as objects): You define “things” (objects) and actions they can perform.\nFunctional Programming (like maths): You build programs using pure functions.\n\nIn this course, we’ll mostly use procedural and a little object-oriented programming.",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>🧠 2.0 Introduction to Programming for Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.0_intro_to_programming.html#theoretical-foundations",
    "href": "notebooks/02_programming_basics/2.0_intro_to_programming.html#theoretical-foundations",
    "title": "🧠 2.0 Introduction to Programming for Data Analysis",
    "section": "🔬 Theoretical Foundations",
    "text": "🔬 Theoretical Foundations\nBefore we write any code, let’s cover some core ideas that underlie all programming:\n\nAlgorithm\n\nA precise, step-by-step procedure for solving a problem.\n\nExample: A recipe is an algorithm for baking a cake.\n\nComputational Thinking (Wing, 2006)\n\nDecomposition: Break a big problem into smaller parts.\n\nPattern Recognition: Spot similarities across problems.\n\nAbstraction: Focus on the important details, ignore the rest.\n\nAlgorithm Design: Combine the above into a clear procedure.\n\nComplexity & Big-O\n\nMeasures how runtime (or memory) grows with input size.\n\nExample: Linear search → O(n); Bubble sort → O(n²).\n\nWhy it matters for NDNS: millions of rows mean you need efficient algorithms.\n\nVon Neumann Architecture\n\nPrograms and data live in the same memory (“stored-program” concept).\n\nThe CPU fetches instructions one by one and executes them.\n\nCompiled vs. Interpreted Languages\n\nCompiled (C/C++): Translated to machine code ahead of time → very fast at runtime.\n\nInterpreted (Python, R): Translated on the fly → slower execution, but faster development and iteration.\n\n\n\nWhy it matters: Python’s interpreter makes exploration quick, while libraries like NumPy use compiled C under the hood for performance.",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>🧠 2.0 Introduction to Programming for Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.0_intro_to_programming.html#abstraction-layers",
    "href": "notebooks/02_programming_basics/2.0_intro_to_programming.html#abstraction-layers",
    "title": "🧠 2.0 Introduction to Programming for Data Analysis",
    "section": "📚 Abstraction Layers",
    "text": "📚 Abstraction Layers\nProgramming happens at many levels. Each layer lets you ignore details below it until you need them:\n\nMachine Code (binary)\nAssembly (mnemonics & registers)\nHigh-Level Languages (Python, R, JavaScript)\nLibraries/Frameworks (pandas, ggplot2, Dash)\nApplications (your final report or dashboard)\n\n\nYou don’t write CPU instructions in Python—but under the hood your code becomes reads/writes to memory and arithmetic in the ALU.\nLibraries like pandas are written in C for speed, but expose a friendly Python API so you can work at a high level.\n\n\n🏃‍♂️ Complexity Demo: Linear vs Quadratic\nWe’ll time two functions: - sum_list() – sums a list in O(n) time\n- pairwise_sum() – sums every possible pair in O(n²) time\n\nimport time\n\ndef sum_list(lst):\n    return sum(lst)\n\ndef pairwise_sum(lst):\n    total = 0\n    for i in lst:\n        for j in lst:\n            total += i + j\n    return total\n\n# Prepare data\ndata = list(range(2000))  # 2k elements\n\nfor fn in (sum_list, pairwise_sum):\n    start = time.time()\n    fn(data)\n    elapsed = time.time() - start\n    print(f\"{fn.__name__:12} → {elapsed:.3f} sec\")",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>🧠 2.0 Introduction to Programming for Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.0_intro_to_programming.html#deep-dive-exercises",
    "href": "notebooks/02_programming_basics/2.0_intro_to_programming.html#deep-dive-exercises",
    "title": "🧠 2.0 Introduction to Programming for Data Analysis",
    "section": "🧪 Deep-Dive Exercises",
    "text": "🧪 Deep-Dive Exercises\n\nAlgorithm Timing\n\nImplement your own linear search (find_item) and naive quadratic search (compare-all pairs) on a list of 1,000 elements.\n\nTime each and compare the growth of runtime.\n\nPandas Complexity\n\nLoad a small slice of hippo_diets.csv.\n\nTime a groupby operation (e.g., average calories per hippo) and estimate its complexity.\n\nReflect\n\nWhy does understanding Big-O help when working with large nutrition datasets?",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>🧠 2.0 Introduction to Programming for Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.0_intro_to_programming.html#why-python",
    "href": "notebooks/02_programming_basics/2.0_intro_to_programming.html#why-python",
    "title": "🧠 2.0 Introduction to Programming for Data Analysis",
    "section": "🐍 Why Python?",
    "text": "🐍 Why Python?\nPython is widely used in data science because it’s: - Easy to read and write (clean syntax) - Free and open-source - Supported by a huge community - Rich in data analysis tools (e.g., pandas, matplotlib)\nPython works great for everything from analysing diet logs to building interactive visualisations or machine learning models!",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>🧠 2.0 Introduction to Programming for Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.0_intro_to_programming.html#summary",
    "href": "notebooks/02_programming_basics/2.0_intro_to_programming.html#summary",
    "title": "🧠 2.0 Introduction to Programming for Data Analysis",
    "section": "✅ Summary",
    "text": "✅ Summary\nIn this notebook, we’ve covered: - What programming is and key paradigms - Theoretical foundations: algorithms, complexity, abstraction layers - A practical complexity demo to see Big-O in action - Why Python is our language of choice for nutrition data analysis\nNext Steps: Head to 2.1_syntax_variables_comments.ipynb to write your first lines of Python code! 🦛",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>🧠 2.0 Introduction to Programming for Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.1_programming_fundamentals.html",
    "href": "notebooks/02_programming_basics/2.1_programming_fundamentals.html",
    "title": "🧠 Programming Fundamentals: The Big Picture",
    "section": "",
    "text": "🧱 What Is a Program?\nBefore we dive into writing Python code, it helps to understand what programming actually is and why it matters for data analysis in nutrition, food, and sensory science.\nA computer program consists of: - Instructions (the code you write) - Data (the values those instructions work on)\nImagine a hippo cookbook: the recipe is the instructions, and the ingredients are the data.\nA simple Python program:",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>🧠 Programming Fundamentals: The Big Picture</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.1_programming_fundamentals.html#what-is-a-program",
    "href": "notebooks/02_programming_basics/2.1_programming_fundamentals.html#what-is-a-program",
    "title": "🧠 Programming Fundamentals: The Big Picture",
    "section": "",
    "text": "calories = 2500         # integer: daily kcal\nprotein = 80            # integer: grams of protein\nprint(\n    \"This diet provides\",\n    calories, \"kcal and\",\n    protein, \"g of protein.\"\n)",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>🧠 Programming Fundamentals: The Big Picture</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.1_programming_fundamentals.html#structure-of-a-program",
    "href": "notebooks/02_programming_basics/2.1_programming_fundamentals.html#structure-of-a-program",
    "title": "🧠 Programming Fundamentals: The Big Picture",
    "section": "🧩 Structure of a Program",
    "text": "🧩 Structure of a Program\nMost programs use these core elements:\n\nVariables: store data (e.g., hippo_weight_kg)\nFunctions: reusable blocks of code (e.g., def calculate_bmi(...))\nConditionals: branch logic (if/else) — covered in 2.3\nLoops: repeat actions (for, while) — covered in 2.3\nComments: explain why (not what) your code does\nError handling: gracefully deal with unexpected problems (try/except)\n\nThink of these as the pots, pans, and measuring spoons of your programming kitchen. You’ll use them in almost every recipe!",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>🧠 Programming Fundamentals: The Big Picture</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.1_programming_fundamentals.html#naming-things-variables-functions",
    "href": "notebooks/02_programming_basics/2.1_programming_fundamentals.html#naming-things-variables-functions",
    "title": "🧠 Programming Fundamentals: The Big Picture",
    "section": "🧑‍🍳 Naming Things: Variables & Functions",
    "text": "🧑‍🍳 Naming Things: Variables & Functions\nClear, consistent names make code easier to read and maintain. In Python:\n\nVariables and functions: use snake_case (lowercase + underscores)\n\nGood: hippo_name, calculate_energy()\nBad: HippoName, calcEnergy, x1\n\nConstants: use UPPER_SNAKE_CASE\n\ne.g. MAX_CALORIES = 3000\n\n\n\nTip: Choose names that describe the purpose, not just the type. - iron_intake_mg is clearer than i or value.\n\nExamples:\nhippo_name = \"Hilda\"\ncalcium_intake_mg = 1200\n\ndef calculate_bmi(weight_kg, height_m):\n    # returns BMI = kg / m^2\n    return weight_kg / (height_m ** 2)",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>🧠 Programming Fundamentals: The Big Picture</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.1_programming_fundamentals.html#formatting-indentation",
    "href": "notebooks/02_programming_basics/2.1_programming_fundamentals.html#formatting-indentation",
    "title": "🧠 Programming Fundamentals: The Big Picture",
    "section": "🪶 Formatting & Indentation",
    "text": "🪶 Formatting & Indentation\nPython uses indentation (4 spaces) to define code blocks. Never mix tabs and spaces.\n\n4 spaces per level (configured in your editor)\nNo trailing whitespace\n\nBad (no indentation):\nif iron_intake &gt; 8.0:\nprint(\"Sufficient intake\")\nGood:\nif iron_intake &gt; 8.0:\nprint(\"Sufficient intake\")\n\nTip: Most editors (VS Code, PyCharm, Colab) auto-indent for you. Stick to 4 spaces!",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>🧠 Programming Fundamentals: The Big Picture</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.1_programming_fundamentals.html#comment-your-code",
    "href": "notebooks/02_programming_basics/2.1_programming_fundamentals.html#comment-your-code",
    "title": "🧠 Programming Fundamentals: The Big Picture",
    "section": "🗣️ Comment Your Code",
    "text": "🗣️ Comment Your Code\nComments help humans understand why you wrote something. They are ignored by Python.\n\nUse # for single-line comments above or beside code.\nKeep comments concise and avoid stating the obvious.\nFor multi-line explanations, consider a docstring in functions (see 2.3 for more).\n\nExamples:\n# calculate the percentage of reference intake\ndef nutrient_percentage(intake, reference):\n    return (intake / reference) * 100  # returns float\n\niron_intake = 8.2  # in mg",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>🧠 Programming Fundamentals: The Big Picture</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.1_programming_fundamentals.html#error-handling-with-tryexcept",
    "href": "notebooks/02_programming_basics/2.1_programming_fundamentals.html#error-handling-with-tryexcept",
    "title": "🧠 Programming Fundamentals: The Big Picture",
    "section": "🚨 Error Handling with try/except",
    "text": "🚨 Error Handling with try/except\nUse try/except to catch and handle errors gracefully. This keeps your program running when something unexpected happens.\ntry:\n    result = 100 / user_input  # might be zero or non-numeric\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero — setting result to 0.\")\n    result = 0\nexcept (TypeError, ValueError):\n    print(\"Invalid input, please enter a number.\")\n    result = None",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>🧠 Programming Fundamentals: The Big Picture</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.1_programming_fundamentals.html#in-summary",
    "href": "notebooks/02_programming_basics/2.1_programming_fundamentals.html#in-summary",
    "title": "🧠 Programming Fundamentals: The Big Picture",
    "section": "🦛 In Summary",
    "text": "🦛 In Summary\n\nA program is instructions + data.\nUse clear naming, consistent indentation, and thoughtful comments.\nError handling prevents crashes and aids debugging.\n\nNext up: Dive into Python’s syntax, variables, and comments in 2.1_syntax_variables_comments.ipynb — your first hands-on coding session! 🎉",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>🧠 Programming Fundamentals: The Big Picture</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.2_data_types_conversion.html",
    "href": "notebooks/02_programming_basics/2.2_data_types_conversion.html",
    "title": "🧮 2.2 Data Types and Conversion (Revised)",
    "section": "",
    "text": "🔍 Exploring Data Types\nThis notebook explores Python data types and type conversion, critical for handling nutrition data accurately—and now includes Boolean examples, enhanced error handling, and dtype introspection.\nObjectives: - Understand basic data types: integers, floats, strings, booleans, dates. - Perform type conversion for data consistency. - Use else and finally in try/except. - Inspect pandas dtypes after conversion. - Handle missing values effectively.\nContext: Correct data types ensure reliable calculations, such as averaging nutrient intakes.\nCreate variables with different data types for a hippo’s diet and check their types.\nhippo_id = 'H1'             # string\ncalories = 2500             # integer\nprotein = 80.5              # float\nis_hungry = True            # boolean\n\nprint(f\"hippo_id: {hippo_id}  → {type(hippo_id)}\")\nprint(f\"calories: {calories}      → {type(calories)}\")\nprint(f\"protein: {protein}     → {type(protein)}\")\nprint(f\"is_hungry: {is_hungry}    → {type(is_hungry)}\")",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>🧮 2.2 Data Types and Conversion (Revised)</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.2_data_types_conversion.html#type-conversion",
    "href": "notebooks/02_programming_basics/2.2_data_types_conversion.html#type-conversion",
    "title": "🧮 2.2 Data Types and Conversion (Revised)",
    "section": "🔄 Type Conversion",
    "text": "🔄 Type Conversion\nConvert string nutrient values to numeric types for calculations.\n\niron_str = '8.2'             # nutrient as string\niron_float = float(iron_str)\n\nhungry_str = 'True'          # boolean as string\nhungry_bool = hungry_str.lower() == 'true'  # robust conversion\n\nprint(f\"Iron: {iron_str} → {iron_float} ({type(iron_float)})\")\nprint(f\"Hungry: {hungry_str} → {hungry_bool} ({type(hungry_bool)})\")",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>🧮 2.2 Data Types and Conversion (Revised)</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.2_data_types_conversion.html#enhanced-tryexcept-with-else-and-finally",
    "href": "notebooks/02_programming_basics/2.2_data_types_conversion.html#enhanced-tryexcept-with-else-and-finally",
    "title": "🧮 2.2 Data Types and Conversion (Revised)",
    "section": "🚨 Enhanced try/except with else and finally",
    "text": "🚨 Enhanced try/except with else and finally\nDemonstrate how to handle errors gracefully and run cleanup code.\n\nsample = 'not_a_number'\n\ntry:\n    val = float(sample)\nexcept ValueError:\n    val = np.nan\n    print(\"⚠️ Conversion failed, set to NaN\")\nelse:\n    print(\"✅ Conversion succeeded, value =\", val)\nfinally:\n    print(\"🔚 Finished attempt to convert sample\")",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>🧮 2.2 Data Types and Conversion (Revised)</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.2_data_types_conversion.html#loading-cleaning-mixed-type-data",
    "href": "notebooks/02_programming_basics/2.2_data_types_conversion.html#loading-cleaning-mixed-type-data",
    "title": "🧮 2.2 Data Types and Conversion (Revised)",
    "section": "📊 Loading & Cleaning Mixed-Type Data",
    "text": "📊 Loading & Cleaning Mixed-Type Data\nIn real-world datasets, you might see %, units, or currency mixed in. Use pandas string methods to clean before conversion.\n\ndf = pd.DataFrame({\n    'Iron (%)': ['45%', '50%', None, 'NA'],\n    'Dose (mg)': ['10mg', '12mg', '15 mg', '']\n})\n\n# Strip '%' and convert\ndf['Iron (%)'] = (\n    df['Iron (%)']\n      .str.rstrip('%')\n      .replace({'NA': None})\n      .astype(float)\n)\n\n# Strip 'mg' and whitespace, then convert\ndf['Dose (mg)'] = (\n    df['Dose (mg)']\n      .str.replace(r'\\s*mg', '', regex=True)\n      .replace({'': None})\n      .astype(float)\n)\n\nprint(df)\nprint(\"\\nDataFrame dtypes:\")\nprint(df.dtypes)",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>🧮 2.2 Data Types and Conversion (Revised)</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.2_data_types_conversion.html#working-with-dates",
    "href": "notebooks/02_programming_basics/2.2_data_types_conversion.html#working-with-dates",
    "title": "🧮 2.2 Data Types and Conversion (Revised)",
    "section": "📅 Working with Dates",
    "text": "📅 Working with Dates\nConvert strings to pandas datetime and inspect.\n\ndate_strs = ['2024-01-01', '01/02/2024', 'March 3, 2024']\ndates = pd.to_datetime(date_strs)\nprint(dates)\nprint(\"dtype of dates object:\", dates.dtype)",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>🧮 2.2 Data Types and Conversion (Revised)</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.2_data_types_conversion.html#handling-missing-data-dtype-introspection",
    "href": "notebooks/02_programming_basics/2.2_data_types_conversion.html#handling-missing-data-dtype-introspection",
    "title": "🧮 2.2 Data Types and Conversion (Revised)",
    "section": "🕳 Handling Missing Data & dtype Introspection",
    "text": "🕳 Handling Missing Data & dtype Introspection\nUse pandas coercion and inspect final dtypes.\n\nraw = ['80.5', 'NaN', '75.0', '', None, 'missing']\ndf2 = pd.DataFrame({'Protein (g)': raw})\ndf2['Protein (g)'] = pd.to_numeric(df2['Protein (g)'], errors='coerce')\n\nprint(df2)\nprint(\"\\nAfter coercion, dtypes:\")\nprint(df2.dtypes)\n\n# Optional: fill or drop\nfilled = df2.fillna(0)\nprint(\"\\nFilled missing with 0:\")\nprint(filled)",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>🧮 2.2 Data Types and Conversion (Revised)</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.2_data_types_conversion.html#conclusion",
    "href": "notebooks/02_programming_basics/2.2_data_types_conversion.html#conclusion",
    "title": "🧮 2.2 Data Types and Conversion (Revised)",
    "section": "✅ Conclusion",
    "text": "✅ Conclusion\nYou’ve learned: - Core Python types (including bool). - Enhanced try/except with else & finally. - Cleaning mixed-type columns (%, mg, NA). - Date parsing with pd.to_datetime. - Missing data coercion & dtype inspection with pandas.\nNext Steps: Explore functions and loops in 2.3.\nResources: - Python stdtypes - pandas.to_numeric - Course repo: https://github.com/ggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>🧮 2.2 Data Types and Conversion (Revised)</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.3_functions_loops.html",
    "href": "notebooks/02_programming_basics/2.3_functions_loops.html",
    "title": "🚀 2.3 Functions, Loops, and Control Flow in Python",
    "section": "",
    "text": "🧠 Why These Concepts Matter\nThis notebook introduces three fundamental programming concepts in Python that will help you write efficient, readable, and reusable code for data analysis in nutrition and food science:\nWithout these tools, you’d have to write the same code over and over. Instead, you can: - Use functions to wrap a task and reuse it whenever needed. - Use loops to apply actions across a dataset. - Use if/else to act differently depending on the situation.\nFor example, you might want to: - Convert calories to kilojoules for all hippos in a dataset. - Calculate the percentage of nutrient reference intake. - Flag hippos with low calcium intake.",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>🚀 2.3 Functions, Loops, and Control Flow in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.3_functions_loops.html#what-is-a-function-in-python",
    "href": "notebooks/02_programming_basics/2.3_functions_loops.html#what-is-a-function-in-python",
    "title": "🚀 2.3 Functions, Loops, and Control Flow in Python",
    "section": "🧪 What Is a Function in Python?",
    "text": "🧪 What Is a Function in Python?\nA function in Python is a named block of reusable code that performs a specific task. Think of it like a mini-program within your programme. Functions help you organise, reuse, and simplify your code—especially useful when analysing large nutrition datasets or performing repetitive tasks (like unit conversions or calculations for every participant).\n\n🧭 Why Use Functions?\n\n✅ Avoid repetition: Write code once, use it many times.\n✅ Make code clearer: Logical blocks are easier to understand.\n✅ Support teamwork: Functions allow modularity—team members can focus on specific tasks.\n✅ Enable testing and debugging: You can test individual functions to ensure they behave as expected.\n\n\n\n\n🧰 Defining a Function\nYou define a function in Python using the def keyword, followed by the function name and parentheses ().\ndef greet():\n    \"\"\"This function prints a greeting.\"\"\"\n    print(\"Hello, hippo!\")\nTo use it, you call the function by name with ():\ngreet()\n\n\n\n📦 Functions with Inputs (Arguments)\nYou can pass inputs into functions—these are called arguments. For example, to create a function that greets a specific hippo by ID:\ndef greet_hippo(hippo_id):\n    \"\"\"Greets a hippo by ID.\"\"\"\n    print(f\"Hello, Hippo {hippo_id}!\")\nThen call it like this:\ngreet_hippo(\"H3\")\n\n\n\n🎯 Functions That Return Values\nSome functions do a calculation and return a result using the return keyword:\ndef nutrient_percentage(intake, reference):\n    \"\"\"Returns intake as a percentage of reference.\"\"\"\n    return round((intake / reference) * 100, 1)\nCalling this function:\nresult = nutrient_percentage(8.2, 15)\nprint(result)  # 54.7\n\n\n\n📘 What Is a Docstring?\nA docstring is a special comment enclosed in triple quotes \"\"\" that explains what the function does, its inputs, and output. These appear when using help() and are best practice in well-written code:\ndef calculate_bmi(weight, height):\n    \"\"\"\n    Calculates Body Mass Index (BMI).\n    \n    Args:\n        weight (float): Weight in kilograms.\n        height (float): Height in meters.\n        \n    Returns:\n        float: BMI value\n    \"\"\"\n    return weight / (height ** 2)\n\n\n\n🔒 Scope: Where Variables Exist\nVariables defined inside a function only exist while that function runs. This is called local scope.\ndef local_example():\n    x = 42  # This exists only inside the function\n    print(x)\n\nlocal_example()\n# print(x)  # ❌ This would give an error\nVariables declared outside a function have global scope.\n\n\n\n🧼 Best Practices for Writing Functions\n\nUse meaningful names (e.g. calculate_energy_density instead of func1)\nAlways include a docstring\nKeep functions short and focused on one task\nDon’t use global variables inside functions\nMake functions testable by avoiding print() inside unless needed\n\n\n\n📚 *args & **kwargs\nSometimes you want a function to accept any number of additional arguments:\ndef summarize_intakes(name, *args, **kwargs):\n    \"\"\"Prints a name, then any number of numeric intakes (args),\n    and any number of named extra info (kwargs).\"\"\"\n    print(f\"Summary for {name}:\")\n    if args:\n        print(\"  Intakes:\", args)\n    if kwargs:\n        print(\"  Details:\")\n        for key, value in kwargs.items():\n            print(f\"    {key} = {value}\")\n\n# Example calls:\nsummarize_intakes(\"H1\", 2500, 80.5)\n# Summary for H1:\n#   Intakes: (2500, 80.5)\n\nsummarize_intakes(\"H2\", 2400, 78.0, season=\"summer\", habitat=\"river\")\n# Summary for H2:\n#   Intakes: (2400, 78.0)\n#   Details:\n#     season = summer\n#     habitat = river\n\n*args collects extra positional arguments as a tuple.\n\n**kwargs collects extra named arguments as a dict.\n\nUse them sparingly—only when you really need flexible signatures.",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>🚀 2.3 Functions, Loops, and Control Flow in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.3_functions_loops.html#what-is-a-loop",
    "href": "notebooks/02_programming_basics/2.3_functions_loops.html#what-is-a-loop",
    "title": "🚀 2.3 Functions, Loops, and Control Flow in Python",
    "section": "🔄 What Is a Loop?",
    "text": "🔄 What Is a Loop?\nA loop is a block of code that runs repeatedly based on a condition. Python primarily supports two types of loops:\n\nfor loops: Used to iterate over a sequence (like a list or string)\nwhile loops: Repeats as long as a condition is True",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>🚀 2.3 Functions, Loops, and Control Flow in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.3_functions_loops.html#example-for-loop",
    "href": "notebooks/02_programming_basics/2.3_functions_loops.html#example-for-loop",
    "title": "🚀 2.3 Functions, Loops, and Control Flow in Python",
    "section": "🧪 Example: for Loop",
    "text": "🧪 Example: for Loop\nhippos = ['H1', 'H2', 'H3']\nfor hippo in hippos:\n    print(f\"Analysing data for hippo {hippo}\")\n\nExplanation:\n\nhippos is a list of values\nThe for loop processes each item one by one\nThe variable hippo takes on each value in the list, and the code inside the loop runs for each",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>🚀 2.3 Functions, Loops, and Control Flow in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.3_functions_loops.html#example-while-loop",
    "href": "notebooks/02_programming_basics/2.3_functions_loops.html#example-while-loop",
    "title": "🚀 2.3 Functions, Loops, and Control Flow in Python",
    "section": "⏳ Example: while Loop",
    "text": "⏳ Example: while Loop\nenergy = 100\nwhile energy &gt; 0:\n    print(f\"Energy remaining: {energy}\")\n    energy -= 20  # decrease energy\n\nExplanation:\n\nThe loop runs as long as energy &gt; 0 is True\nEach time it runs, it prints the energy level and reduces it by 20\nOnce energy reaches 0, the loop stops",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>🚀 2.3 Functions, Loops, and Control Flow in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.3_functions_loops.html#loop-tools-range-enumerate-zip",
    "href": "notebooks/02_programming_basics/2.3_functions_loops.html#loop-tools-range-enumerate-zip",
    "title": "🚀 2.3 Functions, Loops, and Control Flow in Python",
    "section": "🛠️ Loop Tools: range(), enumerate(), zip()",
    "text": "🛠️ Loop Tools: range(), enumerate(), zip()\n\nrange(start, stop, step)\nGenerates a sequence of numbers.\nfor i in range(1, 6):\n    print(f\"Day {i}\")\n\n\nenumerate()\nGives you both index and value during iteration.\nfoods = ['carrots', 'apples', 'hay']\nfor index, item in enumerate(foods):\n    print(f\"{index}: {item}\")\n\n\nzip()\nCombines two or more sequences into tuples.\nhippos = ['H1', 'H2']\nweights = [1500, 1600]\nfor hippo, weight in zip(hippos, weights):\n    print(f\"Hippo {hippo} weighs {weight} kg\")",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>🚀 2.3 Functions, Loops, and Control Flow in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.3_functions_loops.html#best-practices",
    "href": "notebooks/02_programming_basics/2.3_functions_loops.html#best-practices",
    "title": "🚀 2.3 Functions, Loops, and Control Flow in Python",
    "section": "✅ Best Practices",
    "text": "✅ Best Practices\n\nUse for loops with lists, dictionaries, or ranges.\nUse enumerate() when you need an index.\nUse zip() when you want to pair values from multiple lists.\nAvoid modifying a list while looping over it (use a copy if needed).\nKeep loops readable; break large logic into functions.",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>🚀 2.3 Functions, Loops, and Control Flow in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.3_functions_loops.html#summary",
    "href": "notebooks/02_programming_basics/2.3_functions_loops.html#summary",
    "title": "🚀 2.3 Functions, Loops, and Control Flow in Python",
    "section": "🧠 Summary",
    "text": "🧠 Summary\nLoops let you:\n\nAutomate repetition\nProcess data item-by-item\nApply calculations across datasets\nMake your code shorter, clearer, and more efficient\n\nMastering loops is essential for real-world programming in nutrition, science, and beyond!",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>🚀 2.3 Functions, Loops, and Control Flow in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.3_functions_loops.html#what-are-conditionals",
    "href": "notebooks/02_programming_basics/2.3_functions_loops.html#what-are-conditionals",
    "title": "🚀 2.3 Functions, Loops, and Control Flow in Python",
    "section": "🧠 What Are Conditionals?",
    "text": "🧠 What Are Conditionals?\nConditionals check whether something is True or False. Based on the result, Python will execute a specific block of code. This allows your programme to behave differently under different circumstances.",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>🚀 2.3 Functions, Loops, and Control Flow in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.3_functions_loops.html#basic-structure",
    "href": "notebooks/02_programming_basics/2.3_functions_loops.html#basic-structure",
    "title": "🚀 2.3 Functions, Loops, and Control Flow in Python",
    "section": "✅ Basic Structure",
    "text": "✅ Basic Structure\nThe most common keywords in conditional logic are:\n\nif: checks a condition\nelif: checks another condition if the previous one was False\nelse: runs if all previous conditions were False\n\n\n📦 Example\niron_intake = 6.5  # in mg\n\nif iron_intake &lt; 8:\n    print(\"Iron intake is below recommended.\")\nelif iron_intake == 8:\n    print(\"Iron intake is exactly on target.\")\nelse:\n    print(\"Iron intake exceeds recommendation.\")",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>🚀 2.3 Functions, Loops, and Control Flow in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.3_functions_loops.html#why-use-conditionals",
    "href": "notebooks/02_programming_basics/2.3_functions_loops.html#why-use-conditionals",
    "title": "🚀 2.3 Functions, Loops, and Control Flow in Python",
    "section": "🪄 Why Use Conditionals?",
    "text": "🪄 Why Use Conditionals?\n\nTo validate data (e.g., flag invalid or missing values)\nTo filter outputs (e.g., select only low or high values)\nTo perform different calculations based on a scenario\nTo make your script adaptable and smart",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>🚀 2.3 Functions, Loops, and Control Flow in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.3_functions_loops.html#indentation-matters",
    "href": "notebooks/02_programming_basics/2.3_functions_loops.html#indentation-matters",
    "title": "🚀 2.3 Functions, Loops, and Control Flow in Python",
    "section": "⚠️ Indentation Matters",
    "text": "⚠️ Indentation Matters\nJust like loops and functions, conditionals rely on indentation to define blocks of code. Each clause (if/elif/else) ends in a colon : and the block underneath must be indented.\ncalories = 2200\n\nif calories &gt; 2500:\n    print(\"High energy intake\")\nelse:\n    print(\"Within normal range\")",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>🚀 2.3 Functions, Loops, and Control Flow in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.3_functions_loops.html#nested-conditionals",
    "href": "notebooks/02_programming_basics/2.3_functions_loops.html#nested-conditionals",
    "title": "🚀 2.3 Functions, Loops, and Control Flow in Python",
    "section": "🧪 Nested Conditionals",
    "text": "🧪 Nested Conditionals\nYou can also put one conditional inside another, though it’s good practice to keep things readable.\nprotein = 60  # in grams\n\nif protein &gt; 50:\n    if protein &lt; 75:\n        print(\"Moderate protein intake\")\n    else:\n        print(\"High protein intake\")",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>🚀 2.3 Functions, Loops, and Control Flow in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.3_functions_loops.html#pythonic-alternatives",
    "href": "notebooks/02_programming_basics/2.3_functions_loops.html#pythonic-alternatives",
    "title": "🚀 2.3 Functions, Loops, and Control Flow in Python",
    "section": "🐍 Pythonic Alternatives",
    "text": "🐍 Pythonic Alternatives\nSometimes, short decisions can be made with a ternary expression:\nstatus = \"Low\" if iron_intake &lt; 8 else \"OK\"\nprint(status)\nThis is shorthand for a simple if...else.",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>🚀 2.3 Functions, Loops, and Control Flow in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.3_functions_loops.html#summary-1",
    "href": "notebooks/02_programming_basics/2.3_functions_loops.html#summary-1",
    "title": "🚀 2.3 Functions, Loops, and Control Flow in Python",
    "section": "📘 Summary",
    "text": "📘 Summary\n\n\n\nKeyword\nPurpose\n\n\n\n\nif\nChecks a condition\n\n\nelif\nAdditional checks if previous if is False\n\n\nelse\nFallback if all other checks are False\n\n\n\nConditionals are fundamental to creating intelligent, responsive scripts in Python!",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>🚀 2.3 Functions, Loops, and Control Flow in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.3_functions_loops.html#conclusion-tying-it-all-together",
    "href": "notebooks/02_programming_basics/2.3_functions_loops.html#conclusion-tying-it-all-together",
    "title": "🚀 2.3 Functions, Loops, and Control Flow in Python",
    "section": "✅ Conclusion: Tying It All Together",
    "text": "✅ Conclusion: Tying It All Together\nCongratulations! You’ve now met three of Python’s most important programming tools:\n\nFunctions: reusable blocks of code that make your scripts cleaner and more modular\n\nLoops: ways to repeat actions efficiently without copy-pasting code\n\nConditionals: decision-making logic that helps your programme respond to different situations\n\nThese are the core building blocks of any real-world data analysis. Whether you’re calculating daily intakes for a herd of hippos 🦛, validating spreadsheet entries, or flagging unusual results, you’ll use these tools again and again.\nGood programming, like good nutrition, is about balance, structure, and clarity.\n\n\n\n🌟 Advanced Insights (Click to expand)\n\n\n🧠 Lambda Functions (Anonymous Functions)\nSometimes you just need a simple one-line function for quick tasks. Enter the lambda function:\ndouble = lambda x: x * 2\nprint(double(4))  # ➝ 8\nVery handy inside functions like map() or filter().\n\n\n\n🌀 List Comprehensions\nLoops can often be replaced with elegant one-liners:\ncalories = [2500, 2600, 2300]\nkilojoules = [c * 4.184 for c in calories]\n\n\n\n🔁 while Loops\nBesides for, you also have while loops, which run as long as a condition is true:\nx = 0\nwhile x &lt; 5:\n    print(x)\n    x += 1\n\n\n\n⚠️ Nested Logic and Complexity\nWatch out for overly complex nesting! If you have functions inside loops inside conditionals inside other functions… it’s probably time to break it up.\n\n\n👉 Up next: you’ll dive into Python’s data structures—lists, tuples, dictionaries, and sets—essential for organising and managing your nutrition data like a pro.",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>🚀 2.3 Functions, Loops, and Control Flow in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.4_data_structures.html",
    "href": "notebooks/02_programming_basics/2.4_data_structures.html",
    "title": "📊 2.4 Data Structures",
    "section": "",
    "text": "📦 What Are Data Structures?\nThis notebook introduces Python data structures — lists, tuples, sets, dictionaries, NumPy arrays, and pandas DataFrames — for organizing and analysing nutrition data.\nObjectives: - Use lists, tuples, and sets to store and manipulate nutrient data. - Work with dictionaries for labelled data. - Perform numerical operations with NumPy arrays and list comprehensions. - Explore pandas DataFrames for tabular data and read real CSV files. - Convert between different data structures.\nContext: Data structures are the backbone of nutrition datasets, like NDNS or hippo diet logs.\nWhen working with data — nutrient intakes, participant IDs, or sensory scores — we need containers to store, organise, and manipulate it. Different structures suit different tasks:\nIn this notebook, we’ll explore each and see how they apply to nutrition research.",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>📊 2.4 Data Structures</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.4_data_structures.html#what-are-data-structures",
    "href": "notebooks/02_programming_basics/2.4_data_structures.html#what-are-data-structures",
    "title": "📊 2.4 Data Structures",
    "section": "",
    "text": "List: ordered collection of items (like a shopping list).\nTuple: immutable sequence (fixed recipe steps).\nSet: unordered collection of unique elements.\nDictionary: key–value pairs (like nutrition labels).\nNumPy array: fast numerical arrays (like spreadsheets optimized for math).\npandas DataFrame: tabular data (rows + columns, like Excel/R tables).",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>📊 2.4 Data Structures</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.4_data_structures.html#lists-in-python",
    "href": "notebooks/02_programming_basics/2.4_data_structures.html#lists-in-python",
    "title": "📊 2.4 Data Structures",
    "section": "🧺 Lists in Python",
    "text": "🧺 Lists in Python\nA list holds an ordered collection of items, which you can modify, extend, and iterate over.\n\n▶ Creating & Accessing\n# Daily calcium intakes for hippos (mg)\ncalcium_intakes = [1200, 1150, 1250]\n\nprint(calcium_intakes[0])   # First element\nprint(calcium_intakes[-1])  # Last element\n\n\n▶ Common Operations\nlen(calcium_intakes)        # Number of items\nsum(calcium_intakes)        # Total intake\nmax(calcium_intakes)        # Highest value\n\ncalcium_intakes.append(1180)     # Add new intake\ncalcium_intakes[1] = 1190        # Update second value\ncalcium_intakes.remove(1250)     # Remove a value\n\n\n▶ List Comprehension vs. NumPy\n# Python list comprehension\nkilojoules = [c * 4.184 for c in calcium_intakes]\nprint(kilojoules)\n\n# Equivalent NumPy operation\nkj_array = np.array(calcium_intakes) * 4.184\nprint(kj_array)\n\n\n💡 Advanced Tip\n\nLists can store mixed types, even nested lists:\nmixed = ['apple', 3.5, True, [1,2,3]]",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>📊 2.4 Data Structures</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.4_data_structures.html#exercise-list-practice",
    "href": "notebooks/02_programming_basics/2.4_data_structures.html#exercise-list-practice",
    "title": "📊 2.4 Data Structures",
    "section": "🧪 Exercise: List Practice",
    "text": "🧪 Exercise: List Practice\n\nCreate a list of iron intakes for three hippos (8.2, 7.9, 8.5).\nPrint the full list.\nAccess and print the second value.\nAdd another intake (8.1).\nCalculate and print the average.\n\nHint: average = sum(my_list) / len(my_list)\n\n# Your code here",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>📊 2.4 Data Structures</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.4_data_structures.html#tuples-sets",
    "href": "notebooks/02_programming_basics/2.4_data_structures.html#tuples-sets",
    "title": "📊 2.4 Data Structures",
    "section": "🔗 Tuples & Sets",
    "text": "🔗 Tuples & Sets\n\nTuple: immutable ordered sequence. Good for fixed collections (e.g. coordinate pairs).\nSet: unordered collection of unique items. Useful to remove duplicates.\n\n# Tuple example\nhippo_ids = ('H1', 'H2', 'H3')\n\n# Set example\nnutrients = {'Iron', 'Calcium', 'Protein', 'Iron'}  # 'Iron' appears once\nprint(hippo_ids)\nprint(nutrients)",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>📊 2.4 Data Structures</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.4_data_structures.html#exercise-tuples-sets",
    "href": "notebooks/02_programming_basics/2.4_data_structures.html#exercise-tuples-sets",
    "title": "📊 2.4 Data Structures",
    "section": "🧪 Exercise: Tuples & Sets",
    "text": "🧪 Exercise: Tuples & Sets\n\nCreate a tuple of unique hippo IDs: ('H1','H2','H3','H2') and print it.\nCreate a set from the list ['apple','banana','apple','carrot'] and print the result.\n\n\n# Your code here",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>📊 2.4 Data Structures</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.4_data_structures.html#dictionaries",
    "href": "notebooks/02_programming_basics/2.4_data_structures.html#dictionaries",
    "title": "📊 2.4 Data Structures",
    "section": "📚 Dictionaries",
    "text": "📚 Dictionaries\nStore key–value pairs for labelled data.\nhippo_nutrients = {\n    'Iron': 8.2,\n    'Calcium': 1200,\n    'Protein': 80.5\n}\nprint(hippo_nutrients['Calcium'])  # 1200\n\n# Add/update/remove\nhippo_nutrients['Vitamin C'] = 90\nhippo_nutrients['Iron'] = 8.5\ndel hippo_nutrients['Protein']\n\n# Looping\nfor k, v in hippo_nutrients.items():\n    print(f'{k}: {v}')",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>📊 2.4 Data Structures</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.4_data_structures.html#exercise-build-a-dictionary",
    "href": "notebooks/02_programming_basics/2.4_data_structures.html#exercise-build-a-dictionary",
    "title": "📊 2.4 Data Structures",
    "section": "🧪 Exercise: Build a Dictionary",
    "text": "🧪 Exercise: Build a Dictionary\nCreate hippo_H5 with: - 'Calories': 2500\n- 'Protein': 82.0\n- 'Water': 3500\nPrint it, then update 'Water' to 3600.\n\n# Your code here",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>📊 2.4 Data Structures</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.4_data_structures.html#numpy-arrays",
    "href": "notebooks/02_programming_basics/2.4_data_structures.html#numpy-arrays",
    "title": "📊 2.4 Data Structures",
    "section": "🧮 NumPy Arrays",
    "text": "🧮 NumPy Arrays\nFast, vectorised arrays for numerical computing.\nimport numpy as np\niron = np.array([8.2, 7.9, 8.5])\nprint(iron + 1)           # [9.2, 8.9, 9.5]\nprint(np.mean(iron))      # 8.866...",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>📊 2.4 Data Structures</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.4_data_structures.html#exercise-numpy-practice",
    "href": "notebooks/02_programming_basics/2.4_data_structures.html#exercise-numpy-practice",
    "title": "📊 2.4 Data Structures",
    "section": "🧪 Exercise: NumPy Practice",
    "text": "🧪 Exercise: NumPy Practice\n\nCreate calcium = np.array([1200,1150,1250])\n\nPrint it and its type.\n\nCompute the mean.\n\nConvert to grams (*0.001).\n\nAccess the last element.\n\n\n# Your code here",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>📊 2.4 Data Structures</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.4_data_structures.html#pandas-dataframes",
    "href": "notebooks/02_programming_basics/2.4_data_structures.html#pandas-dataframes",
    "title": "📊 2.4 Data Structures",
    "section": "🐼 pandas DataFrames",
    "text": "🐼 pandas DataFrames\nTabular data like spreadsheets.\n# From a dict\ndata = {\n  'ID': ['H1','H2','H3'],\n  'Calories': [2500,2450,2600],\n  'Protein': [80.5,78.0,85.2]\n}\ndf = pd.DataFrame(data)\nprint(df.head())\n\n# From a CSV\ndf2 = pd.read_csv('data/hippo_diets.csv')\ndf2.head()",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>📊 2.4 Data Structures</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.4_data_structures.html#exercise-dataframe-practice",
    "href": "notebooks/02_programming_basics/2.4_data_structures.html#exercise-dataframe-practice",
    "title": "📊 2.4 Data Structures",
    "section": "🧪 Exercise: DataFrame Practice",
    "text": "🧪 Exercise: DataFrame Practice\n\nCreate a DataFrame for hippos H4/H5 with Calories, Protein, Water.\nRead hippo_diets.csv, show first 5 rows.\nFilter rows where Calories &gt; 2500.\n\n\n# Your code here",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>📊 2.4 Data Structures</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.4_data_structures.html#converting-between-structures",
    "href": "notebooks/02_programming_basics/2.4_data_structures.html#converting-between-structures",
    "title": "📊 2.4 Data Structures",
    "section": "🔄 Converting Between Structures",
    "text": "🔄 Converting Between Structures\n\nlist → array: np.array(my_list)\n\narray → list: my_array.tolist()\n\ndict → DataFrame: pd.DataFrame([my_dict])\n\nDataFrame → dict: df.to_dict(orient='records')",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>📊 2.4 Data Structures</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.4_data_structures.html#summary",
    "href": "notebooks/02_programming_basics/2.4_data_structures.html#summary",
    "title": "📊 2.4 Data Structures",
    "section": "📋 Summary",
    "text": "📋 Summary\n\n\n\n\n\n\n\n\n\nStructure\nOrdered?\nMutable?\nWhen to Use\n\n\n\n\nList\nYes\nYes\nOrdered collections, simple sequences\n\n\nTuple\nYes\nNo\nFixed sequences, safe keys for dicts\n\n\nSet\nNo\nYes\nUnique elements, membership tests\n\n\nDictionary\nNo\nYes\nLabelled data, fast lookups by key\n\n\nNumPy Array\nYes\nYes\nFast numerical operations, vectorisation\n\n\nDataFrame\nYes\nYes\nTabular data with mixed types and rich methods\n\n\n\nNext: Dive into object oriented programming in 2.4_oop_basics.ipynb 🦛",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>📊 2.4 Data Structures</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.5_input_output.html",
    "href": "notebooks/02_programming_basics/2.5_input_output.html",
    "title": "🖥️ 2.6 Input & Output in Python",
    "section": "",
    "text": "🖨️ Console Output with print()\nThis notebook covers how to read from and write to the console and files in Python—essential for any data analysis workflow in nutrition science.\nObjectives:\n- Perform console output with print() and formatted strings.\n- Read user input via input().\n- Read and write plain text files.\n- Work with CSV and JSON files for structured data.\nContext: Logging results, ingesting external data, and exporting findings are fundamental steps when working with NDNS or lab data.\nUse print() to display values.",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>🖥️ 2.6 Input & Output in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.5_input_output.html#console-output-with-print",
    "href": "notebooks/02_programming_basics/2.5_input_output.html#console-output-with-print",
    "title": "🖥️ 2.6 Input & Output in Python",
    "section": "",
    "text": "Default: space-separated, newline-terminated.\n\nsep: change separator.\n\nend: change line ending.\n\n\nExamples\nname = \"Hilda\"  # hippo name  \ncalories = 2000  # daily kcal  \n\n# basic print  \nprint(\"Hippo eats\", calories, \"kcal\")  \n\n# f-string formatting  \nprint(f\"Hippo {name} consumes {calories} kcal today.\")  \n\n# format() with specifiers  \nprint(\"{0} ate {1:.1f} kcal\".format(name, calories))  \n\n# custom sep and no newline at end  \nprint(\"Calories:\", calories, sep=\"→\", end=\"  \")  \nprint(\" [logged]\")",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>🖥️ 2.6 Input & Output in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.5_input_output.html#console-input-with-input",
    "href": "notebooks/02_programming_basics/2.5_input_output.html#console-input-with-input",
    "title": "🖥️ 2.6 Input & Output in Python",
    "section": "🎤 Console Input with input()",
    "text": "🎤 Console Input with input()\nUse input(prompt) to get user input as a string.\nname = input(\"Enter hippo name: \")  # user types, e.g. Hilda  \ncal_kcal = float(input(\"Enter calories: \"))  # convert to float  \nprint(f\"Recorded {name} → {cal_kcal} kcal\")",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>🖥️ 2.6 Input & Output in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.5_input_output.html#file-io-plain-text",
    "href": "notebooks/02_programming_basics/2.5_input_output.html#file-io-plain-text",
    "title": "🖥️ 2.6 Input & Output in Python",
    "section": "📂 File I/O: Plain Text",
    "text": "📂 File I/O: Plain Text\nOpen a file with open(), then read or write.\n# Write to text file  \nwith open(\"hippo_log.txt\", \"w\") as f:  \n    f.write(f\"{name},{calories}\\n\")  \n\n# Read from text file  \nwith open(\"hippo_log.txt\", \"r\") as f:  \n    for line in f:  \n        print(\"Line:\", line.strip())",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>🖥️ 2.6 Input & Output in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.5_input_output.html#working-with-csv-files",
    "href": "notebooks/02_programming_basics/2.5_input_output.html#working-with-csv-files",
    "title": "🖥️ 2.6 Input & Output in Python",
    "section": "📑 Working with CSV Files",
    "text": "📑 Working with CSV Files\nUse Python’s built-in csv module for lightweight work, or pandas for power.\n\nUsing csv module\nimport csv  \n\n# Write CSV  \nwith open(\"hippos.csv\", \"w\", newline=\"\") as f:  \n    writer = csv.writer(f)  \n    writer.writerow([\"Name\", \"Calories\"])  \n    writer.writerow([name, calories])  \n\n# Read CSV  \nwith open(\"hippos.csv\", \"r\") as f:  \n    reader = csv.reader(f)  \n    for row in reader:  \n        print(row)  \n\n\nUsing pandas for CSV\nimport pandas as pd  \n\ndf = pd.read_csv(\"hippos.csv\")  \nprint(df.head())",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>🖥️ 2.6 Input & Output in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.5_input_output.html#json-files",
    "href": "notebooks/02_programming_basics/2.5_input_output.html#json-files",
    "title": "🖥️ 2.6 Input & Output in Python",
    "section": "🗄️ JSON Files",
    "text": "🗄️ JSON Files\nUse the json module to store structured data.\nimport json  \n\ndata = {\"hippos\": [{\"name\": name, \"calories\": calories}]}  \n\n# Write JSON  \nwith open(\"hippos.json\", \"w\") as f:  \n    json.dump(data, f, indent=2)  \n\n# Read JSON  \nwith open(\"hippos.json\", \"r\") as f:  \n    data_in = json.load(f)  \nprint(data_in)",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>🖥️ 2.6 Input & Output in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.5_input_output.html#exercises",
    "href": "notebooks/02_programming_basics/2.5_input_output.html#exercises",
    "title": "🖥️ 2.6 Input & Output in Python",
    "section": "🧪 Exercises",
    "text": "🧪 Exercises\n\nConsole I/O: Prompt user for two hippo names and calories, then print() a summary.\n\nText File: Write those records to hippos.txt, then read and display them.\n\nCSV: Save to hippos.csv with headers, then load with pandas and show mean calories.\n\nJSON: Dump the data structure to JSON and read it back.",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>🖥️ 2.6 Input & Output in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.5_input_output.html#conclusion",
    "href": "notebooks/02_programming_basics/2.5_input_output.html#conclusion",
    "title": "🖥️ 2.6 Input & Output in Python",
    "section": "✅ Conclusion",
    "text": "✅ Conclusion\nYou’ve learned how to interact with users and external data sources via console, text files, CSV, and JSON.\nThese skills are crucial when importing raw data or exporting your analysis results in nutrition research.\nNext up: dive into 2.7 Error Handling & Logging for robust, production-ready code!",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>🖥️ 2.6 Input & Output in Python</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.6_oop_basics.html",
    "href": "notebooks/02_programming_basics/2.6_oop_basics.html",
    "title": "🧑‍💻 2.5 Object-Oriented Programming (OOP) Basics",
    "section": "",
    "text": "🧠 What Is OOP?\nWelcome to the final notebook in the Programming Basics module! Here we explore Object-Oriented Programming (OOP), a paradigm for writing code that is organized, modular, and reusable.",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>🧑‍💻 2.5 Object-Oriented Programming (OOP) Basics</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.6_oop_basics.html#what-is-oop",
    "href": "notebooks/02_programming_basics/2.6_oop_basics.html#what-is-oop",
    "title": "🧑‍💻 2.5 Object-Oriented Programming (OOP) Basics",
    "section": "",
    "text": "OOP stands for Object-Oriented Programming.\nIt models software as interacting objects, each bundling data and behavior.\nThink of objects like real-world entities (e.g. Food, Hippo), with attributes (data) and methods (actions).",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>🧑‍💻 2.5 Object-Oriented Programming (OOP) Basics</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.6_oop_basics.html#why-use-oop",
    "href": "notebooks/02_programming_basics/2.6_oop_basics.html#why-use-oop",
    "title": "🧑‍💻 2.5 Object-Oriented Programming (OOP) Basics",
    "section": "🎯 Why Use OOP?",
    "text": "🎯 Why Use OOP?\n\nModularity: Group related data and functions into classes.\nReusability: Inherit common behavior via inheritance.\nMaintainability: Encapsulate complexity; change internals without breaking external code.\nAbstraction: Hide implementation details behind clear interfaces.\nScalability: Easier to manage large codebases by dividing into classes.",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>🧑‍💻 2.5 Object-Oriented Programming (OOP) Basics</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.6_oop_basics.html#terminology-nomenclature",
    "href": "notebooks/02_programming_basics/2.6_oop_basics.html#terminology-nomenclature",
    "title": "🧑‍💻 2.5 Object-Oriented Programming (OOP) Basics",
    "section": "📝 Terminology (Nomenclature)",
    "text": "📝 Terminology (Nomenclature)\n\n\n\n\n\n\n\nTerm\nMeaning\n\n\n\n\nClass\nBlueprint or template for objects\n\n\nObject\nInstance of a class holding actual data\n\n\nAttribute\nData stored in an object (variables on self)\n\n\nMethod\nFunction defined inside a class, operates on instances\n\n\nInheritance\nMechanism to create a new class from an existing one\n\n\nEncapsulation\nHiding internal state; exposing only public interfaces",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>🧑‍💻 2.5 Object-Oriented Programming (OOP) Basics</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.6_oop_basics.html#method-attribute-chaining",
    "href": "notebooks/02_programming_basics/2.6_oop_basics.html#method-attribute-chaining",
    "title": "🧑‍💻 2.5 Object-Oriented Programming (OOP) Basics",
    "section": "🔗 Method & Attribute Chaining",
    "text": "🔗 Method & Attribute Chaining\n\nYou can chain multiple attribute or method accesses using the . operator.\nEach . drills into the result of the previous expression.\n\n\nExamples\n# String methods\ns = \"  hello world  \"\nprint(s.strip().upper().replace(\" \", \"_\"))\n# -&gt; \"HELLO_WORLD\"\n\n# pandas chaining\nimport pandas as pd\ndf = pd.DataFrame({'x': [1, 2, None, 4]})\nprint(\n    df\n    .dropna()\n    .assign(y=lambda d: d.x * 2)\n    .head()\n)\n\nNote: Over-chaining can hurt readability — use intermediate variables if needed.",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>🧑‍💻 2.5 Object-Oriented Programming (OOP) Basics</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.6_oop_basics.html#python-classes-a-practical-introduction",
    "href": "notebooks/02_programming_basics/2.6_oop_basics.html#python-classes-a-practical-introduction",
    "title": "🧑‍💻 2.5 Object-Oriented Programming (OOP) Basics",
    "section": "🧱 Python Classes: A Practical Introduction",
    "text": "🧱 Python Classes: A Practical Introduction\nWe’ll now define a basic class to see these concepts in action.\n\nDefining a Food class\nclass Food:\n    def __init__(self, name, calories):\n        \"\"\"Initialize with name and calories per serving.\"\"\"\n        self.name = name          # attribute\n        self.calories = calories  # attribute\n\n    def describe(self):\n        \"\"\"Return a summary of the food item.\"\"\"\n        return f\"{self.name}: {self.calories} kcal per serving\"\n\napple = Food('Apple', 95)\nbanana = Food('Banana', 120)\nprint(apple.describe())   # Apple: 95 kcal per serving\nprint(banana.describe())  # Banana: 120 kcal per serving",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>🧑‍💻 2.5 Object-Oriented Programming (OOP) Basics</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.6_oop_basics.html#exercise-1-define-your-own-class",
    "href": "notebooks/02_programming_basics/2.6_oop_basics.html#exercise-1-define-your-own-class",
    "title": "🧑‍💻 2.5 Object-Oriented Programming (OOP) Basics",
    "section": "🧪 Exercise 1: Define Your Own Class",
    "text": "🧪 Exercise 1: Define Your Own Class\nCreate a class Drink with: - Attributes: name, volume_ml, sugar_g - Method: sugar_concentration() → grams of sugar per 100 mL\n\n# Your implementation here\nclass Drink:\n    pass\n\n\n\n💡 Solution\n\nclass Drink:\n    def __init__(self, name, volume_ml, sugar_g):\n        self.name = name\n        self.volume_ml = volume_ml\n        self.sugar_g = sugar_g\n\n    def sugar_concentration(self):\n        return (self.sugar_g / self.volume_ml) * 100\n\n# Example\njuice = Drink('Orange Juice', 250, 22)\nprint(f\"{juice.name}: {juice.sugar_concentration():.1f} g sugar/100 mL\")\n```&lt;/details&gt;\n\n## 🥗 Inheritance\n\nExtend `Food` to `Diet` by adding a `diet_type`:\n\n```python\nclass Diet(Food):\n    def __init__(self, name, calories, diet_type):\n        super().__init__(name, calories)\n        self.diet_type = diet_type\n\n    def describe(self):\n        base = super().describe()\n        return f\"{base} ({self.diet_type})\"\n\nketo = Diet('Avocado', 160, 'Keto')\nprint(keto.describe())  # Avocado: 160 kcal per serving (Keto)",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>🧑‍💻 2.5 Object-Oriented Programming (OOP) Basics</span>"
    ]
  },
  {
    "objectID": "notebooks/02_programming_basics/2.6_oop_basics.html#summary-next-steps",
    "href": "notebooks/02_programming_basics/2.6_oop_basics.html#summary-next-steps",
    "title": "🧑‍💻 2.5 Object-Oriented Programming (OOP) Basics",
    "section": "✅ Summary & Next Steps",
    "text": "✅ Summary & Next Steps\n\nOOP models code as objects with attributes and methods.\nInheritance allows code reuse and extension.\nMethod chaining can make code concise—but use sparingly for clarity.\n\n\nExercises:\n\nSnack Class: Inherit from Food, add snack_type & is_healthy().\nMeal Composite: Class taking a list of foods/diets, method total_calories().\ndescribe_meal: Function to print each item’s describe() output.\n\nGreat work! You’re now ready to build complex, modular nutrition analyses with OOP.",
    "crumbs": [
      "02 Programming Basics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>🧑‍💻 2.5 Object-Oriented Programming (OOP) Basics</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/index.html",
    "href": "notebooks/03_data_handling/index.html",
    "title": "Data Handling",
    "section": "",
    "text": "Data Handling 📂\nThis module teaches techniques for importing, cleaning, and transforming nutrition datasets, using hippo-themed data (🦛).",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data Handling</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/index.html#notebooks",
    "href": "notebooks/03_data_handling/index.html#notebooks",
    "title": "Data Handling",
    "section": "Notebooks",
    "text": "Notebooks\n\nWhat is Data\nUnderstand data types and structures.\nView HTML | \nImporting Data\nImport datasets into Python.\nView HTML | \nData Cleaning\nClean and preprocess data.\nView HTML | \nData Transformation\nTransform data with pandas.\nView HTML | \nData Aggregation\nAggregate data for analysis.\nView HTML |",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data Handling</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/index.html#datasets",
    "href": "notebooks/03_data_handling/index.html#datasets",
    "title": "Data Handling",
    "section": "Datasets",
    "text": "Datasets\n\ndata/hippo_nutrients.csv: Used for data handling exercises.",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data Handling</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.0_introduction_to_data_handling.html",
    "href": "notebooks/03_data_handling/3.0_introduction_to_data_handling.html",
    "title": "🧰 3.0 Introduction to Data‑Handling",
    "section": "",
    "text": "Welcome to the wild swamp of data wrangling! In this notebook, we’ll chart the river from raw inputs to tasty summaries, using nutrition and food‑science examples (with a few hippo-themed jokes to keep things hippo-thetical).\n\nLearning Objectives\n\nMap out the end‑to‑end steps from raw inputs to aggregated summaries\n\nIdentify and connect to key nutrition and food‑science data sources\n\nGet familiar with the Python libraries and environment setup\n\n\n\nKey Datasets\n\nhippo_nutrients.csv: Experimental macronutrient and micronutrient profiles for hippopotamus feeding trials\n\nFoodData Central API: REST API access to USDA’s FoodData Central for comprehensive food composition data\n\nclinical_trial_data.xlsx: Simulated dietary intervention results from a randomized trial\n\nLocal SQLite database (fns_db.sqlite): Consolidated tables for nutrient reference values and cohort metadata\n\n\n%run ../../bootstrap.py    # installs requirements + editable package\n\nimport fns_toolkit as fns\n\n\n\n\n\n\n\n### Verify Your Environment\n\n\nRun the following to confirm your versions and ensure everything’s ready for our hippo expedition.\n\n\n::: {#5daf1904 .cell} ``` {.python .cell-code} import sys import pandas as pd import numpy as np\n\n\nprint(f”Python version: {sys.version.split()[0]}“) print(f”pandas version: {pd.__version__}“) print(f”numpy version: {np.__version__}“) ``` :::\n\n\n\n\n\n🚀 Roadmap\nHere’s our journey through Part 3:\n\n3.1 Principles of Tidy Data: Learn the four rules that make data hippo-riffic.\n\n3.2 Importing & Connecting: Pull in datasets from CSVs, APIs, and databases.\n\n3.3 Data Cleaning & Quality Checks: Handle missing values and inconsistent measurements.\n\n3.4 Reshaping & Merging Datasets: Tame wide and long data, and join multiple tables.\n\n3.5 Transformation & Feature Engineering: Create new variables like flavanol ratios.\n\n3.6 Aggregation & Summarization: Summarise intake by cohort and export results.\n\nReady to get your feet wet? Let’s dive into 3.1 next — may your code be ever crash-free and your data as tidy as a basking hippo!",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>🧰 3.0 Introduction to Data‑Handling</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.1_principles_of_tidy_data.html",
    "href": "notebooks/03_data_handling/3.1_principles_of_tidy_data.html",
    "title": "🦛 3.1 Principles of Tidy Data",
    "section": "",
    "text": "📋 The Four Principles of Tidy Data\nIn this notebook, we’ll explore Hadley Wickham’s four principles of tidy data, see why they matter for nutrition research, and apply them to our hippo_nutrients.csv dataset.\nTidy data makes analysis straightforward—no hippo-chasing through messy tables!",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>🦛 3.1 Principles of Tidy Data</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.1_principles_of_tidy_data.html#the-four-principles-of-tidy-data",
    "href": "notebooks/03_data_handling/3.1_principles_of_tidy_data.html#the-four-principles-of-tidy-data",
    "title": "🦛 3.1 Principles of Tidy Data",
    "section": "",
    "text": "Each variable is a column\n\nEach observation is a row\n\nEach type of observational unit forms a table\n\nEach cell contains a single value",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>🦛 3.1 Principles of Tidy Data</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.1_principles_of_tidy_data.html#from-messy-to-tidy-a-visual-example",
    "href": "notebooks/03_data_handling/3.1_principles_of_tidy_data.html#from-messy-to-tidy-a-visual-example",
    "title": "🦛 3.1 Principles of Tidy Data",
    "section": "🐘 From Messy to Tidy: A Visual Example",
    "text": "🐘 From Messy to Tidy: A Visual Example\nMessy data (wide format, multiple values in a cell):\n\n\n\nID\nIron_2024;2025\nCalcium_2024;2025\n\n\n\n\nH1\n8.2; 8.5\n1200; 1250\n\n\n\nTidy data (long format, one value per cell):\n\n\n\nID\nYear\nNutrient\nValue\n\n\n\n\nH1\n2024\nIron\n8.2\n\n\nH1\n2025\nIron\n8.5\n\n\nH1\n2024\nCalcium\n1200\n\n\nH1\n2025\nCalcium\n1250\n\n\n\n\n# Setup environment (Colab/local)\n%run ../../bootstrap.py  # installs requirements + editable package\n\nimport pandas as pd\nimport numpy as np\n\nprint(\"Environment ready. pandas:\", pd.__version__, \"numpy:\", np.__version__)",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>🦛 3.1 Principles of Tidy Data</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.1_principles_of_tidy_data.html#load-preview-hippo_nutrients.csv",
    "href": "notebooks/03_data_handling/3.1_principles_of_tidy_data.html#load-preview-hippo_nutrients.csv",
    "title": "🦛 3.1 Principles of Tidy Data",
    "section": "🔍 Load & Preview hippo_nutrients.csv",
    "text": "🔍 Load & Preview hippo_nutrients.csv\nLet’s load the dataset and inspect its initial structure.\n\ndf = pd.read_csv('hippo_nutrients.csv')\ndf.head()",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>🦛 3.1 Principles of Tidy Data</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.1_principles_of_tidy_data.html#reshaping-with-melt",
    "href": "notebooks/03_data_handling/3.1_principles_of_tidy_data.html#reshaping-with-melt",
    "title": "🦛 3.1 Principles of Tidy Data",
    "section": "🔄 Reshaping with melt()",
    "text": "🔄 Reshaping with melt()\nWe’ll transform from wide to long format, making each nutrient-year combination its own row.\n\n# Assume columns: ID, Age, Sex, Iron_2024, Iron_2025, Calcium_2024, Calcium_2025\ndf_melted = df.melt(id_vars=['ID', 'Age', 'Sex'],\n                    var_name='Nutrient_Year',\n                    value_name='Value')\n# Separate 'Nutrient' and 'Year'\ndf_tidy = df_melted.assign(\n    Nutrient = df_melted['Nutrient_Year'].str.split('_').str[0],\n    Year = df_melted['Nutrient_Year'].str.split('_').str[1].astype(int)\n).drop(columns='Nutrient_Year')\ndf_tidy.head()",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>🦛 3.1 Principles of Tidy Data</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.1_principles_of_tidy_data.html#back-to-wide-with-pivot_table",
    "href": "notebooks/03_data_handling/3.1_principles_of_tidy_data.html#back-to-wide-with-pivot_table",
    "title": "🦛 3.1 Principles of Tidy Data",
    "section": "🔄 Back to Wide with pivot_table()",
    "text": "🔄 Back to Wide with pivot_table()\nTo show inverse operation, pivot back to wide format for Iron values.\n\ndf_pivot = df_tidy[df_tidy['Nutrient']=='Iron'] \\\n    .pivot_table(index=['ID','Age','Sex'],\n                 columns='Year',\n                 values='Value') \\\n    .reset_index()\ndf_pivot.head()",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>🦛 3.1 Principles of Tidy Data</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.1_principles_of_tidy_data.html#exercises",
    "href": "notebooks/03_data_handling/3.1_principles_of_tidy_data.html#exercises",
    "title": "🦛 3.1 Principles of Tidy Data",
    "section": "🐾 Exercises",
    "text": "🐾 Exercises\n\nFilter Iron Data: Use the tidy DataFrame df_tidy to filter only Iron entries.\n\nCompute Average Calcium: Group by ID and compute the average Calcium intake across years.\n\nHint: Use df_tidy[df_tidy['Nutrient']=='Iron'] and groupby.",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>🦛 3.1 Principles of Tidy Data</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.1_principles_of_tidy_data.html#quick-quiz",
    "href": "notebooks/03_data_handling/3.1_principles_of_tidy_data.html#quick-quiz",
    "title": "🦛 3.1 Principles of Tidy Data",
    "section": "❓ Quick Quiz",
    "text": "❓ Quick Quiz\nTrue or False: In tidy data, you can have multiple years combined in one column header.\n\n\nAnswer\n\nFalse. Each variable (e.g., Year) must be its own column.",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>🦛 3.1 Principles of Tidy Data</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.1_principles_of_tidy_data.html#conclusion",
    "href": "notebooks/03_data_handling/3.1_principles_of_tidy_data.html#conclusion",
    "title": "🦛 3.1 Principles of Tidy Data",
    "section": "🎬 Conclusion",
    "text": "🎬 Conclusion\n\nWe covered the four tidy data principles.\n\nConverted messy tables to tidy format using melt and pivot.\n\nNext: Dive into 3.2 Importing & Connecting to pull in external data sources!",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>🦛 3.1 Principles of Tidy Data</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.2_importing_data.html",
    "href": "notebooks/03_data_handling/3.2_importing_data.html",
    "title": "📊 3.2 Importing Data",
    "section": "",
    "text": "Importing a CSV File\nThis notebook covers importing datasets into Python, a critical step for nutrition data analysis.\nObjectives: - Import CSV and Excel files using pandas. - Verify data integrity after import. - Apply importing skills to hippo_nutrients.csv.\nContext: Importing data correctly ensures accurate analysis of nutrition datasets like NDNS.\nLoad hippo_nutrients.csv and verify its contents.\n# Load CSV file\ndf_csv = fns.get_dataset('hippo_nutrients')  # Path relative to notebook\n\n# Verify data\nprint(f'Shape: {df_csv.shape}')  # Display number of rows and columns\nprint(f'Columns: {df_csv.columns.tolist()}')  # Display column names\n\nShape: (100, 6)\nColumns: ['ID', 'Nutrient', 'Year', 'Value', 'Age', 'Sex']",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>📊 3.2 Importing Data</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.2_importing_data.html#importing-an-excel-file",
    "href": "notebooks/03_data_handling/3.2_importing_data.html#importing-an-excel-file",
    "title": "📊 3.2 Importing Data",
    "section": "Importing an Excel File",
    "text": "Importing an Excel File\nAssume hippo_nutrients.xlsx exists (same data as CSV) and import it.\n\n# Load Excel file (commented out as file may not exist)\n# df_excel = pd.read_excel('data/hippo_nutrients.xlsx')  # Path relative to notebook\n# print(f'Excel shape: {df_excel.shape}')  # Display shape\n\n# For this example, reuse CSV data\ndf_excel = df_csv  # Simulate Excel import\nprint(f'Excel shape: {df_excel.shape}')  # Display shape",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>📊 3.2 Importing Data</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.2_importing_data.html#exercise-1-import-and-summarise",
    "href": "notebooks/03_data_handling/3.2_importing_data.html#exercise-1-import-and-summarise",
    "title": "📊 3.2 Importing Data",
    "section": "Exercise 1: Import and Summarise",
    "text": "Exercise 1: Import and Summarise\nImport hippo_nutrients.csv and calculate the mean Value for Iron intakes. Document your code with comments.\nGuidance: Filter for Nutrient == 'Iron' and use df['Value'].mean().\nAnswer:\nMy import and summary code is…",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>📊 3.2 Importing Data</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.2_importing_data.html#conclusion",
    "href": "notebooks/03_data_handling/3.2_importing_data.html#conclusion",
    "title": "📊 3.2 Importing Data",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve learned to import CSV and Excel files, preparing nutrition data for analysis.\nNext Steps: Explore data cleaning in 3.3.\nResources: - Pandas I/O - OpenPyXL Documentation - Repository: github.com/ggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>📊 3.2 Importing Data</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.3_data_cleaning.html",
    "href": "notebooks/03_data_handling/3.3_data_cleaning.html",
    "title": "🧹 3.3 Data Cleaning",
    "section": "",
    "text": "Data Preparation\nThis notebook covers data cleaning techniques to prepare nutrition datasets for analysis.\nObjectives: - Handle missing values and duplicates. - Validate data consistency. - Clean hippo_nutrients.csv for analysis.\nContext: Clean data is essential for reliable nutrition research, like NDNS studies.\nLoad hippo_nutrients.csv and check for issues like missing values.\n# Load the dataset\ndf = fns.get_dataset('hippo_nutrients')  # Path relative to notebook\n\n# Check for missing values\nprint('Missing values:')\nprint(df.isnull().sum())  # Display count of missing values per column\n\nMissing values:\nID          0\nNutrient    0\nYear        0\nValue       8\nAge         0\nSex         0\ndtype: int64",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>🧹 3.3 Data Cleaning</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.3_data_cleaning.html#handling-missing-values",
    "href": "notebooks/03_data_handling/3.3_data_cleaning.html#handling-missing-values",
    "title": "🧹 3.3 Data Cleaning",
    "section": "Handling Missing Values",
    "text": "Handling Missing Values\nFill missing Value entries with the mean for each nutrient.\n\n# Group by nutrient and fill missing values with mean\ndf['Value'] = df.groupby('Nutrient')['Value'].transform(lambda x: x.fillna(x.mean()))\n\n# Verify no missing values\nprint(f'Missing values after filling: {df[\"Value\"].isnull().sum()}')  # Check Value column\n\nMissing values after filling: 0",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>🧹 3.3 Data Cleaning</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.3_data_cleaning.html#removing-duplicates",
    "href": "notebooks/03_data_handling/3.3_data_cleaning.html#removing-duplicates",
    "title": "🧹 3.3 Data Cleaning",
    "section": "Removing Duplicates",
    "text": "Removing Duplicates\nCheck for and remove duplicate rows.\n\n# Check for duplicates\nduplicates = df.duplicated().sum()  # Count duplicate rows\nprint(f'Duplicates: {duplicates}')\n\n# Remove duplicates if any\ndf = df.drop_duplicates()  # Drop duplicate rows\n\nDuplicates: 0",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>🧹 3.3 Data Cleaning</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.3_data_cleaning.html#exercise-1-clean-and-validate",
    "href": "notebooks/03_data_handling/3.3_data_cleaning.html#exercise-1-clean-and-validate",
    "title": "🧹 3.3 Data Cleaning",
    "section": "Exercise 1: Clean and Validate",
    "text": "Exercise 1: Clean and Validate\nFilter for Calcium data, fill missing Value entries with the median, and check for duplicates. Document your code.\nGuidance: Use df[df['Nutrient'] == 'Calcium'] and fillna(df['Value'].median()).\nAnswer:\nMy cleaning code is…",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>🧹 3.3 Data Cleaning</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.3_data_cleaning.html#conclusion",
    "href": "notebooks/03_data_handling/3.3_data_cleaning.html#conclusion",
    "title": "🧹 3.3 Data Cleaning",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve learned to clean nutrition data by handling missing values and duplicates.\nNext Steps: Explore data transformation in 3.4.\nResources: - Pandas Data Cleaning - Data Cleaning Guide - Repository: github.com/ggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>🧹 3.3 Data Cleaning</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.4_reshaping_merging.html",
    "href": "notebooks/03_data_handling/3.4_reshaping_merging.html",
    "title": "🔄 3.4 Data Transformation",
    "section": "",
    "text": "Filtering Data\nThis notebook explores data transformation techniques to prepare nutrition datasets for analysis.\nObjectives: - Filter and group data for insights. - Pivot data for alternative views. - Transform hippo_nutrients.csv for analysis.\nContext: Transformation enables meaningful insights from nutrition data, like comparing nutrient intakes across groups.\n## Data Preparation\nLoad hippo_nutrients.csv and inspect its structure.\nFilter for female hippos and Iron intakes.\n# Filter for female hippos and Iron\ndf_female_iron = df[(df['Sex'] == 'F') & (df['Nutrient'] == 'Iron')]\nprint(df_female_iron.head(2))  # Display filtered data\n\n   ID Nutrient  Year  Value  Age Sex\n0  H1     Iron  2024    8.2   25   F\n1  H1     Iron  2025    8.5   26   F",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>🔄 3.4 Data Transformation</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.4_reshaping_merging.html#grouping-data",
    "href": "notebooks/03_data_handling/3.4_reshaping_merging.html#grouping-data",
    "title": "🔄 3.4 Data Transformation",
    "section": "Grouping Data",
    "text": "Grouping Data\nGroup by Nutrient and calculate mean Value.\n\n# Group by nutrient and compute mean\nmean_values = df.groupby('Nutrient')['Value'].mean()\nprint(mean_values)  # Display mean values\n\nNutrient\nCalcium     1150.0\nIron           8.0\nVitamin_D     10.5\nName: Value, dtype: float64\n\n\n## Pivoting Data\nPivot the data to show Value by Nutrient and Year.\n\n# Pivot data\ndf_pivot = df.pivot_table(values='Value', index='Nutrient', columns='Year', aggfunc='mean')\nprint(df_pivot)  # Display pivoted data\n\nYear      2024  2025\nNutrient            \nCalcium   1150  1140\nIron         8     8\nVitamin_D   10    11\n\n\n## Exercise 1: Transform Data\nFilter for Vitamin_D data in 2024, group by Sex, and compute median Value. Document your code.\nGuidance: Use df[(df['Nutrient'] == 'Vitamin_D') & (df['Year'] == 2024)] and groupby('Sex')['Value'].median().\nAnswer:\nMy transformation code is…",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>🔄 3.4 Data Transformation</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.4_reshaping_merging.html#conclusion",
    "href": "notebooks/03_data_handling/3.4_reshaping_merging.html#conclusion",
    "title": "🔄 3.4 Data Transformation",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve learned to transform nutrition data through filtering, grouping, and pivoting.\nNext Steps: Explore data aggregation in 3.5.\nResources: - Pandas GroupBy - Pandas Pivot - Repository: github.com/ggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>🔄 3.4 Data Transformation</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.5_feature_engineering_and_calculation.html",
    "href": "notebooks/03_data_handling/3.5_feature_engineering_and_calculation.html",
    "title": "🔄 3.4 Data Transformation",
    "section": "",
    "text": "Filtering Data\nThis notebook explores data transformation techniques to prepare nutrition datasets for analysis.\nObjectives: - Filter and group data for insights. - Pivot data for alternative views. - Transform hippo_nutrients.csv for analysis.\nContext: Transformation enables meaningful insights from nutrition data, like comparing nutrient intakes across groups.\n## Data Preparation\nLoad hippo_nutrients.csv and inspect its structure.\nFilter for female hippos and Iron intakes.\n# Filter for female hippos and Iron\ndf_female_iron = df[(df['Sex'] == 'F') & (df['Nutrient'] == 'Iron')]\nprint(df_female_iron.head(2))  # Display filtered data\n\n   ID Nutrient  Year  Value  Age Sex\n0  H1     Iron  2024    8.2   25   F\n1  H1     Iron  2025    8.5   26   F",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>🔄 3.4 Data Transformation</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.5_feature_engineering_and_calculation.html#grouping-data",
    "href": "notebooks/03_data_handling/3.5_feature_engineering_and_calculation.html#grouping-data",
    "title": "🔄 3.4 Data Transformation",
    "section": "Grouping Data",
    "text": "Grouping Data\nGroup by Nutrient and calculate mean Value.\n\n# Group by nutrient and compute mean\nmean_values = df.groupby('Nutrient')['Value'].mean()\nprint(mean_values)  # Display mean values\n\nNutrient\nCalcium     1150.0\nIron           8.0\nVitamin_D     10.5\nName: Value, dtype: float64\n\n\n## Pivoting Data\nPivot the data to show Value by Nutrient and Year.\n\n# Pivot data\ndf_pivot = df.pivot_table(values='Value', index='Nutrient', columns='Year', aggfunc='mean')\nprint(df_pivot)  # Display pivoted data\n\nYear      2024  2025\nNutrient            \nCalcium   1150  1140\nIron         8     8\nVitamin_D   10    11\n\n\n## Exercise 1: Transform Data\nFilter for Vitamin_D data in 2024, group by Sex, and compute median Value. Document your code.\nGuidance: Use df[(df['Nutrient'] == 'Vitamin_D') & (df['Year'] == 2024)] and groupby('Sex')['Value'].median().\nAnswer:\nMy transformation code is…",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>🔄 3.4 Data Transformation</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.5_feature_engineering_and_calculation.html#conclusion",
    "href": "notebooks/03_data_handling/3.5_feature_engineering_and_calculation.html#conclusion",
    "title": "🔄 3.4 Data Transformation",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve learned to transform nutrition data through filtering, grouping, and pivoting.\nNext Steps: Explore data aggregation in 3.5.\nResources: - Pandas GroupBy - Pandas Pivot - Repository: github.com/ggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>🔄 3.4 Data Transformation</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.6_data_aggregation.html",
    "href": "notebooks/03_data_handling/3.6_data_aggregation.html",
    "title": "📈 3.5 Data Aggregation",
    "section": "",
    "text": "Data Preparation\nThis notebook covers data aggregation techniques to summarise and combine nutrition datasets.\nObjectives: - Summarise data with group-by operations. - Join datasets for comprehensive analysis. - Aggregate hippo_nutrients.csv for insights.\nContext: Aggregation provides high-level insights from nutrition data, like average intakes across groups.\nLoad hippo_nutrients.csv and inspect its structure.\n# Load the dataset\ndf = fns.get_dataset('hippo_nutrients')  # Path relative to notebook\nprint(df.head(2))  # Display first two rows\n\n   ID Nutrient  Year  Value  Age Sex\n0  H1     Iron  2024    8.2   25   F\n1  H1     Iron  2025    8.5   26   F\n## Summarising Data\nSummarise Value by Nutrient and Sex using mean and count.\n# Group by Nutrient and Sex, compute mean and count\nsummary = df.groupby(['Nutrient', 'Sex'])['Value'].agg(['mean', 'count'])\nprint(summary)  # Display summary\n\n                   Value       \n                    mean count\nNutrient   Sex                 \nCalcium    F    1150.0    25\n           M    1140.0    25\nIron       F       8.1    25\n           M       8.0    25\nVitamin_D  F      10.6    25\n           M      10.4    25",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>📈 3.5 Data Aggregation</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.6_data_aggregation.html#joining-data",
    "href": "notebooks/03_data_handling/3.6_data_aggregation.html#joining-data",
    "title": "📈 3.5 Data Aggregation",
    "section": "Joining Data",
    "text": "Joining Data\nCreate a small dataset of hippo weights and join with df.\n\n# Create a small weight dataset\nweights = pd.DataFrame({\n    'ID': ['H1', 'H2', 'H3'],\n    'Weight': [2000, 2100, 1950]  # Weight in kg\n})\n\n# Join with main dataset\ndf_joined = df.merge(weights, on='ID', how='left')\nprint(df_joined.head(2))  # Display joined data\n\n   ID Nutrient  Year  Value  Age Sex  Weight\n0  H1     Iron  2024    8.2   25   F    2000\n1  H1     Iron  2025    8.5   26   F    2000",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>📈 3.5 Data Aggregation</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.6_data_aggregation.html#exercise-1-aggregate-and-join",
    "href": "notebooks/03_data_handling/3.6_data_aggregation.html#exercise-1-aggregate-and-join",
    "title": "📈 3.5 Data Aggregation",
    "section": "Exercise 1: Aggregate and Join",
    "text": "Exercise 1: Aggregate and Join\nSummarise mean Value by Nutrient and Year, then join with a dataset of reference intakes (e.g., Iron: 15 mg). Document your code.\nGuidance: Use groupby(['Nutrient', 'Year']) and merge().\nAnswer:\nMy aggregation and join code is…",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>📈 3.5 Data Aggregation</span>"
    ]
  },
  {
    "objectID": "notebooks/03_data_handling/3.6_data_aggregation.html#conclusion",
    "href": "notebooks/03_data_handling/3.6_data_aggregation.html#conclusion",
    "title": "📈 3.5 Data Aggregation",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve learned to aggregate nutrition data through summarisation and joining, unlocking key insights.\nNext Steps: Move to data analysis in 4.1.\nResources: - Pandas Merging - Pandas GroupBy - Repository: github.com/ggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "03 Data Handling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>📈 3.5 Data Aggregation</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/index.html",
    "href": "notebooks/04_data_analysis/index.html",
    "title": "Data Analysis",
    "section": "",
    "text": "Data Analysis 📊\nThis module introduces techniques for exploring and analysing nutrition data, from visualisations to regression models, using hippo-themed datasets (🦛).",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/index.html#notebooks",
    "href": "notebooks/04_data_analysis/index.html#notebooks",
    "title": "Data Analysis",
    "section": "Notebooks",
    "text": "Notebooks\n\nDistributions and Visualisation\nExplore data distributions and create visualisations.\nView HTML | \nExploratory Data Analysis\nPerform EDA to uncover patterns in nutrition data.\nView HTML | \nCorrelation Analysis\nAnalyse relationships between variables.\nView HTML | \nStatistical Testing\nConduct hypothesis tests for nutrition studies.\nView HTML | \nRegression Modelling\nBuild and interpret regression models for vitamin D data.\nView HTML | \nLogistic Regression and Survival Analysis\nApply logistic regression and survival analysis to vitamin D trial outcomes.\nView HTML | \nClinical Trial Analysis\nAnalyse a simulated clinical trial with effect size estimation.\nView HTML |",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/index.html#datasets",
    "href": "notebooks/04_data_analysis/index.html#datasets",
    "title": "Data Analysis",
    "section": "Datasets",
    "text": "Datasets\n\ndata/vitamin_trial.csv: Used in regression, statistical analysis, and survival analysis notebooks.\ndata/simulated_trial.csv: Used in the clinical trial analysis notebook.",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.1_distributions_visualisation.html",
    "href": "notebooks/04_data_analysis/4.1_distributions_visualisation.html",
    "title": "📊 4.1 Data Distributions and Visualisation",
    "section": "",
    "text": "Data Preparation\nThis notebook introduces visualisation techniques for nutrient data distributions, essential for nutrition research.\nObjectives: - Create histograms, boxplots, and violin plots. - Interpret distribution characteristics (e.g., skewness, outliers). - Apply visualizations to real-world nutrition data.\nContext: Visualizing distributions helps identify patterns in datasets like vitamin_trial.csv.\nLoad vitamin_trial.csv, a simulated dataset of vitamin D levels from a clinical trial.\ndf = fns.get_dataset('vitamin_trial')\nprint(df.head(1))\n\n   ID     Group  Vitamin_D  Time  Outcome\n0  P1  Control     10.5     0  Normal",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>📊 4.1 Data Distributions and Visualisation</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.1_distributions_visualisation.html#visualizing-distributions",
    "href": "notebooks/04_data_analysis/4.1_distributions_visualisation.html#visualizing-distributions",
    "title": "📊 4.1 Data Distributions and Visualisation",
    "section": "Visualizing Distributions",
    "text": "Visualizing Distributions\nCreate a violin plot to compare vitamin D levels across trial groups.\n\nplt.figure(figsize=(10, 5))\nsns.violinplot(x='Group', y='Vitamin_D', data=df)\nplt.title('Vitamin D Distribution by Treatment Group')\nplt.xlabel('Trial Group')\nplt.ylabel('Vitamin D (µg)')\nplt.show()",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>📊 4.1 Data Distributions and Visualisation</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.1_distributions_visualisation.html#exercise-1-create-a-boxplot",
    "href": "notebooks/04_data_analysis/4.1_distributions_visualisation.html#exercise-1-create-a-boxplot",
    "title": "📊 4.1 Data Distributions and Visualisation",
    "section": "Exercise 1: Create a Boxplot",
    "text": "Exercise 1: Create a Boxplot\nGenerate a boxplot for the same data and describe any outliers in a Markdown cell.\nGuidance: Use sns.boxplot() and check for extreme values.\nAnswer:\nThe boxplot shows…",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>📊 4.1 Data Distributions and Visualisation</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.1_distributions_visualisation.html#conclusion",
    "href": "notebooks/04_data_analysis/4.1_distributions_visualisation.html#conclusion",
    "title": "📊 4.1 Data Distributions and Visualisation",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve learned to visualize nutrient distributions using violin and boxplots. Next, explore EDA in 4.2.\nResources: - Seaborn Documentation - Matplotlib Documentation - Repository: github.com/ggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>📊 4.1 Data Distributions and Visualisation</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.2_exploratory_data_analysis.html",
    "href": "notebooks/04_data_analysis/4.2_exploratory_data_analysis.html",
    "title": "📊 4.2 Exploratory Data Analysis",
    "section": "",
    "text": "Data Preparation\nThis notebook introduces exploratory data analysis (EDA) techniques to uncover patterns in nutrition datasets.\nObjectives: - Summarise data with descriptive statistics. - Visualise distributions and relationships. - Apply EDA to vitamin_trial.csv.\nContext: EDA is crucial for understanding nutrition data, like vitamin D trial outcomes.\nLoad vitamin_trial.csv, a dataset of vitamin D trial outcomes, and inspect its structure.\n# Load the dataset\ndf = fns.get_dataset('vitamin_trial')  # Path relative to notebook\n\n# Display descriptive statistics\nprint(df[['Vitamin_D']].describe())  # Summarise Vitamin_D column\n\n       Vitamin_D\ncount  200.000000\nmean    12.750000\nstd      2.950000\nmin      9.500000\n25%     10.200000\n50%     12.750000\n75%     15.300000\nmax     16.200000",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>📊 4.2 Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.2_exploratory_data_analysis.html#visualizing-distributions",
    "href": "notebooks/04_data_analysis/4.2_exploratory_data_analysis.html#visualizing-distributions",
    "title": "📊 4.2 Exploratory Data Analysis",
    "section": "Visualizing Distributions",
    "text": "Visualizing Distributions\nCreate a histogram of Vitamin D levels by group.\n\n# Plot histogram\nsns.histplot(data=df, x='Vitamin_D', hue='Group', bins=20, kde=True)\nplt.xlabel('Vitamin D (µg)')  # Label x-axis\nplt.ylabel('Count')  # Label y-axis\nplt.title('Distribution of Vitamin D by Group')  # Plot title\nplt.grid(True, alpha=0.3)  # Add light grid\nplt.show()  # Display plot",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>📊 4.2 Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.2_exploratory_data_analysis.html#exploring-relationships",
    "href": "notebooks/04_data_analysis/4.2_exploratory_data_analysis.html#exploring-relationships",
    "title": "📊 4.2 Exploratory Data Analysis",
    "section": "Exploring Relationships",
    "text": "Exploring Relationships\nCreate a boxplot to compare Vitamin D levels by Group and Outcome.\n\n# Plot boxplot\nsns.boxplot(data=df, x='Group', y='Vitamin_D', hue='Outcome')\nplt.xlabel('Group')  # Label x-axis\nplt.ylabel('Vitamin D (µg)')  # Label y-axis\nplt.title('Vitamin D Levels by Group and Outcome')  # Plot title\nplt.show()  # Display plot",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>📊 4.2 Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.2_exploratory_data_analysis.html#exercise-1-perform-eda",
    "href": "notebooks/04_data_analysis/4.2_exploratory_data_analysis.html#exercise-1-perform-eda",
    "title": "📊 4.2 Exploratory Data Analysis",
    "section": "Exercise 1: Perform EDA",
    "text": "Exercise 1: Perform EDA\nCreate a scatter plot of Vitamin_D vs. Time colored by Group. Describe patterns in a Markdown cell.\nGuidance: Use sns.scatterplot() with x='Time', y='Vitamin_D', hue='Group'.\nAnswer:\nMy scatter plot code and observations are…",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>📊 4.2 Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.2_exploratory_data_analysis.html#conclusion",
    "href": "notebooks/04_data_analysis/4.2_exploratory_data_analysis.html#conclusion",
    "title": "📊 4.2 Exploratory Data Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve applied EDA to explore nutrition data, uncovering patterns through visualizations.\nNext Steps: Explore correlation analysis in 4.3.\nResources: - Seaborn Documentation - Pandas EDA - Repository: github.com/ggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>📊 4.2 Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.3_correlation_analysis.html",
    "href": "notebooks/04_data_analysis/4.3_correlation_analysis.html",
    "title": "📈 4.3 Correlation Analysis",
    "section": "",
    "text": "Data Preparation\nThis notebook explores correlation analysis to identify relationships in nutrition datasets.\nObjectives: - Calculate Pearson and Spearman correlations. - Visualise correlations with heatmaps. - Apply correlation analysis to vitamin_trial.csv.\nContext: Correlation analysis helps understand relationships, like vitamin D and trial outcomes.\nLoad vitamin_trial.csv and select numerical columns for correlation.\n# Load the dataset\ndf = fns.get_dataset('vitamin_trial')  # Path relative to notebook\n\n# Select numerical columns\nnum_cols = ['Vitamin_D', 'Time']  # Numerical columns for correlation\nprint(f'Numerical columns: {num_cols}')  # Display selected columns\n\nNumerical columns: ['Vitamin_D', 'Time']",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>📈 4.3 Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.3_correlation_analysis.html#pearson-correlation",
    "href": "notebooks/04_data_analysis/4.3_correlation_analysis.html#pearson-correlation",
    "title": "📈 4.3 Correlation Analysis",
    "section": "Pearson Correlation",
    "text": "Pearson Correlation\nCalculate Pearson correlation between Vitamin_D and Time.\n\n# Calculate Pearson correlation\ncorr, p_value = pearsonr(df['Vitamin_D'], df['Time'])\nprint(f'Pearson correlation: {round(corr, 2)}, p-value: {p_value:.1e}')  # Display results\n\nPearson correlation: 0.85, p-value: 1.2e-45",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>📈 4.3 Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.3_correlation_analysis.html#correlation-heatmap",
    "href": "notebooks/04_data_analysis/4.3_correlation_analysis.html#correlation-heatmap",
    "title": "📈 4.3 Correlation Analysis",
    "section": "Correlation Heatmap",
    "text": "Correlation Heatmap\nVisualise correlations with a heatmap.\n\n# Compute correlation matrix\ncorr_matrix = df[num_cols].corr(method='pearson')  # Pearson correlation matrix\n\n# Plot heatmap\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.title('Correlation Heatmap')  # Plot title\nplt.show()  # Display plot",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>📈 4.3 Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.3_correlation_analysis.html#exercise-1-spearman-correlation",
    "href": "notebooks/04_data_analysis/4.3_correlation_analysis.html#exercise-1-spearman-correlation",
    "title": "📈 4.3 Correlation Analysis",
    "section": "Exercise 1: Spearman Correlation",
    "text": "Exercise 1: Spearman Correlation\nCalculate Spearman correlation between Vitamin_D and Time and visualise with a scatter plot. Document your code.\nGuidance: Use spearmanr() and sns.scatterplot().\nAnswer:\nMy Spearman correlation code is…",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>📈 4.3 Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.3_correlation_analysis.html#conclusion",
    "href": "notebooks/04_data_analysis/4.3_correlation_analysis.html#conclusion",
    "title": "📈 4.3 Correlation Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve learned to identify relationships in nutrition data using correlation analysis.\nNext Steps: Explore statistical testing in 4.4.\nResources: - SciPy Stats - Seaborn Heatmaps - Repository: github.com/ggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>📈 4.3 Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.4_statistical_testing.html",
    "href": "notebooks/04_data_analysis/4.4_statistical_testing.html",
    "title": "🧪 4.4 Statistical Testing",
    "section": "",
    "text": "Data Preparation\nThis notebook introduces statistical hypothesis testing to compare nutrition data groups.\nObjectives: - Perform t-tests and ANOVA. - Interpret p-values and effect sizes. - Apply tests to vitamin_trial.csv.\nContext: Statistical tests validate differences, like vitamin D levels between trial groups.\nLoad vitamin_trial.csv and split by Group.\n# Load the dataset\ndf = fns.get_dataset('vitamin_trial')  # Path relative to notebook\n\n# Split by group\ncontrol = df[df['Group'] == 'Control']['Vitamin_D']\ntreatment = df[df['Group'] == 'Treatment']['Vitamin_D']\nprint(f'Control mean: {round(control.mean(), 1)}, Treatment mean: {round(treatment.mean(), 1)}')\n\nControl mean: 10.2, Treatment mean: 15.3",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>🧪 4.4 Statistical Testing</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.4_statistical_testing.html#t-test",
    "href": "notebooks/04_data_analysis/4.4_statistical_testing.html#t-test",
    "title": "🧪 4.4 Statistical Testing",
    "section": "T-Test",
    "text": "T-Test\nPerform a t-test to compare Vitamin D levels between groups.\n\n# Perform t-test\nt_stat, p_value = ttest_ind(control, treatment, equal_var=True)\nprint(f'T-test: t={round(t_stat, 1)}, p-value={p_value:.1e}')  # Display results\n\nT-test: t=12.5, p-value=1.3e-20",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>🧪 4.4 Statistical Testing</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.4_statistical_testing.html#exercise-1-anova-test",
    "href": "notebooks/04_data_analysis/4.4_statistical_testing.html#exercise-1-anova-test",
    "title": "🧪 4.4 Statistical Testing",
    "section": "Exercise 1: ANOVA Test",
    "text": "Exercise 1: ANOVA Test\nPerform an ANOVA test to compare Vitamin_D across Outcome groups (Normal, Improved). Interpret the results in a Markdown cell.\nGuidance: Use f_oneway() with groups split by df['Outcome'].\nAnswer:\nMy ANOVA code and interpretation is…",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>🧪 4.4 Statistical Testing</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.4_statistical_testing.html#conclusion",
    "href": "notebooks/04_data_analysis/4.4_statistical_testing.html#conclusion",
    "title": "🧪 4.4 Statistical Testing",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve learned to apply statistical tests to compare nutrition data groups.\nNext Steps: Explore regression modelling in 4.5.\nResources: - SciPy Stats - Statistical Testing Guide - Repository: github.com/ggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>🧪 4.4 Statistical Testing</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.5_regression_modelling.html",
    "href": "notebooks/04_data_analysis/4.5_regression_modelling.html",
    "title": "📉 4.5 Regression Modelling",
    "section": "",
    "text": "Data Preparation\nThis notebook introduces regression modelling to predict nutrition outcomes.\nObjectives: - Build linear regression models. - Evaluate model performance. - Apply regression to vitamin_trial.csv.\nContext: Regression models predict outcomes, like vitamin D levels based on trial data.\nLoad vitamin_trial.csv and prepare features for regression.\n# Load the dataset\ndf = fns.get_dataset('vitamin_trial')  # Path relative to notebook\n\n# Prepare features and target\nX = df[['Time']]  # Feature: Time\ny = df['Vitamin_D']  # Target: Vitamin D levels\nprint(f'Features shape: {X.shape}, Target shape: {y.shape}')  # Display shapes\n\nFeatures shape: (200, 1), Target shape: (200,)",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>📉 4.5 Regression Modelling</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.5_regression_modelling.html#linear-regression",
    "href": "notebooks/04_data_analysis/4.5_regression_modelling.html#linear-regression",
    "title": "📉 4.5 Regression Modelling",
    "section": "Linear Regression",
    "text": "Linear Regression\nBuild and evaluate a linear regression model.\n\n# Initialize and fit model\nmodel = LinearRegression()  # Create regression model\nmodel.fit(X, y)  # Fit model to data\n\n# Predict and evaluate\ny_pred = model.predict(X)  # Predict Vitamin D levels\nr2 = r2_score(y, y_pred)  # Calculate R² score\nprint(f'R² score: {round(r2, 2)}')  # Display R²\n\nR² score: 0.72",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>📉 4.5 Regression Modelling</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.5_regression_modelling.html#exercise-1-build-a-model",
    "href": "notebooks/04_data_analysis/4.5_regression_modelling.html#exercise-1-build-a-model",
    "title": "📉 4.5 Regression Modelling",
    "section": "Exercise 1: Build a Model",
    "text": "Exercise 1: Build a Model\nBuild a regression model using Time and a dummy variable for Group (Control=0, Treatment=1). Report the R² score. Document your code.\nGuidance: Use pd.get_dummies() to encode Group.\nAnswer:\nMy regression code is…",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>📉 4.5 Regression Modelling</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.5_regression_modelling.html#conclusion",
    "href": "notebooks/04_data_analysis/4.5_regression_modelling.html#conclusion",
    "title": "📉 4.5 Regression Modelling",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve learned to build and evaluate regression models for nutrition data.\nNext Steps: Explore Bayesian methods in 5.1.\nResources: - Scikit-Learn Regression - Regression Guide - Repository: github.com/ggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>📉 4.5 Regression Modelling</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/04_logistic_and_survival.html",
    "href": "notebooks/04_data_analysis/04_logistic_and_survival.html",
    "title": "Logistic Regression and Survival Analysis",
    "section": "",
    "text": "📊 4.6 Logistic Regression and Survival Analysis\nThis notebook introduces logistic regression and survival analysis for nutrition research, focusing on binary outcomes and time-to-event data.\nObjectives: - Apply logistic regression to predict binary outcomes. - Perform survival analysis to model time-to-event data. - Use vitamin_trial.csv to analyse vitamin D trial outcomes.\nContext: Logistic regression predicts outcomes like improved health, while survival analysis models time to events, such as response to treatment, in nutrition studies.\n# Setup for Google Colab: Fetch datasets automatically or manually\n%run ../../bootstrap.py    # installs requirements + editable package\n\nimport fns_toolkit as fns\n\nimport pandas as pd\nimport numpy as np",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression and Survival Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/04_logistic_and_survival.html#data-preparation",
    "href": "notebooks/04_data_analysis/04_logistic_and_survival.html#data-preparation",
    "title": "Logistic Regression and Survival Analysis",
    "section": "📥 Data Preparation",
    "text": "📥 Data Preparation\nWe’ll load vitamin_trial.csv, which contains data from a vitamin D trial. The dataset includes: - ID: Participant identifier. - Group: Control or Treatment. - Vitamin_D: Vitamin D level (ng/mL). - Time: Time to outcome (months). - Outcome: Normal or Improved.\nLet’s load and inspect the data to prepare for analysis.\n\n# Load the dataset\ndf = fns.get_dataset('vitamin_trial')  # Path relative to the current working directory (notebooks/04_data_analysis/)\n\n# Display basic information about the dataset\nprint(f'Data shape: {df.shape}')  # Show the number of rows and columns\nprint(f'Sample row: ID={df.iloc[0][\"ID\"]}, Group={df.iloc[0][\"Group\"]}, Vitamin_D={df.iloc[0][\"Vitamin_D\"]}, Time={df.iloc[0][\"Time\"]}, Outcome={df.iloc[0][\"Outcome\"]}')  # Show the first row for inspection\n\n# Display the first few rows to understand the data structure\ndf.head()",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression and Survival Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/04_logistic_and_survival.html#logistic-regression",
    "href": "notebooks/04_data_analysis/04_logistic_and_survival.html#logistic-regression",
    "title": "Logistic Regression and Survival Analysis",
    "section": "📈 Logistic Regression",
    "text": "📈 Logistic Regression\nWe’ll use logistic regression to model the probability of the Outcome being “Improved” (binary outcome) based on predictors Vitamin_D and Group. Logistic regression is ideal for binary classification tasks, predicting the log-odds of the outcome.\nSteps: 1. Encode categorical variables (Group and Outcome) as numerical values. 2. Fit a logistic regression model. 3. Interpret the coefficients.\n\n# Import library for logistic regression\nfrom sklearn.linear_model import LogisticRegression  # Scikit-learn's logistic regression model\nfrom sklearn.preprocessing import LabelEncoder      # For encoding categorical variables as numbers\n\n# Encode categorical variables\n# Convert 'Group' (Control/Treatment) to numerical values: Control=0, Treatment=1\nle_group = LabelEncoder()\ndf['Group_Encoded'] = le_group.fit_transform(df['Group'])\n\n# Convert 'Outcome' (Normal/Improved) to numerical values: Normal=0, Improved=1\nle_outcome = LabelEncoder()\ndf['Outcome_Encoded'] = le_outcome.fit_transform(df['Outcome'])\n\n# Prepare features (X) and target (y) for the model\nX = df[['Vitamin_D', 'Group_Encoded']]  # Features: Vitamin D level and Group\ny = df['Outcome_Encoded']               # Target: Outcome (0 or 1)\n\n# Fit the logistic regression model\nmodel = LogisticRegression(random_state=42)  # random_state ensures reproducibility\nmodel.fit(X, y)                              # Train the model on the data\n\n# Print the coefficients\n# Positive coefficients indicate an increase in the predictor increases the log-odds of 'Improved'\nprint('Logistic Regression Coefficients:')\nprint(f'- Vitamin_D: {model.coef_[0][0]:.3f}')  # Coefficient for Vitamin_D\nprint(f'- Group (Treatment): {model.coef_[0][1]:.3f}')  # Coefficient for Group (Treatment vs Control)\n\n# Interpretation\nprint('\\nInterpretation:')\nprint(f'- A 1-unit increase in Vitamin_D changes the log-odds of \"Improved\" by {model.coef_[0][0]:.3f}.')\nprint(f'- Being in the Treatment group (vs Control) changes the log-odds of \"Improved\" by {model.coef_[0][1]:.3f}.')",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression and Survival Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/04_logistic_and_survival.html#survival-analysis",
    "href": "notebooks/04_data_analysis/04_logistic_and_survival.html#survival-analysis",
    "title": "Logistic Regression and Survival Analysis",
    "section": "⏳ Survival Analysis",
    "text": "⏳ Survival Analysis\nSurvival analysis models the time to an event. Here, we’ll estimate Kaplan-Meier survival curves to model the time to Outcome = “Improved”, stratified by Group. The Kaplan-Meier method is a non-parametric approach to estimate survival probabilities over time.\nSteps: 1. Create an event indicator (1 if Outcome = Improved, 0 otherwise). 2. Fit Kaplan-Meier curves for each group. 3. Plot the survival curves to compare groups.\n\n# Import libraries for survival analysis and plotting\nfrom lifelines import KaplanMeierFitter  # For Kaplan-Meier survival analysis\nimport matplotlib.pyplot as plt         # For plotting survival curves\n\n# Create an event indicator\n# Event = 1 if Outcome is 'Improved', 0 if 'Normal'\ndf['Event'] = df['Outcome'].apply(lambda x: 1 if x == 'Improved' else 0)\n\n# Initialize the Kaplan-Meier fitter\nkmf = KaplanMeierFitter()\n\n# Set up the plot\nplt.figure(figsize=(8, 6))\n\n# Fit and plot survival curves for each group\nfor group in ['Control', 'Treatment']:\n    # Create a mask to filter data for the current group\n    mask = df['Group'] == group\n    # Fit the Kaplan-Meier model to the group's data\n    kmf.fit(df[mask]['Time'], df[mask]['Event'], label=group)\n    # Plot the survival curve\n    kmf.plot_survival_function()\n\n# Add plot labels and title\nplt.title('Kaplan-Meier Survival Curves by Group')\nplt.xlabel('Time (Months)')\nplt.ylabel('Survival Probability (Not Improved)')\nplt.grid(True)  # Add a grid for readability\nplt.show()      # Display the survival curves\n\n# Note: In this context, 'survival' means the probability of not having the event (Outcome = Improved).\n# A lower curve indicates a higher probability of 'Improved' occurring earlier.",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression and Survival Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/04_logistic_and_survival.html#exercises-extend-the-analysis",
    "href": "notebooks/04_data_analysis/04_logistic_and_survival.html#exercises-extend-the-analysis",
    "title": "Logistic Regression and Survival Analysis",
    "section": "🧪 Exercises: Extend the Analysis",
    "text": "🧪 Exercises: Extend the Analysis\nLet’s deepen your understanding with two tasks:\n\nExtend Logistic Regression: Add Time as a predictor in the logistic regression model and report the new coefficients.\nSurvival Analysis: Compute the median survival time (time to 50% probability of not having the event) for each group.\n\nGuidance: - For the logistic regression, include Time in the feature matrix X and re-fit the model. - For survival analysis, use kmf.median_survival_time_ after fitting the Kaplan-Meier model to get the median survival time.\nYour Answers:\nExercise 1: Extend Logistic Regression\nAdd Time as a predictor and report the coefficients.\n# Extend the feature matrix to include Time\nX_extended = df[['Vitamin_D', 'Group_Encoded', 'Time']]\ny_extended = df['Outcome_Encoded']\n\n# Fit the extended logistic regression model\nmodel_extended = LogisticRegression(random_state=42)\nmodel_extended.fit(X_extended, y_extended)\n\n# Print the coefficients\nprint('Extended Logistic Regression Coefficients:')\nprint(f'- Vitamin_D: {model_extended.coef_[0][0]:.3f}')\nprint(f'- Group (Treatment): {model_extended.coef_[0][1]:.3f}')\nprint(f'- Time: {model_extended.coef_[0][2]:.3f}')\nCoefficients: - Vitamin_D: [Your Result] - Group (Treatment): [Your Result] - Time: [Your Result]\nExercise 2: Median Survival Times\nCompute the median survival time for each group.\n# Compute median survival times\nkmf = KaplanMeierFitter()\nfor group in ['Control', 'Treatment']:\n    mask = df['Group'] == group\n    kmf.fit(df[mask]['Time'], df[mask]['Event'], label=group)\n    median_time = kmf.median_survival_time_\n    print(f'Median survival time for {group}: {median_time:.1f} months')\nMedian Survival Times: - Control: [Your Result] months - Treatment: [Your Result] months",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression and Survival Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/04_logistic_and_survival.html#conclusion",
    "href": "notebooks/04_data_analysis/04_logistic_and_survival.html#conclusion",
    "title": "Logistic Regression and Survival Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve applied logistic regression and survival analysis to model vitamin D trial outcomes, uncovering predictors of improvement and time-to-event patterns. These techniques are powerful for understanding health outcomes in nutrition research.\nNext Steps: Explore clinical trial analysis in 4.7_clinical_trial_analysis.ipynb or dive into advanced topics in notebooks/05_advanced/.\nResources: - Scikit-Learn Documentation - Lifelines Documentation - Repository: github.com/ggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression and Survival Analysis</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.7_clinical_trial_analysis.html",
    "href": "notebooks/04_data_analysis/4.7_clinical_trial_analysis.html",
    "title": "🩺 4.7 Analysing a Simulated Clinical Trial",
    "section": "",
    "text": "🧠 What is a Clinical Trial?\nIn this notebook, we’ll analyse a simulated clinical trial, a common method in nutrition research to compare outcomes between groups.\nYou will: - Simulate trial data - Create a baseline summary table (Table 1) - Visualise variable distributions - Calculate effect sizes using frequentist and Bayesian approaches - Interpret results, including visualising posterior chains\nA clinical trial is a study where participants are randomly assigned to groups to test the effect of an intervention (e.g. a new diet) on a health outcome.",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>🩺 4.7 Analysing a Simulated Clinical Trial</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.7_clinical_trial_analysis.html#step-1-simulate-the-data",
    "href": "notebooks/04_data_analysis/4.7_clinical_trial_analysis.html#step-1-simulate-the-data",
    "title": "🩺 4.7 Analysing a Simulated Clinical Trial",
    "section": "🧪 Step 1: Simulate the Data",
    "text": "🧪 Step 1: Simulate the Data\nWe simulate a dataset for 100 participants, with two groups: - Control (group = 0) - Intervention (group = 1)\nWe’ll generate age, BMI, and a simulated outcome (e.g., biomarker change).\n\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\nn = 100\ndf = pd.DataFrame({\n    'participant_id': range(1, n+1),\n    'age': np.random.normal(40, 10, n),\n    'bmi': np.random.normal(27, 4, n),\n    'group': np.random.choice([0, 1], size=n)\n})\ndf['outcome'] = np.where(\n    df['group'] == 0,\n    np.random.normal(0, 2, n),\n    np.random.normal(1, 2, n)\n)\n\ndf.head()",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>🩺 4.7 Analysing a Simulated Clinical Trial</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.7_clinical_trial_analysis.html#step-2-baseline-table",
    "href": "notebooks/04_data_analysis/4.7_clinical_trial_analysis.html#step-2-baseline-table",
    "title": "🩺 4.7 Analysing a Simulated Clinical Trial",
    "section": "📋 Step 2: Baseline Table",
    "text": "📋 Step 2: Baseline Table\nThis shows the average characteristics (age and BMI) in each group, before the intervention’s effect is measured.\n\ntable1 = df.groupby('group')[['age', 'bmi']].agg(['mean', 'std']).round(1)\ntable1.columns = ['Age (Mean)', 'Age (SD)', 'BMI (Mean)', 'BMI (SD)']\ntable1.index = ['Control', 'Intervention']\ntable1['Age'] = table1.apply(lambda row: f\"{row['Age (Mean)']} ± {row['Age (SD)']}\", axis=1)\ntable1['BMI'] = table1.apply(lambda row: f\"{row['BMI (Mean)']} ± {row['BMI (SD)']}\", axis=1)\ntable1[['Age', 'BMI']]",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>🩺 4.7 Analysing a Simulated Clinical Trial</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.7_clinical_trial_analysis.html#step-3-visualise-distributions",
    "href": "notebooks/04_data_analysis/4.7_clinical_trial_analysis.html#step-3-visualise-distributions",
    "title": "🩺 4.7 Analysing a Simulated Clinical Trial",
    "section": "📈 Step 3: Visualise Distributions",
    "text": "📈 Step 3: Visualise Distributions\nWe’ll use histograms and KDEs to visualise the distributions of each variable by group.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(style='whitegrid')\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\nvariables = ['age', 'bmi', 'outcome']\ntitles = ['Age Distribution', 'BMI Distribution', 'Outcome Distribution']\n\nfor i, var in enumerate(variables):\n    sns.histplot(data=df, x=var, hue='group', kde=True, element=\"step\", stat=\"density\",\n                 common_norm=False, palette='Set1', ax=axes[i])\n    axes[i].set_title(titles[i])\n    axes[i].legend(title='Group', labels=['Control', 'Intervention'])\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>🩺 4.7 Analysing a Simulated Clinical Trial</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.7_clinical_trial_analysis.html#step-4-frequentist-analysis",
    "href": "notebooks/04_data_analysis/4.7_clinical_trial_analysis.html#step-4-frequentist-analysis",
    "title": "🩺 4.7 Analysing a Simulated Clinical Trial",
    "section": "📏 Step 4: Frequentist Analysis",
    "text": "📏 Step 4: Frequentist Analysis\nWe use Cohen’s d to measure the size of the effect, and a t-test to check for statistical significance.\n\nfrom scipy.stats import ttest_ind\n\ngroup0 = df[df['group'] == 0]['outcome']\ngroup1 = df[df['group'] == 1]['outcome']\n\nmean_diff = group1.mean() - group0.mean()\npooled_sd = np.sqrt((group0.var() + group1.var()) / 2)\ncohens_d = mean_diff / pooled_sd\n\nt_stat, p_val = ttest_ind(group1, group0)\n\nprint(f\"Cohen's d: {cohens_d:.2f}\")\nprint(f\"T-test: t = {t_stat:.2f}, p = {p_val:.3f}\")",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>🩺 4.7 Analysing a Simulated Clinical Trial</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.7_clinical_trial_analysis.html#step-5-bayesian-analysis",
    "href": "notebooks/04_data_analysis/4.7_clinical_trial_analysis.html#step-5-bayesian-analysis",
    "title": "🩺 4.7 Analysing a Simulated Clinical Trial",
    "section": "🧠 Step 5: Bayesian Analysis",
    "text": "🧠 Step 5: Bayesian Analysis\nWe use PyMC to estimate the difference in outcome between groups and generate a posterior distribution.\nWe will: - Estimate the posterior mean difference - Calculate a 95% HDI (Highest Density Interval) - Visualise the posterior - Plot the sampling chains to assess convergence\n\nimport pymc as pm\nimport arviz as az\n\nwith pm.Model() as model:\n    mu = pm.Normal(\"mu\", mu=0, sigma=10, shape=2)\n    sigma = pm.HalfNormal(\"sigma\", sigma=2)\n    y_obs = pm.Normal(\"y_obs\", mu=mu[df['group']], sigma=sigma, observed=df['outcome'])\n    diff = pm.Deterministic(\"diff\", mu[1] - mu[0])\n    trace = pm.sample(1000, tune=1000, return_inferencedata=True, progressbar=True)\n\naz.plot_posterior(trace, var_names=[\"diff\"], ref_val=0)\nplt.title(\"Posterior Difference in Means\")\nplt.show()\n\n# Posterior mean and HDI\nposterior_diff = trace.posterior['diff'].values.flatten()\nposterior_mean = posterior_diff.mean()\nhdi_bounds = az.hdi(posterior_diff, hdi_prob=0.95)\n\nprint(f\"Posterior mean difference: {posterior_mean:.2f}\")\nprint(f\"95% HDI: [{hdi_bounds[0]:.2f}, {hdi_bounds[1]:.2f}]\")\n\n\n🔁 Step 5b: Visualise Chains\nThis helps us assess MCMC convergence (i.e. did the sampler explore the space thoroughly?)\n\naz.plot_trace(trace, var_names=[\"diff\"])\nplt.suptitle(\"Trace Plot for Posterior Difference\", y=1.02)\nplt.show()",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>🩺 4.7 Analysing a Simulated Clinical Trial</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.7_clinical_trial_analysis.html#summary",
    "href": "notebooks/04_data_analysis/4.7_clinical_trial_analysis.html#summary",
    "title": "🩺 4.7 Analysing a Simulated Clinical Trial",
    "section": "✅ Summary",
    "text": "✅ Summary\nYou’ve: - Simulated a clinical trial dataset - Analysed it with frequentist and Bayesian methods - Visualised the posterior and trace\nKey Insight:\nBayesian methods offer more nuance, while frequentist methods are simpler and quicker. Both can be valuable.\n\n\n🔁 Optional Exercises\n\nChange the simulated effect size (e.g., Intervention mean = 2). Re-run and compare.\nAdd age as a covariate in the Bayesian model.\nCreate a scatterplot of outcome vs. bmi, coloured by group.",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>🩺 4.7 Analysing a Simulated Clinical Trial</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.8_clinical_trial_analysis_bp.html",
    "href": "notebooks/04_data_analysis/4.8_clinical_trial_analysis_bp.html",
    "title": "🩺 4.8 Advanced Clinical Trial Analysis: Pre/Post Blood Pressure Study",
    "section": "",
    "text": "🧪 Hypothetical Study\nIn this notebook, we simulate and analyse a within-subject clinical trial.\nThis is common in nutrition, where we measure outcomes before and after an intervention.\nWe simulate a study testing the effect of a dietary intervention on systolic blood pressure (SBP).",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>🩺 4.8 Advanced Clinical Trial Analysis: Pre/Post Blood Pressure Study</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.8_clinical_trial_analysis_bp.html#step-1-simulate-data",
    "href": "notebooks/04_data_analysis/4.8_clinical_trial_analysis_bp.html#step-1-simulate-data",
    "title": "🩺 4.8 Advanced Clinical Trial Analysis: Pre/Post Blood Pressure Study",
    "section": "📊 Step 1: Simulate Data",
    "text": "📊 Step 1: Simulate Data\nWe simulate 100 participants: - group: 0 (Control) or 1 (Intervention) - sbp_pre: Baseline systolic BP (mean 140 ± 10) - sbp_post: Post-intervention values with: - No change in Control group - Reduction (mean -5 mmHg) in Intervention group\n\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(123)\nn = 100\ngroup = np.random.choice([0, 1], size=n)  # 0 = Control, 1 = Intervention\nsbp_pre = np.random.normal(140, 10, n)\n\n# Post values depend on group\nsbp_post = sbp_pre + np.where(group == 0,\n                              np.random.normal(0, 5, n),     # Control\n                              np.random.normal(-5, 5, n))    # Intervention\n\ndf = pd.DataFrame({\n    'participant_id': range(1, n+1),\n    'group': group,\n    'sbp_pre': sbp_pre,\n    'sbp_post': sbp_post\n})\n\ndf.head()",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>🩺 4.8 Advanced Clinical Trial Analysis: Pre/Post Blood Pressure Study</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.8_clinical_trial_analysis_bp.html#step-2-visualise-change",
    "href": "notebooks/04_data_analysis/4.8_clinical_trial_analysis_bp.html#step-2-visualise-change",
    "title": "🩺 4.8 Advanced Clinical Trial Analysis: Pre/Post Blood Pressure Study",
    "section": "📈 Step 2: Visualise Change",
    "text": "📈 Step 2: Visualise Change\nWe plot SBP before and after for each group to explore: - Individual trajectories - Average change in each group\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style='whitegrid')\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot individual lines\nfor g, ax in zip([0, 1], axes):\n    sub = df[df['group'] == g]\n    for i in range(len(sub)):\n        ax.plot(['Pre', 'Post'], [sub.iloc[i]['sbp_pre'], sub.iloc[i]['sbp_post']], color='grey', alpha=0.3)\n    sns.pointplot(data=sub.melt(id_vars='participant_id', value_vars=['sbp_pre', 'sbp_post']),\n                  x='variable', y='value', ci='sd', color='red', ax=ax)\n    ax.set_title('Control Group' if g == 0 else 'Intervention Group')\n    ax.set_ylabel('Systolic BP (mmHg)')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>🩺 4.8 Advanced Clinical Trial Analysis: Pre/Post Blood Pressure Study</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.8_clinical_trial_analysis_bp.html#step-3-paired-t-tests",
    "href": "notebooks/04_data_analysis/4.8_clinical_trial_analysis_bp.html#step-3-paired-t-tests",
    "title": "🩺 4.8 Advanced Clinical Trial Analysis: Pre/Post Blood Pressure Study",
    "section": "📏 Step 3: Paired t-tests",
    "text": "📏 Step 3: Paired t-tests\nWe compare the within-subject change in each group separately, and then compare the change between groups.\n\nfrom scipy.stats import ttest_rel, ttest_ind\n\ndf['change'] = df['sbp_post'] - df['sbp_pre']\ncontrol_change = df[df['group'] == 0]['change']\nintervention_change = df[df['group'] == 1]['change']\n\n# Paired t-tests within each group\nt_ctrl, p_ctrl = ttest_rel(df[df['group'] == 0]['sbp_post'], df[df['group'] == 0]['sbp_pre'])\nt_int, p_int = ttest_rel(df[df['group'] == 1]['sbp_post'], df[df['group'] == 1]['sbp_pre'])\n\n# Independent t-test of changes\nt_diff, p_diff = ttest_ind(intervention_change, control_change)\n\nprint(\"Within-group change:\")\nprint(f\"  Control: t = {t_ctrl:.2f}, p = {p_ctrl:.3f}\")\nprint(f\"  Intervention: t = {t_int:.2f}, p = {p_int:.3f}\")\nprint(\"\\nBetween-group difference in change:\")\nprint(f\"  t = {t_diff:.2f}, p = {p_diff:.3f}\")",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>🩺 4.8 Advanced Clinical Trial Analysis: Pre/Post Blood Pressure Study</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.8_clinical_trial_analysis_bp.html#step-4-bayesian-analysis-of-change",
    "href": "notebooks/04_data_analysis/4.8_clinical_trial_analysis_bp.html#step-4-bayesian-analysis-of-change",
    "title": "🩺 4.8 Advanced Clinical Trial Analysis: Pre/Post Blood Pressure Study",
    "section": "🧠 Step 4: Bayesian Analysis of Change",
    "text": "🧠 Step 4: Bayesian Analysis of Change\nWe now use a Bayesian approach to compare change in SBP between groups, using the same logic as before.\nThis time we model the difference in change scores.\n\nimport pymc as pm\nimport arviz as az\n\nwith pm.Model() as model:\n    mu = pm.Normal('mu', mu=0, sigma=10, shape=2)  # One mean for each group\n    sigma = pm.HalfNormal('sigma', sigma=5)\n    y_obs = pm.Normal('y_obs', mu=mu[df['group']], sigma=sigma, observed=df['change'])\n    diff = pm.Deterministic('diff', mu[1] - mu[0])\n    trace = pm.sample(1000, tune=1000, return_inferencedata=True)\n\naz.plot_posterior(trace, var_names=['diff'], ref_val=0)\nplt.title(\"Posterior Difference in SBP Change\")\nplt.show()\n\nposterior_diff = trace.posterior['diff'].values.flatten()\nposterior_mean = posterior_diff.mean()\nhdi = az.hdi(posterior_diff, hdi_prob=0.95)\n\nprint(f\"Posterior mean difference: {posterior_mean:.2f} mmHg\")\nprint(f\"95% HDI: [{hdi[0]:.2f}, {hdi[1]:.2f}]\")\n\n\n🔁 Step 4b: Visualise Sampling Chains\n\naz.plot_trace(trace, var_names=['diff'])\nplt.suptitle(\"Trace Plot for Posterior Difference\", y=1.02)\nplt.show()",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>🩺 4.8 Advanced Clinical Trial Analysis: Pre/Post Blood Pressure Study</span>"
    ]
  },
  {
    "objectID": "notebooks/04_data_analysis/4.8_clinical_trial_analysis_bp.html#summary",
    "href": "notebooks/04_data_analysis/4.8_clinical_trial_analysis_bp.html#summary",
    "title": "🩺 4.8 Advanced Clinical Trial Analysis: Pre/Post Blood Pressure Study",
    "section": "✅ Summary",
    "text": "✅ Summary\nYou have: - Simulated a pre/post BP trial dataset - Visualised change by group - Compared outcomes using paired frequentist and Bayesian methods\nTakeaway:\nPaired designs reduce noise and can increase power.\nBayesian methods help understand the magnitude and certainty of change.\n\n\n🔁 Optional Exercises\n\nIncrease the sample size to 200. What happens to the results?\nChange the intervention effect to -10 mmHg. What do you observe?\nModel SBP with a hierarchical structure (e.g. clinic-level effects).",
    "crumbs": [
      "04 Data Analysis",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>🩺 4.8 Advanced Clinical Trial Analysis: Pre/Post Blood Pressure Study</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/index.html",
    "href": "notebooks/05_advanced/index.html",
    "title": "Advanced Topics",
    "section": "",
    "text": "Advanced Topics 🔍\nThis module covers advanced data analysis techniques for nutrition research, using hippo-themed datasets (🦛).",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Advanced Topics</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/index.html#notebooks",
    "href": "notebooks/05_advanced/index.html#notebooks",
    "title": "Advanced Topics",
    "section": "Notebooks",
    "text": "Notebooks\n\nBayesian Methods\nIntroduction to Bayesian methods for nutrition data.\nView HTML | \nWorkflow Automation\nAutomate data analysis workflows.\nView HTML | \nAdvanced Bayesian Modelling\nApply advanced Bayesian methods.\nView HTML | \nDatabases and SQL\nUse SQL for querying nutrition datasets.\nView HTML | \nDashboards\nBuild interactive dashboards for data exploration.\nView HTML |",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Advanced Topics</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/index.html#datasets",
    "href": "notebooks/05_advanced/index.html#datasets",
    "title": "Advanced Topics",
    "section": "Datasets",
    "text": "Datasets\n\ndata/large_food_log.csv: Used in advanced analysis notebooks.",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Advanced Topics</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/5.1_bayesian_methods.html",
    "href": "notebooks/05_advanced/5.1_bayesian_methods.html",
    "title": "📈 5.1 Bayesian Methods in Nutrition Research",
    "section": "",
    "text": "📊 Load and Explore Data\nThis notebook introduces Bayesian methods for nutrition data analysis, focusing on comparing two groups (e.g., Female vs. Male) using PyMC. Bayesian statistics allow us to model uncertainty and update our beliefs with data—perfect for nutrition studies where variability is common!\nObjectives: - Learn the basics of Bayesian inference. - Apply Bayesian methods to compare nutrient intake between groups. - Visualise posterior distributions using ArviZ.\nContext: In nutrition research, we often compare groups (e.g., vitamin levels in females vs. males). Bayesian methods provide a flexible framework to estimate parameters and quantify uncertainty.\nWe’ll use vitamin_trial.csv to compare vitamin levels between females and males. The dataset includes: - vitamin_level: The measured vitamin level (e.g., Vitamin D in ng/mL). - sex: Group indicator (0 for Female, 1 for Male).\n# Load the dataset\ndata = fns.get_dataset('vitamin_trial')\n\n# Extract vitamin levels and group indicators\ny = data['vitamin_level'].values  # Response variable\ngroup = data['sex'].values        # 0 for Female, 1 for Male\n\n# Quick summary\nprint('Mean vitamin levels:')\nprint(f'Female: {np.mean(y[group == 0]):.1f}')\nprint(f'Male: {np.mean(y[group == 1]):.1f}')",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>📈 5.1 Bayesian Methods in Nutrition Research</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/5.1_bayesian_methods.html#bayesian-model",
    "href": "notebooks/05_advanced/5.1_bayesian_methods.html#bayesian-model",
    "title": "📈 5.1 Bayesian Methods in Nutrition Research",
    "section": "🧠 Bayesian Model",
    "text": "🧠 Bayesian Model\nWe’ll model the vitamin levels as normally distributed with different means for each group:\n\nPriors:\n\nmu[0]: Mean vitamin level for Females ~ Normal(0, 10)\nmu[1]: Mean vitamin level for Males ~ Normal(0, 10)\nsigma: Standard deviation ~ HalfNormal(1)\n\nLikelihood:\n\nvitamin_level ~ Normal(mu[group], sigma)\n\n\n\n# Import Bayesian libraries\nimport pymc as pm\nimport arviz as az\nimport matplotlib.pyplot as plt\n\n# Check PyMC version for compatibility\nprint(f'PyMC version: {pm.__version__}')\n\n# Define the Bayesian model\nwith pm.Model() as model:\n    # Priors for the means of Female (group 0) and Male (group 1)\n    mu = pm.Normal('mu', mu=0, sigma=10, shape=2)  # Mean for each group\n    \n    # Prior for standard deviation\n    sigma = pm.HalfNormal('sigma', sigma=1)\n    \n    # Likelihood: vitamin levels are normally distributed\n    y_obs = pm.Normal('y_obs', mu=mu[group], sigma=sigma, observed=y)\n    \n    # Sample from the posterior\n    trace = pm.sample(1000, tune=1000, return_inferencedata=True)",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>📈 5.1 Bayesian Methods in Nutrition Research</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/5.1_bayesian_methods.html#posterior-analysis",
    "href": "notebooks/05_advanced/5.1_bayesian_methods.html#posterior-analysis",
    "title": "📈 5.1 Bayesian Methods in Nutrition Research",
    "section": "📉 Posterior Analysis",
    "text": "📉 Posterior Analysis\nLet’s calculate the posterior means for each group and visualise the posterior distributions of mu.\n\n# Calculate posterior means for Female and Male\nmu_posterior = trace.posterior['mu'].mean(dim=['chain', 'draw'])\nprint(f'Posterior means: Female={round(float(mu_posterior[0]), 1)}, Male={round(float(mu_posterior[1]), 1)}')\n\n# Visualise posterior distributions\naz.plot_posterior(trace, var_names=['mu'])  # Plot histograms of mu\nplt.show()  # Display plot",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>📈 5.1 Bayesian Methods in Nutrition Research</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/5.1_bayesian_methods.html#exercises",
    "href": "notebooks/05_advanced/5.1_bayesian_methods.html#exercises",
    "title": "📈 5.1 Bayesian Methods in Nutrition Research",
    "section": "🧪 Exercises",
    "text": "🧪 Exercises\n\nChange the Prior: Modify the prior for mu to Normal(5, 5) and re-run the analysis. How do the posterior means change? Write your observations in a Markdown cell.\nAdd a Parameter: Extend the model to include a different sigma for each group (e.g., sigma = pm.HalfNormal('sigma', sigma=1, shape=2)). Re-run the sampling and plot the posteriors for both mu and sigma.\nCompare Groups: Calculate the posterior difference between mu[1] (Male) and mu[0] (Female) and plot its distribution using az.plot_posterior.\n\nGuidance: Use the code above as a starting point. Experiment with priors and parameters to see how they affect the results!\nYour Answers:\nExercise 1: Change the Prior\n[Write your observations here]\nExercise 2: Add a Parameter\n[Write your code and results here]\nExercise 3: Compare Groups\n[Write your code and results here]",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>📈 5.1 Bayesian Methods in Nutrition Research</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/5.1_bayesian_methods.html#conclusion",
    "href": "notebooks/05_advanced/5.1_bayesian_methods.html#conclusion",
    "title": "📈 5.1 Bayesian Methods in Nutrition Research",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve applied Bayesian methods to compare vitamin levels between females and males, calculating posterior means and visualising distributions. Bayesian approaches are powerful for nutrition research, allowing you to incorporate prior knowledge and quantify uncertainty.\nNext Steps: Explore workflow automation in 5.2_workflow_automation.ipynb.\nResources: - PyMC Documentation - ArviZ Documentation - Repository: github.com/ggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>📈 5.1 Bayesian Methods in Nutrition Research</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/5.2_workflow_automation.html",
    "href": "notebooks/05_advanced/5.2_workflow_automation.html",
    "title": "⚙️ 5.2 Workflow Automation",
    "section": "",
    "text": "Data Preparation\nThis notebook introduces workflow automation to streamline nutrition data processing.\nObjectives: - Create reusable data processing functions. - Automate workflows with scripts. - Apply automation to large_food_log.csv.\nContext: Automation saves time in nutrition research, like processing large diet logs.\nLoad large_food_log.csv, a dataset of hippo meal nutrients.\n# Load the dataset\ndf = fns.get_dataset('large_food_log')  # Path relative to notebook\nprint(f'Shape: {df.shape}')  # Display number of rows and columns\n\nShape: (500, 5)",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>⚙️ 5.2 Workflow Automation</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/5.2_workflow_automation.html#automation-functions",
    "href": "notebooks/05_advanced/5.2_workflow_automation.html#automation-functions",
    "title": "⚙️ 5.2 Workflow Automation",
    "section": "Automation Functions",
    "text": "Automation Functions\nCreate a function to summarise nutrient amounts by meal.\n\n# Define summary function\ndef summarize_nutrients(df, group_by='Meal'):\n    \"\"\"Summarize nutrient amounts by specified column.\n    Args:\n        df (DataFrame): Input data\n        group_by (str): Column to group by\n    Returns:\n        DataFrame: Mean nutrient amounts\n    \"\"\"\n    summary = df.groupby([group_by, 'Nutrient'])['Amount'].mean().unstack()\n    return summary\n\n# Apply function\nsummary = summarize_nutrients(df, 'Meal')\nprint(summary.head(4))  # Display first few rows\n\n               Amount\nMeal     Nutrient    \nBreakfast Calcium    300.0\n          Iron        2.5\n          Protein    25.0\n          Vitamin_D  11.0",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>⚙️ 5.2 Workflow Automation</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/5.2_workflow_automation.html#exercise-1-automate-workflow",
    "href": "notebooks/05_advanced/5.2_workflow_automation.html#exercise-1-automate-workflow",
    "title": "⚙️ 5.2 Workflow Automation",
    "section": "Exercise 1: Automate Workflow",
    "text": "Exercise 1: Automate Workflow\nCreate a function to filter df for a specific Nutrient and summarise by Date. Apply it to Protein data. Document your code.\nGuidance: Define a function with df[df['Nutrient'] == nutrient] and groupby('Date').\nAnswer:\nMy automation code is…",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>⚙️ 5.2 Workflow Automation</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/5.2_workflow_automation.html#conclusion",
    "href": "notebooks/05_advanced/5.2_workflow_automation.html#conclusion",
    "title": "⚙️ 5.2 Workflow Automation",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve learned to automate nutrition data workflows with reusable functions.\nNext Steps: Explore advanced Bayesian methods in 5.3.\nResources: - Pandas GroupBy - Automation Guide - Repository: github.com/ggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>⚙️ 5.2 Workflow Automation</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/advanced_bayesian.html",
    "href": "notebooks/05_advanced/advanced_bayesian.html",
    "title": "Advanced Bayesian Modelling",
    "section": "",
    "text": "🧠 5.3 Advanced Bayesian Modelling\nThis notebook explores advanced Bayesian modelling techniques for nutrition data analysis, building on basic Bayesian methods.\nObjectives: - Build hierarchical Bayesian models to account for group variations. - Interpret posterior distributions for nutrition insights. - Apply models to hippo_nutrients.csv to estimate nutrient intake variations.\nContext: Hierarchical Bayesian models are powerful for nutrition research, enabling robust estimation of nutrient intakes across groups, such as hippo populations.",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Advanced Bayesian Modelling</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/advanced_bayesian.html#data-preparation",
    "href": "notebooks/05_advanced/advanced_bayesian.html#data-preparation",
    "title": "Advanced Bayesian Modelling",
    "section": "Data Preparation",
    "text": "Data Preparation\nLoad hippo_nutrients.csv from the data handling module and filter for Iron data to model intakes by sex.\n\n# Load the nutrient dataset\ndf = fns.get_dataset('hippo_nutrients')  # Path relative to advanced module\n\n# Filter for Iron data and remove missing values\ndf_iron = df[df['Nutrient'] == 'Iron'].dropna()\nprint(f'Iron data shape: {df_iron.shape}')  # Display number of rows and columns\n\nIron data shape: (50, 6)",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Advanced Bayesian Modelling</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/advanced_bayesian.html#hierarchical-bayesian-model",
    "href": "notebooks/05_advanced/advanced_bayesian.html#hierarchical-bayesian-model",
    "title": "Advanced Bayesian Modelling",
    "section": "Hierarchical Bayesian Model",
    "text": "Hierarchical Bayesian Model\nBuild a hierarchical Bayesian model to estimate Iron intakes, accounting for differences between female (F) and male (M) hippos.\n\n# Encode Sex as numerical index (F=0, M=1)\nsex_idx = df_iron['Sex'].map({'F': 0, 'M': 1}).values\n\n# Define hierarchical model\nwith pm.Model() as model:\n    # Priors for group means (Female and Male)\n    mu = pm.Normal('mu', mu=8, sigma=2, shape=2)  # Mean Iron intake for F (0) and M (1)\n    sigma = pm.HalfNormal('sigma', sigma=1)  # Shared standard deviation\n    \n    # Likelihood of observed Iron intakes\n    iron = pm.Normal('iron', mu=mu[sex_idx], sigma=sigma, observed=df_iron['Value'])\n    \n    # Sample from posterior distribution\n    trace = pm.sample(1000, tune=1000, return_inferencedata=False)  # 1000 samples after tuning",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Advanced Bayesian Modelling</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/advanced_bayesian.html#posterior-analysis",
    "href": "notebooks/05_advanced/advanced_bayesian.html#posterior-analysis",
    "title": "Advanced Bayesian Modelling",
    "section": "Posterior Analysis",
    "text": "Posterior Analysis\nSummarise and visualise the posterior distributions of Iron intake means for female and male hippos.\n\n# Calculate posterior means for Female and Male\nmu_posterior = trace['mu'].mean(axis=0)\nprint(f'Posterior means: Female={round(mu_posterior[0], 1)}, Male={round(mu_posterior[1], 1)}')\n\n# Visualise posterior distributions\npm.plot_posterior(trace, var_names=['mu'])  # Plot histograms of mu\nplt.show()  # Display plot\n\nPosterior means: Female=8.1, Male=8.0",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Advanced Bayesian Modelling</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/advanced_bayesian.html#exercise-build-your-own-model",
    "href": "notebooks/05_advanced/advanced_bayesian.html#exercise-build-your-own-model",
    "title": "Advanced Bayesian Modelling",
    "section": "Exercise: Build Your Own Model",
    "text": "Exercise: Build Your Own Model\nCreate a hierarchical Bayesian model for Calcium intakes, grouped by Year (2024, 2025). Summarise the posterior means in a Markdown cell below.\nGuidance: - Filter df for Nutrient == 'Calcium' and remove missing values. - Encode Year as a numerical index (e.g., 2024=0, 2025=1). - Use pm.Normal for group means and pm.HalfNormal for sigma. - Sample and summarise the posterior with pm.sample() and trace['mu'].mean().\nAnswer:\nMy Calcium model code and posterior summary are as follows:\n# Your code here\nPosterior Summary:\n\nYear 2024: Mean = [Your Result]\nYear 2025: Mean = [Your Result]",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Advanced Bayesian Modelling</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/advanced_bayesian.html#conclusion",
    "href": "notebooks/05_advanced/advanced_bayesian.html#conclusion",
    "title": "Advanced Bayesian Modelling",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve applied advanced Bayesian modelling to estimate nutrient intake variations, capturing uncertainty across groups.\nNext Steps: Explore database querying with SQL in 5.4_databases_sql.ipynb.\nResources: - PyMC Documentation - Bayesian Analysis Guide - Repository: github.com/ggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Advanced Bayesian Modelling</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/5.4_databases_sql.html",
    "href": "notebooks/05_advanced/5.4_databases_sql.html",
    "title": "🗄️ 5.4 Databases and SQL",
    "section": "",
    "text": "Data Preparation\nThis notebook introduces SQL for querying nutrition databases.\nObjectives: - Create and populate a SQLite database. - Write SQL queries for data extraction. - Apply SQL to large_food_log.csv.\nContext: SQL manages large nutrition datasets, like detailed food logs.\nLoad large_food_log.csv and create a SQLite database.\n# Load the dataset\ndf = fns.get_dataset('large_food_log')  # Path relative to notebook\n\n# Create SQLite database\nconn = sqlite3.connect('nutrition.db')  # Create database\ndf.to_sql('food_log', conn, if_exists='replace', index=False)  # Write to table",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>🗄️ 5.4 Databases and SQL</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/5.4_databases_sql.html#sql-queries",
    "href": "notebooks/05_advanced/5.4_databases_sql.html#sql-queries",
    "title": "🗄️ 5.4 Databases and SQL",
    "section": "SQL Queries",
    "text": "SQL Queries\nQuery the database to summarise nutrient amounts by meal.\n\n# Query: Average nutrient amount by meal\nquery = \"\"\"\nSELECT Meal, Nutrient, AVG(Amount) as Avg_Amount\nFROM food_log\nGROUP BY Meal, Nutrient\n\"\"\"\nresult = pd.read_sql_query(query, conn)\nprint(result.head(2))  # Display first two rows\n\n   Meal Nutrient      Avg_Amount\n0  Breakfast  Calcium  300.0\n1  Breakfast     Iron    2.5",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>🗄️ 5.4 Databases and SQL</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/5.4_databases_sql.html#exercise-1-write-a-query",
    "href": "notebooks/05_advanced/5.4_databases_sql.html#exercise-1-write-a-query",
    "title": "🗄️ 5.4 Databases and SQL",
    "section": "Exercise 1: Write a Query",
    "text": "Exercise 1: Write a Query\nWrite a SQL query to find total Amount for Protein by Date. Document your code.\nGuidance: Use SUM(Amount) and GROUP BY Date.\nAnswer:\nMy SQL query code is…",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>🗄️ 5.4 Databases and SQL</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/5.4_databases_sql.html#conclusion",
    "href": "notebooks/05_advanced/5.4_databases_sql.html#conclusion",
    "title": "🗄️ 5.4 Databases and SQL",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve learned to use SQL for querying nutrition databases.\nNext Steps: Explore dashboards in 5.5.\nResources: - SQLite Documentation - SQL Guide - Repository: github.com/ggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>🗄️ 5.4 Databases and SQL</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/5.5_dashboards.html",
    "href": "notebooks/05_advanced/5.5_dashboards.html",
    "title": "📊 5.5 Creating Dashboards",
    "section": "",
    "text": "Data Preparation\nThis notebook introduces Quarto dashboards for visualizing nutrition data, ideal for presenting research findings.\nObjectives:\nContext: Dashboards consolidate complex data, like hippo nutrient logs, into clear insights. 🦛\nLoad large_food_log.csv, a detailed dataset of hippo meals and nutrients.\ndf = fns.get_dataset('large_food_log')\nprint(df.head(1))\n\n   ID      Meal Nutrient  Amount        Date\n0  H1  Breakfast     Iron     2.5  2024-01-01",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>📊 5.5 Creating Dashboards</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/5.5_dashboards.html#building-a-dashboard",
    "href": "notebooks/05_advanced/5.5_dashboards.html#building-a-dashboard",
    "title": "📊 5.5 Creating Dashboards",
    "section": "Building a Dashboard",
    "text": "Building a Dashboard\nCreate a line plot to visualize nutrient trends over time. This can be embedded in a Quarto dashboard.\n\nplt.figure(figsize=(12, 6))\nsns.lineplot(x='Date', y='Amount', hue='Nutrient', data=df)\nplt.title('Nutrient Trends Over Time')\nplt.xlabel('Date')\nplt.ylabel('Amount (mg or g)')\nplt.xticks(rotation=45)\nplt.show()",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>📊 5.5 Creating Dashboards</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/5.5_dashboards.html#exercise-1-customize-the-dashboard",
    "href": "notebooks/05_advanced/5.5_dashboards.html#exercise-1-customize-the-dashboard",
    "title": "📊 5.5 Creating Dashboards",
    "section": "Exercise 1: Customize the Dashboard",
    "text": "Exercise 1: Customize the Dashboard\nAdd a second plot (e.g., bar plot of nutrients by meal type) and describe its insights in a Markdown cell.\nGuidance: Use sns.catplot() with kind='bar'.\nAnswer:\nThe bar plot shows…",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>📊 5.5 Creating Dashboards</span>"
    ]
  },
  {
    "objectID": "notebooks/05_advanced/5.5_dashboards.html#conclusion",
    "href": "notebooks/05_advanced/5.5_dashboards.html#conclusion",
    "title": "📊 5.5 Creating Dashboards",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve built a Quarto dashboard to visualize nutrient trends. Share it via GitHub Pages or PDF.\nResources:\n\nQuarto Dashboards\nSeaborn Documentation\nRepository: github.com/ggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "05 Advanced",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>📊 5.5 Creating Dashboards</span>"
    ]
  },
  {
    "objectID": "notebooks/06_qualitative/index.html",
    "href": "notebooks/06_qualitative/index.html",
    "title": "Qualitative Research",
    "section": "",
    "text": "Qualitative Research Module 📝\nThis module covers basic qualitative research.",
    "crumbs": [
      "06 Qualitative",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Qualitative Research</span>"
    ]
  },
  {
    "objectID": "notebooks/06_qualitative/index.html#notebooks",
    "href": "notebooks/06_qualitative/index.html#notebooks",
    "title": "Qualitative Research",
    "section": "Notebooks",
    "text": "Notebooks\n\nQualitative methods\nIntroduction to qualitative research methods View HTML | \nText analysis\nConduct text analysis\nView HTML |",
    "crumbs": [
      "06 Qualitative",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Qualitative Research</span>"
    ]
  },
  {
    "objectID": "notebooks/06_qualitative/index.html#datasets",
    "href": "notebooks/06_qualitative/index.html#datasets",
    "title": "Qualitative Research",
    "section": "Datasets",
    "text": "Datasets\n\ndata/large_food_log.csv: Used in advanced analysis notebooks.",
    "crumbs": [
      "06 Qualitative",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Qualitative Research</span>"
    ]
  },
  {
    "objectID": "notebooks/06_qualitative/6.1_intro_qualitative_research.html",
    "href": "notebooks/06_qualitative/6.1_intro_qualitative_research.html",
    "title": "🧾 6.1 Introduction to Qualitative Research",
    "section": "",
    "text": "Data Preparation\nThis notebook explores qualitative methods for nutrition studies, such as analysing food preference surveys.\nObjectives: - Compare quantitative and qualitative approaches. - Analyse textual survey data. - Apply basic text processing techniques.\nContext: Qualitative data, like hippo food preferences, provides rich insights into dietary behaviours. 🦛\nLoad food_preferences.txt, a survey of hippo food preferences.\nfile_path = fns.get_data_path(\"food_preferences\")\n\n\nwith open(file_path, 'r') as f:\n    responses = f.readlines()\nprint(responses[:2])\n\n['Hippo H1: I enjoy crunchy carrots for their sweetness.\\n', 'Hippo H2: Grass is acceptable, but fruit is preferred.\\n']",
    "crumbs": [
      "06 Qualitative",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>🧾 6.1 Introduction to Qualitative Research</span>"
    ]
  },
  {
    "objectID": "notebooks/06_qualitative/6.1_intro_qualitative_research.html#basic-text-analysis",
    "href": "notebooks/06_qualitative/6.1_intro_qualitative_research.html#basic-text-analysis",
    "title": "🧾 6.1 Introduction to Qualitative Research",
    "section": "Basic Text Analysis",
    "text": "Basic Text Analysis\nCount the frequency of words like ‘carrots’ or ‘fruit’ in the responses.\n\nword_counts = {'carrots': 0, 'fruit': 0}\nfor response in responses:\n    for word in word_counts:\n        word_counts[word] += response.lower().count(word)\nprint(f'Word counts: {word_counts}')\n\nWord counts: {'carrots': 10, 'fruit': 15}",
    "crumbs": [
      "06 Qualitative",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>🧾 6.1 Introduction to Qualitative Research</span>"
    ]
  },
  {
    "objectID": "notebooks/06_qualitative/6.1_intro_qualitative_research.html#exercise-1-expand-analysis",
    "href": "notebooks/06_qualitative/6.1_intro_qualitative_research.html#exercise-1-expand-analysis",
    "title": "🧾 6.1 Introduction to Qualitative Research",
    "section": "Exercise 1: Expand Analysis",
    "text": "Exercise 1: Expand Analysis\nAdd another word (e.g., ‘vegetables’) to the count and describe its prevalence in a Markdown cell.\nAnswer:\nThe word ‘vegetables’ appears…",
    "crumbs": [
      "06 Qualitative",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>🧾 6.1 Introduction to Qualitative Research</span>"
    ]
  },
  {
    "objectID": "notebooks/06_qualitative/6.1_intro_qualitative_research.html#conclusion",
    "href": "notebooks/06_qualitative/6.1_intro_qualitative_research.html#conclusion",
    "title": "🧾 6.1 Introduction to Qualitative Research",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve explored qualitative data analysis with text processing. Next, dive deeper into text analysis in 6.2.\nResources: - Python Text Processing - Qualitative Research Guide - Repository: github.com/ggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "06 Qualitative",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>🧾 6.1 Introduction to Qualitative Research</span>"
    ]
  },
  {
    "objectID": "notebooks/06_qualitative/6.2_text_analysis.html",
    "href": "notebooks/06_qualitative/6.2_text_analysis.html",
    "title": "📝 6.2 Text Analysis for Qualitative Research",
    "section": "",
    "text": "📝 6.2 Text Analysis for Qualitative Research\nThis notebook introduces text analysis techniques for qualitative nutrition research, focusing on processing survey responses.\nObjectives:\nContext: Qualitative analysis of survey data, like hippo food preferences, reveals insights into dietary behaviours, complementing quantitative methods. 🦛\nThis notebook introduces text analysis techniques for qualitative nutrition research, focusing on processing survey responses.\nObjectives: - Preprocess text data using tokenization and stopword removal. - Perform word frequency analysis and visualization. - Apply techniques to food_preferences.txt to uncover hippo dietary preferences.\nContext: Qualitative analysis of survey data, like hippo food preferences, reveals insights into dietary behaviours, complementing quantitative methods.\n# Download NLTK resources\nnltk.download('punkt_tab')  # Tokenizer\nnltk.download('stopwords')  # Stopwords list\nprint('Text analysis environment ready.')\n\nText analysis environment ready.",
    "crumbs": [
      "06 Qualitative",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>📝 6.2 Text Analysis for Qualitative Research</span>"
    ]
  },
  {
    "objectID": "notebooks/06_qualitative/6.2_text_analysis.html#data-preparation",
    "href": "notebooks/06_qualitative/6.2_text_analysis.html#data-preparation",
    "title": "📝 6.2 Text Analysis for Qualitative Research",
    "section": "Data Preparation",
    "text": "Data Preparation\nLoad food_preferences.txt, containing 50 hippo survey responses, and preprocess the text.\n\n# Load survey responses\nfile_path = fns.get_data_path(\"food_preferences\")\n\n\nwith open(file_path, 'r') as f:\n    responses = f.readlines()\nprint(responses[:2])\n\nprint(f'Number of responses: {len(responses)}')  # Display total responses\nprint(f'Sample response: {responses[0]}')  # Show first response\n\nNumber of responses: 50\nSample response: Hippo H1: I enjoy crunchy carrots.",
    "crumbs": [
      "06 Qualitative",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>📝 6.2 Text Analysis for Qualitative Research</span>"
    ]
  },
  {
    "objectID": "notebooks/06_qualitative/6.2_text_analysis.html#text-preprocessing",
    "href": "notebooks/06_qualitative/6.2_text_analysis.html#text-preprocessing",
    "title": "📝 6.2 Text Analysis for Qualitative Research",
    "section": "Text Preprocessing",
    "text": "Text Preprocessing\nTokenize responses, convert to lowercase, and remove stopwords and punctuation.\n\n# Initialize stopwords\nstop_words = set(stopwords.words('english')).union({':', '.', 'hippo', 'i'})\n\n# Tokenize and clean responses\ntokens = []\nfor response in responses:\n    words = word_tokenize(response.lower())  # Convert to lowercase and tokenize\n    clean_words = [word for word in words if word.isalpha() and word not in stop_words]\n    tokens.extend(clean_words)\n\nprint(f'Sample tokens from first response: {tokens[:3]}')  # Show first few tokens\n\nSample tokens from first response: ['enjoy', 'crunchy', 'carrots']",
    "crumbs": [
      "06 Qualitative",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>📝 6.2 Text Analysis for Qualitative Research</span>"
    ]
  },
  {
    "objectID": "notebooks/06_qualitative/6.2_text_analysis.html#word-frequency-analysis",
    "href": "notebooks/06_qualitative/6.2_text_analysis.html#word-frequency-analysis",
    "title": "📝 6.2 Text Analysis for Qualitative Research",
    "section": "Word Frequency Analysis",
    "text": "Word Frequency Analysis\nCount and visualize the most common words in the responses.\n\n# Count word frequencies\nword_freq = Counter(tokens)\ntop_words = word_freq.most_common(5)\nprint(f'Top 5 words: {top_words}')\n\n# Visualize word frequencies\nwords, counts = zip(*top_words)\nplt.figure(figsize=(8, 6))\nplt.bar(words, counts)\nplt.title('Top 5 Words in Hippo Food Preferences')\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.show()  # Display bar plot\n\nTop 5 words: [('carrots', 15), ('crunchy', 12), ('sweet', 10), ('enjoy', 8), ('greens', 7)]\n\n\n&lt;Figure size 800x600 with 1 Axes&gt;",
    "crumbs": [
      "06 Qualitative",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>📝 6.2 Text Analysis for Qualitative Research</span>"
    ]
  },
  {
    "objectID": "notebooks/06_qualitative/6.2_text_analysis.html#exercise-analyze-adjectives",
    "href": "notebooks/06_qualitative/6.2_text_analysis.html#exercise-analyze-adjectives",
    "title": "📝 6.2 Text Analysis for Qualitative Research",
    "section": "Exercise: Analyze Adjectives",
    "text": "Exercise: Analyze Adjectives\nModify the preprocessing to extract only adjectives (e.g., ‘crunchy’, ‘sweet’) and count their frequencies. Visualize the top 3 adjectives in a bar plot.\nGuidance: - Use NLTK’s part-of-speech tagging (nltk.pos_tag) with nltk.download('averaged_perceptron_tagger'). - Filter for adjectives (POS tag ‘JJ’). - Create a bar plot of the top 3 adjectives.\nAnswer:\nMy adjective analysis code and results are as follows:\n# Your code here\nTop 3 Adjectives:\n\n[Adjective 1]: [Count]\n[Adjective 2]: [Count]\n[Adjective 3]: [Count]",
    "crumbs": [
      "06 Qualitative",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>📝 6.2 Text Analysis for Qualitative Research</span>"
    ]
  },
  {
    "objectID": "notebooks/06_qualitative/6.2_text_analysis.html#conclusion",
    "href": "notebooks/06_qualitative/6.2_text_analysis.html#conclusion",
    "title": "📝 6.2 Text Analysis for Qualitative Research",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve applied text analysis to uncover dietary preferences from hippo survey responses, revealing key themes like ‘crunchy carrots’.\nNext Steps: Apply these skills to your own qualitative datasets or revisit earlier modules for quantitative analysis.\nResources: - NLTK Documentation - Text Analysis Tutorial - Repository: github.com/ggkuhnle/data-analysis-toolkit-FNS",
    "crumbs": [
      "06 Qualitative",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>📝 6.2 Text Analysis for Qualitative Research</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/index.html",
    "href": "notebooks/10_mini_projects/index.html",
    "title": "10 Mini Projects",
    "section": "",
    "text": "🧪 10 Mini Projects\nThis module contains applied mini-projects that consolidate learning from the earlier sections:\n\nClinical Trial Analysis\nView HTML | \nEpidemiology Case Study\nView HTML | \nHandling Missing Data\nView HTML | \nMetabolomics Analysis\nView HTML | \nMachine Learning Applications\nView HTML | \nAI in Nutrition Science\nView HTML | \nLarge Language Models (LLM)\nView HTML | \nBayesian vs. Frequentist Methods\nView HTML | \nFortran in Nutrition Science\nView HTML |",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>10 Mini Projects</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "",
    "text": "Methodology\nThis notebook explores data from a simulated 2-year clinical trial testing the effects of a novel nutrient, Hipponol, on blood pressure and survival.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#study-design",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#study-design",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "Study Design",
    "text": "Study Design\n\nPopulation: 1000 participants, aged 40–70, male/female, smokers and non-smokers.\nDesign: Randomised controlled trial (RCT), 1:1 allocation to Hipponol vs. placebo.\nBlinding: Double-blind.\nDuration: 2-year follow-up.\nOutcomes:\n\nPrimary: Survival at 2 years.\nSecondary: Change in systolic blood pressure (SBP) from baseline to follow-up.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#analysis-plan",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#analysis-plan",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "Analysis Plan",
    "text": "Analysis Plan\nThis notebook will follow key elements of the CONSORT guidelines for RCT reporting.\n\n🔍 Descriptive Analysis (Baseline)\n\nGenerate a Table 1 comparing baseline characteristics (age, sex, smoking status, baseline SBP) between groups.\nUse appropriate statistical tests:\n\nContinuous variables: t-test or Mann–Whitney U\nCategorical variables: Chi-squared test\n\n\n\n\n📉 Outcome Analysis\nAnalysis of results using Bayesian and Frequentist methods\n\n\nPrimary endpoint\n\nSurvival:\n\nLogistic regression\nKaplan–Meier survival curves by group.\nLog-rank test and Cox proportional hazards model.\n\nBlood Pressure:\n\nCompare change in SBP using paired and unpaired t-tests.\nAdjust for baseline covariates using linear regression.\n\n\n\n\n📌 Notes\n\nMissing data handling: listwise deletion for simplicity.\nSensitivity analyses optional (not shown in basic notebook).\nAssumptions for all statistical models will be checked.\n\n\n\n🦛 Fun Fact\n\nThe nutrient Hipponol is purely fictional—but if hippos had trials, we bet they’d run them by CONSORT too!\n\n\n# Setup for Google Colab: Fetch datasets automatically or manually\n%run ../../bootstrap.py    # installs requirements + editable package\n\nimport fns_toolkit as fns\n\n\nprint('Environment ready.')",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#requirements",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#requirements",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "✅ Requirements",
    "text": "✅ Requirements\n\npandas: Used for handling and manipulating tabular data (DataFrames).\nWe use pandas to structure and prepare the clinical trial data for analysis.\nnumpy: Provides efficient numerical operations, including array handling and random number generation.\nWe use numpy for data generation, numerical summaries, and efficient computation.\nmatplotlib: Generates static plots and visualisations (e.g., line plots, histograms, and KDEs).\nWe use matplotlib to create figures illustrating key findings and trends.\nscipy: Offers statistical functions and tests (e.g., t-tests, chi-squared tests).\nWe use scipy for hypothesis testing and basic statistical analysis.\nlifelines: Supports survival analysis, including Kaplan–Meier estimation and Cox proportional hazards models.\nWe use lifelines to model and visualise survival outcomes over the intervention period.\nseaborn: Enhances data visualisation, providing high-level functions for attractive statistical plots (e.g., KDE plots).\nWe use seaborn to simplify and beautify plots, especially distributions and correlations.\npymc: Enables building and sampling from Bayesian statistical models.\nWe use pymc to fit Bayesian models for endpoints such as survival and blood pressure change.\narviz: Summarises, visualises, and diagnoses outputs from Bayesian models.\nWe use arviz to assess model fit and interpret posterior distributions.\n\n\n# Setup\n# Import libraries for data manipulation, visualisation, and statistical analysis\nimport pandas as pd            # For data manipulation and analysis\nimport numpy as np             # For numerical operations\nimport matplotlib.pyplot as plt # For plotting\nimport seaborn as sns          # For enhanced statistical visualisations\nfrom scipy.stats import ttest_ind, chi2_contingency  # For hypothesis testing\n\n# Import Bayesian modelling and visualisation libraries\nimport pymc as pm              # For Bayesian statistical modelling\nimport arviz as az             # For Bayesian inference visualisation\n\nfrom lifelines import KaplanMeierFitter\nfrom lifelines import CoxPHFitter\nimport scipy.stats as stats\n\nimport patsy",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#table-1-summary-function",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#table-1-summary-function",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "🔍 Table 1: Summary Function",
    "text": "🔍 Table 1: Summary Function\nThis cell builds a summary table comparing baseline characteristics across treatment groups using standard Python libraries:\n\nNumerical variables (e.g., age, baseline BP) are compared using independent t-tests. We report mean ± standard deviation.\nCategorical variables (e.g., gender, smoker) are compared using chi-squared tests, and shown as counts with percentages.\n\nThe function create_table1():\n\nTakes the dataset and grouping column (e.g., “group”) as input.\nReturns a DataFrame summarising each variable by group, along with p-values for between-group comparisons.\n\nThis approach is useful when you want:\n\nFull control over what’s included in the table.\nClear, interpretable formatting for reports or publications.\n\nTo avoid installing additional packages (e.g., tableone).\n🛠️ This method aligns with CONSORT guidelines for reporting baseline characteristics.\n\ndef create_table1_clean(data, group_col):\n    \"\"\"Create a clean Table 1: baseline characteristics by group.\"\"\"\n    if group_col not in data.columns:\n        raise KeyError(f\"Grouping column '{group_col}' not found in DataFrame.\")\n    \n    group_names = data[group_col].unique()\n    if len(group_names) != 2:\n        raise ValueError(\"This function currently supports exactly two groups.\")\n\n    group1, group2 = group_names\n    summary = []\n\n    numeric_cols = ['Age', 'Baseline_SBP']\n    cat_cols = ['Sex', 'SmokingStatus']\n\n    # Continuous variables\n    for col in numeric_cols:\n        vals1 = data[data[group_col] == group1][col].dropna()\n        vals2 = data[data[group_col] == group2][col].dropna()\n        mean1, std1 = vals1.mean(), vals1.std()\n        mean2, std2 = vals2.mean(), vals2.std()\n        t_stat, p_val = ttest_ind(vals1, vals2)\n\n        summary.append({\n            'Variable': col,\n            f'{group1} (Mean ± SD)': f'{mean1:.1f} ± {std1:.1f}',\n            f'{group2} (Mean ± SD)': f'{mean2:.1f} ± {std2:.1f}',\n            'p-value': f'{p_val:.3f}'\n        })\n\n    # Categorical variables\n    for col in cat_cols:\n        cont_table = pd.crosstab(data[col], data[group_col])\n        chi2, p_val, _, _ = chi2_contingency(cont_table)\n\n        for i, cat in enumerate(cont_table.index):\n            row = {\n                'Variable': f\"{col} = {cat}\",\n                f'{group1} (Count %)': f\"{cont_table.loc[cat, group1]} ({cont_table.loc[cat, group1] / cont_table[group1].sum() * 100:.1f}%)\",\n                f'{group2} (Count %)': f\"{cont_table.loc[cat, group2]} ({cont_table.loc[cat, group2] / cont_table[group2].sum() * 100:.1f}%)\",\n                'p-value': f\"{p_val:.3f}\" if i == 0 else ''\n            }\n            summary.append(row)\n\n    return pd.DataFrame(summary)\n\n\n\n# Create and show Table 1\n\ntable1_df = create_table1_clean(df1, group_col='Group')\ndisplay(table1_df)",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#exploring-distributions",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#exploring-distributions",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "📊 Exploring Distributions",
    "text": "📊 Exploring Distributions\nBefore diving into inferential statistics, it’s crucial to understand the distribution of our variables. This helps us:\n\nCheck whether groups are comparable\nIdentify outliers or skewed data\nDecide which statistical tests are appropriate (e.g., parametric vs. non-parametric)\n\n\n🎯 Focus: Baseline Blood Pressure and Age\nFor this study, we’ll explore:\n\nAge\nBaseline Systolic Blood Pressure (SBP)\n\nThese continuous variables should ideally be similarly distributed across the control and intervention groups if randomisation was successful.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#visualising-distributions",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#visualising-distributions",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "🔍 Visualising Distributions",
    "text": "🔍 Visualising Distributions\nWe can use the following plots to explore the distribution:\n\nHistogram: Shows the shape of the distribution (e.g. normal, skewed)\nBoxplot: Useful for comparing groups and spotting outliers\nViolin plot (optional): Combines boxplot with a kernel density estimate\n\n\n\n# Set plotting style\nsns.set(style=\"whitegrid\")\n\n# Plot: Distribution of Age\nplt.figure(figsize=(10, 4))\nsns.histplot(data=df1, x='Age', hue='Group', kde=True, bins=20)\nplt.title('Age Distribution by Group')\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.show()\n\n# Plot: Baseline Systolic BP\nplt.figure(figsize=(10, 4))\nsns.boxplot(data=df1, x='Group', y='Baseline_SBP')\nplt.title('Baseline Systolic Blood Pressure by Group')\nplt.ylabel('Systolic BP (mmHg)')\nplt.xlabel('')\nplt.show()\n\n# Plot 3: Violin plot of Baseline Systolic BP\nplt.figure(figsize=(10, 4))\nsns.violinplot(data=df1, x='Group', y='Baseline_SBP', inner='box')\nplt.title('Violin Plot: Baseline Systolic Blood Pressure by Group')\nplt.ylabel('Systolic BP (mmHg)')\nplt.xlabel('')\nplt.show()\n\n\n💡 Interpretation Tips\n\nLook for symmetry or skewness.\nCheck for group overlap—similar distributions are a good sign of effective randomisation.\nUse this to support your interpretation of Table 1.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#analysing-survival-outcomes",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#analysing-survival-outcomes",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "🧬 Analysing Survival Outcomes",
    "text": "🧬 Analysing Survival Outcomes\nThe primary endpoint of the Hipponol trial is 2-year survival, coded as:\n\n1 = **Survived\n0 = Died during follow-up\n\nSurvival data is binary and typically analysed using methods suited to categorical outcomes or time-to-event data (e.g. Cox models, Kaplan–Meier). In our case, we assume binary survival at 2 years, so logistic regression is appropriate.\n\n🧪 Quick Check: Chi-squared Test for Survival\nBefore diving into regression models, we can start with a simple comparison of survival rates between the Hipponol and Control groups using a Chi-squared test.\n✅ Why use it?\n\nIt’s a straightforward way to compare proportions between two groups.\nUseful for binary outcomes like survival (1 = survived, 0 = died).\nHelps confirm whether survival rates differ significantly between the two groups.\n\n🧾 Method\nWe use a contingency table to count survivors and non-survivors in each group, then apply the Chi-squared test of independence:\n\ncontingency = pd.crosstab(df1['Group'], df1['Survival'])\nchi2, p, dof, expected = chi2_contingency(contingency)\n\nprint(\"Contingency Table:\")\nprint(contingency)\nprint(f\"Chi-squared test p-value: {p:.3f}\")\n\n📌 Interpretation\n\nA small p-value (typically &lt; 0.05) suggests a statistically significant difference in survival between groups.\nThis test does not account for confounders like age or smoking — use logistic regression for that.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#logistic-regression",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#logistic-regression",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "🔎 Logistic Regression",
    "text": "🔎 Logistic Regression\nOnce we’ve done a basic comparison (like a Chi-squared test), the next step is a logistic regression. This lets us model the probability of survival while also:\n\nControlling for other variables (e.g. age, smoking)\nEstimating odds ratios, which are easier to interpret than raw coefficients\n\n\n💡 What is logistic regression?\nLogistic regression is used when your outcome is binary — in this case, survival (1) vs. death (0). It models the log-odds of survival as a function of one or more predictors.\n\n# Fit a logistic regression model\nlogit_model = smf.logit('Survival ~ Group', data=df1).fit()\n\n# Print summary\nprint(logit_model.summary())\n\n🔁 From log-odds to odds ratio\nThe model estimates log-odds, which are not intuitive. We convert them to odds ratios using the exponential function:\n\nodds_ratios = logit_model.params.apply(np.exp)\nprint(odds_ratios)\n\nAn odds ratio (OR):\n\n= 1 means no effect\n\n1 means increased odds (e.g. Hipponol improves survival)\n\n&lt; 1 means reduced odds (e.g. Hipponol worsens survival)\n\n\n\n📌 Example Interpretation\nIf Group[T.Hipponol] has an OR of 1.6:\n\n“Participants in the Hipponol group had 60% higher odds of surviving the 2-year follow-up compared to the control group.”\n\n\n\n📎 Optional: Add other variables\nYou can also control for age, sex, and smoking:\n\nlogit_full = smf.logit('Survival ~ Group + Age + C(Sex) + C(SmokingStatus)', data=df1).fit()\nprint(logit_full.summary())\n\nThis gives you adjusted odds ratios — a better estimate when multiple factors influence the outcome.\n\n\n📏 Confidence Intervals for Odds Ratios\nWhen you run a logistic regression, the model provides standard errors for the coefficients. You can use these to calculate a 95% confidence interval (CI) for each odds ratio:\n\n# Get confidence intervals for the log-odds\nconf = logit_model.conf_int()\nconf.columns = ['2.5%', '97.5%']\n\n# Add odds ratios\nconf_exp = np.exp(conf)\nconf_exp['Odds Ratio'] = np.exp(logit_model.params)\n\nprint(conf_exp)\n\n\n\n🧠 How to interpret this:\nIf the 95% CI for an odds ratio: - Includes 1, the result is not statistically significant at p &lt; 0.05 - Excludes 1, then the effect is statistically significant\n\n\n📌 Example:\n\n\n\nVariable\nOR\n2.5%\n97.5%\n\n\n\n\nGroup[T.Hipponol]\n1.62\n1.02\n2.58\n\n\n\n\n\n_“The odds of survival in the Hipponol group are 62% higher than in the control group (95% CI: 1.02–2.58).”\n\nThis suggests a statistically significant effect, as the interval does not include 1.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#bayesian-logistic-regression-survival",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#bayesian-logistic-regression-survival",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "Bayesian Logistic Regression – Survival",
    "text": "Bayesian Logistic Regression – Survival\nIn this section, we apply Bayesian logistic regression to evaluate whether assignment to the Hipponol group affects the probability of survival compared to the Control group.\nThis analysis complements the Frequentist approach by offering a full posterior distribution of the treatment effect, providing a probabilistic interpretation of results rather than just point estimates and p-values.\n\n🧮 Why Bayesian?\nBayesian modelling is particularly useful when:\n\nYou want probabilistic interpretations of effect size.\nYou have prior knowledge you wish to incorporate.\nYou want a posterior distribution instead of a point estimate.\nYou’re dealing with small samples, where Frequentist power is limited.\n\nIn our case, we use weakly informative priors (e.g. Normal(0, 10)) to express general uncertainty without strong assumptions.\n\n\n\n🎯 Goal\nWe aim to:\n\nEstimate the posterior probability distribution of the survival effect due to Hipponol.\nQuantify the uncertainty in this effect.\nCalculate the posterior odds ratio for survival in the Hipponol group compared to Control.\n\nBayesian inference allows us to express the result as:\n\n“There is a XX% probability that Hipponol increases survival compared to control.”\n\nThis is fundamentally different from the Frequentist interpretation, which only tells us the probability of observing our data under the null hypothesis.\n\n\n\n📦 Define Predictor and Outcome\nTo build our model, we first define the predictor (treatment group) and outcome (survival).\n\nX: A binary predictor indicating group membership\n0 = Control, 1 = Hipponol\ny: A binary outcome\n0 = Did not survive, 1 = Survived\n\nThis mirrors the structure required for Bernoulli trials, where each observation represents a binary outcome.\n\n# Convert group to numeric: 0 = Control, 1 = Hipponol\nX = (df['Group'] == 'Hipponol').astype(int).values\n\n# Survival outcome: already coded as 0 = no, 1 = yes\ny = df['Survival'].astype(int).values\n\nNext, we build and fit the Bayesian model using PyMC.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#step-by-step-bayesian-logistic-regression-in-pymc",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#step-by-step-bayesian-logistic-regression-in-pymc",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "🔧 Step-by-Step: Bayesian Logistic Regression in PyMC",
    "text": "🔧 Step-by-Step: Bayesian Logistic Regression in PyMC\nWe now construct and sample from a Bayesian logistic regression model using PyMC. This model estimates the effect of group assignment (Hipponol vs Control) on the probability of survival.\n\n\n📐 Model Structure\n\nData:\n\nx_shared: Encodes treatment group (0 = Control, 1 = Hipponol)\n\ny_shared: Binary survival outcome (0 = no, 1 = yes)\n\nPriors:\n\nIntercept: Prior belief about survival probability in the Control group.\n→ Set as Normal(0, 10) to reflect wide uncertainty.\nBeta_Group: Prior for the effect of the Hipponol group.\n→ Also Normal(0, 10) (can be tightened if prior knowledge exists).\n\nLogistic function:\n\nComputes log-odds: logit_p = intercept + beta_group * x_shared\n\nConverts to probability via sigmoid: p = sigmoid(logit_p)\n\nLikelihood:\n\nModels observed survival data as a Bernoulli trial with success probability p.\n\nSampling:\n\nWe use Hamiltonian Monte Carlo (HMC) via the NUTS sampler.\n\n4 chains × 2000 draws each, with 1000 tuning samples per chain.\n\n\n\n\n\n📦 PyMC Model Code\n\nwith pm.Model() as model:\n    # Data containers (for flexibility in updating)\n    x_shared = pm.Data(\"x_shared\", X)\n    y_shared = pm.Data(\"y_shared\", y)\n\n    # Priors\n    intercept = pm.Normal(\"Intercept\", mu=0, sigma=10)\n    beta_group = pm.Normal(\"Beta_Group\", mu=0, sigma=10)\n\n    # Logistic function\n    logit_p = intercept + beta_group * x_shared\n    p = pm.Deterministic(\"p\", pm.math.sigmoid(logit_p))\n\n    # Likelihood\n    y_obs = pm.Bernoulli(\"y_obs\", p=p, observed=y_shared)\n\n    # Sampling from the posterior\n    trace = pm.sample(2000, tune=1000, target_accept=0.95, return_inferencedata=True)\n\n🔍 Next Steps After sampling, we will:\n\nInspect the posterior of Beta_Group to assess the treatment effect.\nConvert this to an odds ratio (by exponentiating Beta_Group).\nVisualise the posterior distribution using arviz.\n\n\n# Plot posterior for Beta_Group\naz.plot_posterior(trace, var_names=['Beta_Group'], ref_val=0)\nplt.title(\"Posterior Distribution: Group Effect (log-odds)\")\nplt.show()\n\n# Convert to odds ratio\nodds_ratios = np.exp(trace.posterior['Beta_Group'].values.flatten())\n\n# Plot posterior odds ratio\naz.plot_posterior(odds_ratios, ref_val=1)\nplt.title(\"Posterior Odds Ratio: Hipponol vs Control\")\nplt.xlabel(\"Odds Ratio\")\nplt.show()\n\n\n\n🧠 Interpretation\n\nIf the posterior of Beta_Group is mostly above 0 → Hipponol increases odds of survival.\nIf the 95% credible interval (HDI) for odds_ratios excludes 1 → strong evidence for a difference.\n\nYou can compute az.hdi() for numeric intervals:\n\naz.hdi(odds_ratios, hdi_prob=0.95)\n\n✅ This Bayesian approach allows for direct probability statements:\n“There is a 95% probability that Hipponol increases the odds of survival by X–Y%.”",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#kaplanmeier-survival-curves",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#kaplanmeier-survival-curves",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "📈 Kaplan–Meier Survival Curves",
    "text": "📈 Kaplan–Meier Survival Curves\nTo visualise how survival changes over time in each group, we use Kaplan–Meier survival estimates. These curves provide a non-parametric estimate of the survival function, which reflects the probability of survival beyond a given time point.\n\n🔍 Why Kaplan–Meier?\nKaplan–Meier curves are ideal for visualising time-to-event data because they:\n\nHandle censored data (participants who didn’t experience the event by the end of follow-up)\nShow the proportion of participants surviving over time\nAllow intuitive comparison between groups (e.g. Hipponol vs Control)\n\n\n\n📘 Interpretation\nEach drop in the curve corresponds to an event (e.g. death). A steeper decline indicates more frequent events, whereas a flat curve suggests stable survival.\nIf the curves for the two groups diverge:\n\nWider separation may indicate a meaningful difference in survival.\nOverlap suggests similar survival patterns.\n\n\n# Split data\ndf_control = df[df['Group'] == 'Control']\ndf_hipponol = df[df['Group'] == 'Hipponol']\n\n# Initialise fitter\nkmf_control = KaplanMeierFitter()\nkmf_hipponol = KaplanMeierFitter()\n\n# Fit and plot\nplt.figure(figsize=(10, 6))\n\nkmf_control.fit(durations=df_control['Time_to_Event'],\n                event_observed=df_control['Survival'],\n                label='Control')\nkmf_control.plot_survival_function(ci_show=True)\n\nkmf_hipponol.fit(durations=df_hipponol['Time_to_Event'],\n                 event_observed=df_hipponol['Survival'],\n                 label='Hipponol')\nkmf_hipponol.plot_survival_function(ci_show=True)\n\nplt.title(\"Kaplan–Meier Survival Curves by Group\")\nplt.xlabel(\"Time to Event (months)\")\nplt.ylabel(\"Survival Probability\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#compare-groups-statistically-log-rank-test",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#compare-groups-statistically-log-rank-test",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "🧪 Compare groups statistically – Log-rank test",
    "text": "🧪 Compare groups statistically – Log-rank test\nThis checks whether the survival distributions are significantly different between the groups.\n\n🔍 What is the Log-Rank Test?\nThe log-rank test is a non-parametric statistical test used to compare the survival distributions of two or more groups. It’s particularly useful in clinical trials or studies with time-to-event data.\n✅ What it does:\n\nCompares the observed number of events in each group to the expected number under the null hypothesis (that all groups have the same survival curve).\nIt handles censored data properly.\nOutputs a p-value: if this is small (typically &lt; 0.05), it suggests the survival curves differ significantly.\n\n\n# Filter your data\ndf_control = df[df[\"Group\"] == \"Control\"]\ndf_hipponol = df[df[\"Group\"] == \"Hipponol\"]\n\n# Run the log-rank test\nresults = logrank_test(\n    df_control[\"Time_to_Event\"], df_hipponol[\"Time_to_Event\"],\n    event_observed_A=df_control[\"Survival\"],\n    event_observed_B=df_hipponol[\"Survival\"]\n)\n\n# Display the results\nprint(\"📈 Log-rank test between Control and Hipponol:\")\nprint(f\"Test statistic: {results.test_statistic:.3f}\")\nprint(f\"p-value: {results.p_value:.4f}\")\n\n# Optional: simple interpretation\nif results.p_value &lt; 0.05:\n    print(\"⚠️ There is a significant difference in survival between groups.\")\nelse:\n    print(\"✅ No significant difference in survival between groups.\")\n\n\n\n🧪 When to Use:\nUse this when:\n\nYou’ve plotted Kaplan-Meier curves and want to test if the difference is significant.\nYou’re evaluating intervention effects in RCTs or cohort studies.\nYou want a simple test without assuming specific hazard functions (unlike Cox models).",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#cox-proportional-hazards-model-a-detailed-guide",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#cox-proportional-hazards-model-a-detailed-guide",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "Cox Proportional Hazards Model: A Detailed Guide",
    "text": "Cox Proportional Hazards Model: A Detailed Guide\n\n✅ What Is the Cox Proportional Hazards Model?\nThe Cox PH model is a semi-parametric regression model used in survival analysis. It relates the time that passes before an event occurs to one or more covariates.\n\n\nCore idea:\nIt models the hazard rate at time t as:\n[ h(t|X) = h_0(t) (_1X_1 + _2X_2 + + _pX_p) ]\n\n( h_0(t) ) is the baseline hazard (unspecified)\n( X_i ) are the covariates (e.g. Group, Age, Sex)\n( _i ) are coefficients estimated from the data\n\nThe model makes the proportional hazards assumption: the ratio of hazards between groups is constant over time.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#how-to-fit-it-in-python-with-lifelines",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#how-to-fit-it-in-python-with-lifelines",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "🛠️ 2. How to Fit It in Python (with lifelines)",
    "text": "🛠️ 2. How to Fit It in Python (with lifelines)\n\n# Select relevant columns only\ndf_cox = df[[\"Age\", \"Sex\", \"SmokingStatus\", \"Group\", \"Time_to_Event\", \"Survival\"]].copy()\n\n# One-hot encode categorical variables, dropping first category to avoid multicollinearity\ndf_cox = pd.get_dummies(df_cox, drop_first=True)\n\n# Fit the Cox model\ncph = CoxPHFitter()\ncph.fit(df_cox, duration_col=\"Time_to_Event\", event_col=\"Survival\")\n\n# Print the summary\ncph.print_summary()\n\n\n📏 How to Interpret the Output\nThe summary table includes:\n\ncoef: log of the hazard ratio (HR)\nexp(coef): the hazard ratio\np: p-value testing if the coefficient is significantly different from 0\n-log2(p): significance in a log-scale\n95% CI: confidence intervals for the HR\n\n\n\nExample Interpretation:\n\n\n\n\n\n\n\n\n\nVariable\nexp(coef)\np-value\nInterpretation\n\n\n\n\nGroup_Hipponol\n0.65\n0.001\nHipponol reduces the hazard by 35% vs. Control\n\n\nAge\n1.02\n0.050\nEach year of age increases hazard by 2%\n\n\nSex_Male\n1.10\n0.200\nNo significant difference vs. Female\n\n\n\n\n\n\n⚠️ Test the Proportional Hazards (PH) Assumption\nThis is essential because if PH is violated, your hazard ratios are not valid over time.\n\n# Check proportional hazards assumption\ncph.check_assumptions(df_cox, p_value_threshold=0.05, show_plots=True)\n\n\nPrint a test result for each covariate\nShow Schoenfeld residuals plots:\n\nIf residuals show systematic trends, PH is likely violated\nFlat residuals = good\n\n\n\n\nIf PH is violated:\n\nAdd interaction terms with time\nUse stratified Cox models\nSplit time into intervals (time-dependent covariates)\n\n\n\n🧪 5. (Optional) Predict or Visualise\nYou can predict survival curves for a given profile:\n\n# Create a base profile (mean age, female, non-smoker)\nbase = pd.DataFrame({\n    'Age': [df['Age'].mean()],\n    'Sex_Male': [0],\n    'SmokingStatus_Smoker': [0],\n    'Group_Hipponol': [0]  # Control group\n})\n\n# Copy and modify for Hipponol\nhipponol = base.copy()\nhipponol['Group_Hipponol'] = 1  # Switch to Hipponol group\n\n# Predict survival curves\nsf_control = cph.predict_survival_function(base)\nsf_hipponol = cph.predict_survival_function(hipponol)\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.plot(sf_control, label=\"Control\")\nplt.plot(sf_hipponol, label=\"Hipponol\")\nplt.title(\"Predicted Survival Curve for Control vs. Hipponol\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Survival Probability\")\nplt.legend()\nplt.grid(True)\nplt.show()",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#visual-exploration-of-distributions",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#visual-exploration-of-distributions",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "1. 🎨 Visual Exploration of Distributions",
    "text": "1. 🎨 Visual Exploration of Distributions\nBefore diving into formal tests, it’s useful to examine the data visually. You can compare the SBP distributions for the two groups at:\n\nBaseline\n\n# Set aesthetic style\nsns.set(style=\"whitegrid\")\n\n# KDE plot: Baseline SBP\nplt.figure(figsize=(10, 5))\nsns.kdeplot(data=df, x='Baseline_SBP', hue='Group', fill=True, common_norm=False, alpha=0.4)\nplt.title(\"Baseline Systolic BP Distribution by Group\")\nplt.xlabel(\"Baseline SBP (mmHg)\")\nplt.show()\n\n\n\nFollow-up\n\n# KDE plot: Follow-up SBP\nplt.figure(figsize=(10, 5))\nsns.kdeplot(data=df, x='Followup_SBP', hue='Group', fill=True, common_norm=False, alpha=0.4)\nplt.title(\"Follow-up Systolic BP Distribution by Group\")\nplt.xlabel(\"Follow-up SBP (mmHg)\")\nplt.show()\n\n\n\nChange in Blood Pressure\n\n# Calculate and plot SBP change\ndf['SBP_change'] = df['Followup_SBP'] - df['Baseline_SBP']\nplt.figure(figsize=(10, 5))\nsns.kdeplot(data=df, x='SBP_change', hue='Group', fill=True, common_norm=False, alpha=0.4)\nplt.title(\"Change in SBP (Follow-up – Baseline) by Group\")\nplt.axvline(0, color='black', linestyle='--')\nplt.xlabel(\"Change in SBP (mmHg)\")\nplt.show()",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#t-test-for-comparing-change-in-sbp-between-two-groups",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#t-test-for-comparing-change-in-sbp-between-two-groups",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "🔍 t-Test for Comparing Change in SBP Between Two Groups",
    "text": "🔍 t-Test for Comparing Change in SBP Between Two Groups\n\nWhat is it?\nA two-sample t-test (also called an independent samples t-test) is used to determine whether there is a statistically significant difference in the means ↪of a continuous variable between two independent groups.\nIn this case, we’re interested in:\n\nIs the average change in SBP significantly different between participants in the Control group and those in the Hipponol group?\n\n\n\n\n🧪 When to use it\nUse a two-sample t-test when: - The outcome variable (here: SBP change = Followup_SBP − Baseline_SBP) is continuous. - The two groups are independent (no participant is in both groups). - The data are approximately normally distributed in each group. - Variances are assumed to be equal (although Welch’s correction can be used otherwise).\n\n\n\n🧠 Hypotheses\nLet: - $ _1 $: mean change in SBP in the Control group - $ _2 $: mean change in SBP in the Hipponol group\nThen:\n\nNull hypothesis $ H_0 $: $ _1 = _2 $ (no difference)\nAlternative hypothesis $ H_A $: $ _1 _2 $ (there is a difference)\n\nYou can also use a one-sided test if you expect Hipponol to reduce SBP.\n\n\n\nhip = df[df['Group'] == 'Hipponol']['SBP_change']\ncon = df[df['Group'] == 'Control']['SBP_change']\ncon = df[df['Group'] == 'Control']['SBP_change']\n\n# Mean difference\ndiff = hip.mean() - con.mean()\n\n# Standard error of the difference\nse_diff = np.sqrt(hip.var(ddof=1)/len(hip) + con.var(ddof=1)/len(con))\n\n# 95% Confidence Interval\ndof = len(hip) + len(con) - 2\nci = stats.t.interval(0.95, dof, loc=diff, scale=se_diff)\n\n# t-test\nt_stat, p_val = ttest_ind(hip, con)\n\nprint(f\"Mean difference: {diff:.2f} mmHg\")\nprint(f\"95% CI: {ci}\")\nprint(f\"t-statistic: {t_stat:.3f}, p-value: {p_val:.4f}\")",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#interpreting-results",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#interpreting-results",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "🧾 Interpreting results",
    "text": "🧾 Interpreting results\n\nA small p-value (typically &lt; 0.05) means you reject the null hypothesis and conclude that SBP change differs significantly between the groups.\nA large p-value suggests that the observed difference might be due to chance.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#anova-and-ancova",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#anova-and-ancova",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "🧪 ANOVA and ANCOVA",
    "text": "🧪 ANOVA and ANCOVA\n\nAnalysis of Variance (ANOVA)\nANOVA is used to determine whether there are statistically significant differences in the means of a continuous variable across two or more groups.\nIn our case, we can use one-way ANOVA to compare the change in systolic blood pressure (SBP) between the Control and Hipponol groups.\n\n\n🔍 Assumptions:\n\nIndependence of observations\nNormally distributed SBP changes within each group\nHomogeneity of variances across groups\n\n\n# Assume df is your DataFrame and has already loaded the dataset\ndf['SBP_Change'] = df['Followup_SBP'] - df['Baseline_SBP']\n\n# Split by group\ncontrol = df[df['Group'] == 'Control']['SBP_Change']\nhipponol = df[df['Group'] == 'Hipponol']['SBP_Change']\n\n# Run one-way ANOVA\nf_stat, p_value = stats.f_oneway(control, hipponol)\n\nprint(\"One-way ANOVA\")\nprint(f\"F-statistic: {f_stat:.4f}\")\nprint(f\"P-value: {p_value:.4f}\")\n\n\n\n🧪 Comparison: t-test vs. ANOVA\n\n\n\n\n\n\n\n\nFeature\nt-test\nANOVA (one-way)\n\n\n\n\nUse case\nComparing two group means\nDesigned for comparing three or more\n\n\nStatistic reported\nt-statistic\nF-statistic\n\n\nP-value\nSame as ANOVA when only 2 groups\nSame as t-test when only 2 groups\n\n\nInterpretation\nMean difference\nVariance between vs. within groups\n\n\nExtension\nNeeds other models for &gt;2 groups\nNaturally extends to &gt;2 group comparisons",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#mathematical-note",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#mathematical-note",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "📐 Mathematical Note",
    "text": "📐 Mathematical Note\nWhen there are exactly two groups, the relationship between the statistics is:\n\\[\nF = t^2\n\\]\nSo the numerical p-value and the result will be identical — the difference is in the formulation.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#when-to-use-each",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#when-to-use-each",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "✅ When to Use Each?",
    "text": "✅ When to Use Each?\n\n✅ Use t-test: when you are only comparing two groups and want the simplest approach.\n✅ Use ANOVA: when you are planning to include more than two groups in future or need consistency in your analysis framework.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#practical-implication",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#practical-implication",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "🔄 Practical Implication",
    "text": "🔄 Practical Implication\nFor the SBP change analysis with just Control and Hipponol: - Both methods are valid and will return the same p-value. - ANOVA provides more flexibility later (e.g., adjusting for other factors via ANCOVA or general linear models).\n\nAnalysis of Covariance (ANCOVA)\nANCOVA extends ANOVA by including one or more continuous covariates that may influence the dependent variable. This is particularly useful when there are baseline differences between groups (e.g. in baseline SBP).\nIn our context, ANCOVA allows us to compare the Follow-up SBP, adjusting for Baseline SBP as a covariate.\n\n💡 Why use ANCOVA?\n\nIt reduces residual variance, improving statistical power\nIt adjusts for potential imbalances in baseline values\n\n\n\n🔍 Assumptions:\n\nSame as ANOVA, plus:\n\nLinearity between covariate and outcome\nHomogeneity of regression slopes (no interaction between group and covariate)\n\n\n\n# Convert Group to categorical if needed\ndf['Group'] = df['Group'].astype('category')\n\n# Fit ANCOVA model\nmodel = smf.ols('Followup_SBP ~ Group + Baseline_SBP + Age + Sex + SmokingStatus', data=df).fit()\n\n# Show summary\nprint(model.summary())\nprint(df.head())\n\n📊 Notes\n\nANOVA checks whether the mean changes in SBP differ between groups.\nANCOVA goes further by adjusting for baseline SBP, increasing statistical power and controlling for initial differences.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#step-4-regression-with-covariates-optional",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#step-4-regression-with-covariates-optional",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "🧮 Step 4: Regression with Covariates (Optional)",
    "text": "🧮 Step 4: Regression with Covariates (Optional)\nTo adjust for potential confounders.\nNote: Sex and SmokingStatus should be treated as categorical variables. You may need to encode them using C() in the formula:\n\n\nmodel_adj = smf.ols('SBP_Change ~ Treatment + Age + C(Sex) + C(SmokingStatus)', data=df).fit()\nprint(model_adj.summary())\n\n\n📈 Step 5: Visualise the Regression\n\n# Encode group as binary treatment (if not already done)\ndf['Treatment'] = (df['Group'] == 'Hipponol').astype(int)\n\nplt.figure(figsize=(10, 6))\n\n# Add jittered individual points\nsns.stripplot(x='Treatment', y='SBP_Change', data=df,\n              jitter=0.2, size=5, alpha=0.6, color='grey')\n\n# Add group means and error bars (95% CI)\nsns.pointplot(x='Treatment', y='SBP_Change', data=df,\n              linestyle='none', capsize=0.1, err_kws={'linewidth': 1.5}, color='blue')\n\n# Customise axes\nplt.xticks([0, 1], ['Control', 'Hipponol'])\nplt.title('Change in Systolic Blood Pressure by Treatment Group')\nplt.xlabel('Treatment Group')\nplt.ylabel('Change in SBP (mmHg)')\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()\n\n\n\n🧠 Interpretation\n\nA negative coefficient for Treatment implies a greater reduction in SBP in the Hipponol group.\nThe p-value tells you whether this difference is statistically significant.\nThe adjusted R² provides an indication of how much variance in SBP change is explained by the model.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#t-test",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#t-test",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "🔹 T-test",
    "text": "🔹 T-test\nThe T-test compares the mean SBP change directly between the two groups, assuming equal variance. The estimated mean difference was statistically significant, indicating that the SBP change in the Hipponol group was different from the control group. This method provides a straightforward test of the null hypothesis with a confidence interval for the mean difference.\n\nAdvantage: Simple and easy to interpret.\nLimitation: Does not adjust for potential confounders such as age, sex, or smoking status.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#ancova",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#ancova",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "🔹 ANCOVA",
    "text": "🔹 ANCOVA\nThe Analysis of Covariance (ANCOVA) extends the OLS model by including baseline covariates—such as age, sex, and smoking status—to control for confounding and improve precision. This method adjusts the treatment effect for baseline imbalances and provides a more accurate estimate.\n\nAdvantage: Adjusts for potential confounders, improving estimate precision.\nLimitation: Requires linear relationships between covariates and outcome, and assumes homogeneity of regression slopes.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#ols-regression",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#ols-regression",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "🔹 OLS Regression",
    "text": "🔹 OLS Regression\nThe OLS regression treated group membership as a binary predictor for SBP change. The estimate from this model was numerically identical to the T-test result because no additional covariates were included. However, regression frameworks allow for the extension to multiple covariates and interaction terms.\n\nAdvantage: Can be extended to include confounders or interactions.\nLimitation: Assumes linear relationships and homoscedasticity.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#bayesian-regression",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#bayesian-regression",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "🔹 Bayesian Regression",
    "text": "🔹 Bayesian Regression\nThe Bayesian model estimated the treatment effect using probabilistic priors and provided a full posterior distribution for the treatment coefficient. The 95% Highest Density Interval (HDI) offers a Bayesian analogue to the confidence interval, interpreted as the range within which the true effect lies with 95% probability.\n\nAdvantage: Flexibility, intuitive probabilistic interpretation, and explicit incorporation of prior beliefs.\nLimitation: Requires computational resources and careful choice of priors.\n\n\n#\n# Assume df is already loaded and processed\ndf['SBP_Change'] = df['Followup_SBP'] - df['Baseline_SBP']\ndf['Treatment'] = (df['Group'] == 'Hipponol').astype(int)\n\n# --- T-test ---\ncontrol = df[df['Treatment'] == 0]['SBP_Change']\nhipponol = df[df['Treatment'] == 1]['SBP_Change']\n\nt_stat, t_pval = stats.ttest_ind(hipponol, control)\nmean_diff = hipponol.mean() - control.mean()\nci_low, ci_high = stats.t.interval(\n    0.95, df=len(control) + len(hipponol) - 2,\n    loc=mean_diff,\n    scale=stats.sem(np.concatenate([hipponol.values, -control.values]))\n)\n\n# --- OLS Regression ---\nmodel_adj = smf.ols('SBP_Change ~ Treatment + Age + C(Sex) + C(SmokingStatus)', data=df).fit()\nols_estimate = model_adj.params['Treatment']\nols_ci_low, ols_ci_high = model_adj.conf_int().loc['Treatment']\n\n\n# ANCOVA: Adjusting for age, sex, smoking\ndf_ancova = df.copy()\ndf_ancova = pd.get_dummies(df_ancova, columns=['Sex', 'SmokingStatus'], drop_first=True)\nformula = \"SBP_Change ~ Treatment + Age + Sex_Male + SmokingStatus_Smoker\"\ny_ancova, X_ancova = patsy.dmatrices(formula, data=df_ancova, return_type='dataframe')\nancova_model = sm.OLS(y_ancova, X_ancova).fit()\nancova_estimate = ancova_model.params['Treatment']\nancova_ci_low, ancova_ci_high = ancova_model.conf_int().loc['Treatment']\n\n\n# --- Bayesian Regression ---\nwith pm.Model() as model:\n    beta_0 = pm.Normal(\"Intercept\", mu=0, sigma=10)\n    beta_1 = pm.Normal(\"Treatment\", mu=0, sigma=5)\n    sigma = pm.HalfNormal(\"Sigma\", sigma=5)\n\n    mu = beta_0 + beta_1 * df['Treatment'].values\n    y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=df['SBP_Change'])\n\n    trace = pm.sample(2000, tune=1000, target_accept=0.95, return_inferencedata=True)\n\nbayes_summary = az.summary(trace, var_names=[\"Treatment\"], hdi_prob=0.95)\nbayes_estimate = bayes_summary.loc[\"Treatment\", \"mean\"]\nbayes_hdi_low = bayes_summary.loc[\"Treatment\", \"hdi_2.5%\"]\nbayes_hdi_high = bayes_summary.loc[\"Treatment\", \"hdi_97.5%\"]\n\n# --- Comparison Table with Formatting ---\neffect_df = pd.DataFrame({\n    \"Method\": [\"T-test\", \"OLS regression\", \"ANCOVA\", \"Bayesian regression\"],\n    \"Estimate (mmHg)\": [mean_diff, ols_estimate, ancova_estimate, bayes_estimate],\n    \"95% CI Lower\": [ci_low, ols_ci_low, ancova_ci_low, bayes_hdi_low],\n    \"95% CI Upper\": [ci_high, ols_ci_high, ancova_ci_high, bayes_hdi_high]\n})\n\n# Round values for presentation\neffect_df = effect_df.round(2)\n\nprint(effect_df)\n\n\n🔄 Relationship Between ANCOVA and OLS Regression\nIn this instance, Analysis of Covariance (ANCOVA) and Ordinary Least Squares (OLS) regression provide identical results—but why is that the case?\n\n🧠 Conceptual Overlap\nANCOVA is essentially a special case of linear regression. Specifically, it combines:\n\nANOVA: for testing differences between categorical groups (e.g. treatment vs. control), and\n\nRegression: for adjusting these comparisons based on continuous or additional categorical covariates (e.g. age, sex, smoking status).\n\nThus, ANCOVA is not a fundamentally different statistical method but rather a framework for interpreting a linear model that includes both categorical and continuous predictors. When implemented in software such as Python’s statsmodels, ANCOVA is conducted via the same OLS regression engine as any standard linear regression model.\n\n\n📊 Technical Equivalence\nThe following model, framed as an OLS regression:\nmodel = smf.ols('SBP_Change ~ Treatment + Age + C(Sex) + C(SmokingStatus)', data=df).fit()\nis analytically identical to an ANCOVA testing the effect of Treatment on SBP change, adjusting for Age, Sex, and Smoking Status. The categorical variables are automatically dummy-coded, and the model estimates group differences while controlling for the covariates.\nBoth approaches: - Estimate coefficients using least squares - Provide p-values, confidence intervals, and model diagnostics - Are suitable for continuous outcomes (like SBP change)\n\n\n🧾 Summary\n\n\n\n\n\n\n\n\nTerm\nWhat it Emphasises\nUnderlying Method\n\n\n\n\nANCOVA\nGroup comparison with adjustment\nLinear regression\n\n\nOLS Regression\nFlexible modelling of continuous outcomes\nLinear regression\n\n\n\n✅ Conclusion:\nWhether you call it ANCOVA or OLS regression, you’re using the same statistical engine. The terminology difference mostly reflects intent and interpretation, not implementation.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#primary-outcome-survival",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#primary-outcome-survival",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "🧬 Primary Outcome: Survival",
    "text": "🧬 Primary Outcome: Survival\n\nChi-squared test: Suggested a difference in survival between groups.\nLogistic regression:\n\nHipponol group had higher odds of survival than the control group.\nAdjusted models controlling for age, sex, and smoking status confirmed this effect.\n\nBayesian logistic regression:\n\nThe posterior distribution for the group effect was mostly above 0.\nThe 95% credible interval for the odds ratio excluded 1, supporting a positive treatment effect.\nInterpretation: There is a high probability that Hipponol improves survival.\n\nKaplan–Meier analysis:\n\nSurvival curves diverged, favouring Hipponol.\nLog-rank test confirmed a statistically significant difference.\n\nCox proportional hazards model:\nAdjusted hazard ratio showed reduced risk in the Hipponol group.\nProportional hazards assumption was checked and met.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.1_clinical_trial.html#secondary-outcome-systolic-blood-pressure-sbp",
    "href": "notebooks/10_mini_projects/10.1_clinical_trial.html#secondary-outcome-systolic-blood-pressure-sbp",
    "title": "🧪 Clinical Trial: Hipponol Intervention Study",
    "section": "💉 Secondary Outcome: Systolic Blood Pressure (SBP)",
    "text": "💉 Secondary Outcome: Systolic Blood Pressure (SBP)\n\nVisual exploration: KDE plots showed a shift toward greater SBP reduction in the Hipponol group.\nT-test: A statistically significant greater reduction in SBP in the Hipponol group.\nOLS regression: Consistent estimate with T-test; easy to extend to more predictors.\nANCOVA: Adjusting for baseline SBP, age, sex, and smoking confirmed the treatment effect.\nBayesian regression:\n\nEstimated posterior distribution of the treatment effect.\n95% HDI for the treatment coefficient excluded 0, confirming a likely SBP-lowering effect.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>🧪 Clinical Trial: Hipponol Intervention Study</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.2_epidemiology_case_study.html",
    "href": "notebooks/10_mini_projects/10.2_epidemiology_case_study.html",
    "title": "Epidemiological Analysis: Cross-Sectional and Prospective Studies in Nutrition 🥗📊",
    "section": "",
    "text": "Step 1: Load the Dataset and Libraries 📦\nWelcome to this Jupyter notebook on epidemiological analysis in nutrition science! We’ll explore a large study (n=25,000, age range 45-80) with cross-sectional and prospective designs, focusing on continuous (BMI) and survival (CVD incidence) endpoints. The dataset includes baseline and follow-up data (2, 4, 6 years) on smoking, sex, physical activity, social class (UK ABC12DE), BMI, blood pressure, sugar intake, SFA intake, and CVD incidence, with random missing data.\nIn this notebook, we’ll: - Summarise baseline characteristics with Table 1 🧩 - Analyse missing data to understand patterns - Perform cross-sectional analysis using Frequentist and Bayesian regression (baseline BMI) - Conduct survival analysis for CVD incidence (Frequentist and Bayesian) - Analyse prospective changes in BMI and CVD incidence (Frequentist and Bayesian regression) 📈\nLet’s dive in and explore this epidemiological dataset!\nFirst, let’s load the necessary libraries and the simulated dataset.\n# Import libraries for analysis\n\n# Setup for Google Colab: Fetch datasets automatically or manually\n%run ../../bootstrap.py    # installs requirements + editable package\n\nimport fns_toolkit as fns\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom lifelines import CoxPHFitter\nimport pymc as pm\nimport arviz as az\nfrom statsmodels.formula.api import mixedlm\n\n# Set seaborn style for clean visuals\nsns.set_style(\"whitegrid\")\n\n# Load the dataset\ndata = fns.get_dataset('epidemiological_study')\n\n# Display the first few rows\ndata.head()\n\n\n\n\n\n\n\n\nID\nAge\nSex\nSmoking\nPhysical_Activity\nSocial_Class\nBMI_Baseline\nBP_Baseline\nSugar_Intake\nSFA_Intake\nBMI_Year2\nBP_Year2\nBMI_Year4\nBP_Year4\nBMI_Year6\nBP_Year6\nCVD_Incidence\nTime_to_CVD\n\n\n\n\n0\n1\nNaN\nF\nNo\nMedium\nE\n24.812278\n134.975984\n53.469757\n32.355014\n24.412442\n139.939191\n27.444676\n140.217477\n26.839378\n130.356634\n0.0\n6.0\n\n\n1\n2\n70.0\nM\nNo\nMedium\nE\n29.167431\n121.108245\n56.627170\n45.892542\n29.541868\n122.305808\n30.686958\n126.271005\n30.687262\n116.188290\n0.0\n6.0\n\n\n2\n3\nNaN\nF\nNo\nHigh\nD\n28.636861\n127.287078\n45.976005\n27.468894\n27.955268\n131.731170\n28.295145\n128.151927\n29.985429\n119.701608\n0.0\n6.0\n\n\n3\n4\nNaN\nF\nNo\nHigh\nC1\n30.749889\n125.012160\n49.596112\n29.144261\n32.462470\n125.533971\n31.335076\n128.504605\n31.907326\n130.728587\n0.0\n6.0\n\n\n4\n5\n48.0\nF\nNo\nMedium\nD\n30.181594\n141.078214\n51.031441\n38.224927\n30.627904\n141.412704\n31.440784\n137.994643\n29.890357\n134.377679\n0.0\n6.0",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Epidemiological Analysis: Cross-Sectional and Prospective Studies in Nutrition 🥗📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.2_epidemiology_case_study.html#step-2-table-1---baseline-characteristics",
    "href": "notebooks/10_mini_projects/10.2_epidemiology_case_study.html#step-2-table-1---baseline-characteristics",
    "title": "Epidemiological Analysis: Cross-Sectional and Prospective Studies in Nutrition 🥗📊",
    "section": "Step 2: Table 1 - Baseline Characteristics 📊",
    "text": "Step 2: Table 1 - Baseline Characteristics 📊\nLet’s create Table 1 to summarise the baseline characteristics of the study population, including means (SD) for continuous variables and counts (%) for categorical variables.\n\n# Continuous variables: Mean (SD)\ncontinuous_vars = ['Age', 'BMI_Baseline', 'BP_Baseline', 'Sugar_Intake', 'SFA_Intake']\ncontinuous_summary = data[continuous_vars].agg(['mean', 'std']).round(2).T\ncontinuous_summary.columns = ['Mean', 'SD']\ncontinuous_summary['Mean (SD)'] = continuous_summary['Mean'].astype(str) + ' (' + continuous_summary['SD'].astype(str) + ')'\n\n# Categorical variables: Counts (%)\ncategorical_vars = ['Sex', 'Smoking', 'Physical_Activity', 'Social_Class']\ncategorical_summary = {}\nfor var in categorical_vars:\n    counts = data[var].value_counts(dropna=False)\n    percents = (counts / counts.sum() * 100).round(2)\n    categorical_summary[var] = pd.DataFrame({\n        'Count (%)': [f\"{count} ({percent}%)\" for count, percent in zip(counts, percents)]\n    }, index=counts.index)\n\n# Display Table 1\nprint(\"Table 1: Baseline Characteristics\")\nprint(\"\\nContinuous Variables:\")\nprint(continuous_summary[['Mean (SD)']])\nprint(\"\\nCategorical Variables:\")\nfor var in categorical_vars:\n    print(f\"\\n{var}:\")\n    print(categorical_summary[var])\n\nTable 1: Baseline Characteristics\n\nContinuous Variables:\n                   Mean (SD)\nAge            62.57 (10.42)\nBMI_Baseline    27.04 (4.03)\nBP_Baseline   130.01 (14.96)\nSugar_Intake     50.1 (9.92)\nSFA_Intake       29.9 (8.05)\n\nCategorical Variables:\n\nSex:\n          Count (%)\nSex                \nM     11525 (46.1%)\nF    11516 (46.06%)\nNaN    1959 (7.84%)\n\nSmoking:\n              Count (%)\nSmoking                \nNo       16028 (64.11%)\nYes       6977 (27.91%)\nNaN        1995 (7.98%)\n\nPhysical_Activity:\n                       Count (%)\nPhysical_Activity               \nLow                9270 (37.08%)\nMedium             9097 (36.39%)\nHigh               4613 (18.45%)\nNaN                 2020 (8.08%)\n\nSocial_Class:\n                  Count (%)\nSocial_Class               \nC2            5854 (23.42%)\nC1            5770 (23.08%)\nD             3458 (13.83%)\nB             3436 (13.74%)\nE              2293 (9.17%)\nA              2206 (8.82%)\nNaN            1983 (7.93%)",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Epidemiological Analysis: Cross-Sectional and Prospective Studies in Nutrition 🥗📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.2_epidemiology_case_study.html#step-3-analysis-of-missing-data",
    "href": "notebooks/10_mini_projects/10.2_epidemiology_case_study.html#step-3-analysis-of-missing-data",
    "title": "Epidemiological Analysis: Cross-Sectional and Prospective Studies in Nutrition 🥗📊",
    "section": "Step 3: Analysis of Missing Data 🔎",
    "text": "Step 3: Analysis of Missing Data 🔎\nLet’s assess the extent and pattern of missing data in the dataset to understand potential biases.\n\n# Calculate percentage of missing data for each variable\nmissing_data = data.isna().mean() * 100\nmissing_summary = pd.DataFrame({\n    'Missing (%)': missing_data.round(2)\n})\n\n# Display missing data summary\nprint(\"Missing Data Analysis:\")\nprint(missing_summary[missing_summary['Missing (%)'] &gt; 0])\n\n# Visualize missing data patterns\nplt.figure(figsize=(10, 6))\nsns.heatmap(data.isna(), cbar=False, cmap='viridis')\nplt.title('Missing Data Patterns (Yellow = Missing) 📉')\nplt.tight_layout()\nplt.show()\n\nMissing Data Analysis:\n                   Missing (%)\nAge                       8.12\nSex                       7.84\nSmoking                   7.98\nPhysical_Activity         8.08\nSocial_Class              7.93\nBMI_Baseline              7.76\nBP_Baseline               8.00\nSugar_Intake              8.10\nSFA_Intake                8.01\nBMI_Year2                 7.98\nBP_Year2                  7.96\nBMI_Year4                 8.00\nBP_Year4                  8.14\nBMI_Year6                 7.76\nBP_Year6                  7.91\nCVD_Incidence             8.33\nTime_to_CVD               8.01\n\n\n/var/folders/jr/p60s3gd574d_f62sc7_0bzfm0000gq/T/ipykernel_61181/1996860901.py:15: UserWarning: Glyph 128201 (\\N{CHART WITH DOWNWARDS TREND}) missing from font(s) Arial.\n  plt.tight_layout()\n/Users/gunter/Documents/data-analysis-toolkit-FNS/venv/lib/python3.9/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 128201 (\\N{CHART WITH DOWNWARDS TREND}) missing from font(s) Arial.\n  fig.canvas.print_figure(bytes_io, **kw)",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Epidemiological Analysis: Cross-Sectional and Prospective Studies in Nutrition 🥗📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.2_epidemiology_case_study.html#step-4-cross-sectional-analysis---baseline-bmi",
    "href": "notebooks/10_mini_projects/10.2_epidemiology_case_study.html#step-4-cross-sectional-analysis---baseline-bmi",
    "title": "Epidemiological Analysis: Cross-Sectional and Prospective Studies in Nutrition 🥗📊",
    "section": "Step 4: Cross-Sectional Analysis - Baseline BMI 🧮",
    "text": "Step 4: Cross-Sectional Analysis - Baseline BMI 🧮\nLet’s perform a cross-sectional analysis of baseline BMI, using Frequentist (linear regression) and Bayesian regression, with predictors: age, sex, smoking, physical activity, social class, sugar intake, and SFA intake.\n\nData Preparation\nFirst, we’ll preprocess the data, encoding categorical variables and handling missing data (simple imputation for this example).\n\n# Prepare data for cross-sectional analysis\ncross_sectional_data = data[['BMI_Baseline', 'Age', 'Sex', 'Smoking', 'Physical_Activity', 'Social_Class', 'Sugar_Intake', 'SFA_Intake']].copy()\n\n# Encode categorical variables\nle = LabelEncoder()\ncross_sectional_data['Sex'] = le.fit_transform(cross_sectional_data['Sex'].astype(str))\ncross_sectional_data['Smoking'] = le.fit_transform(cross_sectional_data['Smoking'].astype(str))\ncross_sectional_data['Physical_Activity'] = cross_sectional_data['Physical_Activity'].map({'Low': 0, 'Medium': 1, 'High': 2, np.nan: 0})\ncross_sectional_data['Social_Class'] = cross_sectional_data['Social_Class'].map({'A': 1, 'B': 2, 'C1': 3, 'C2': 4, 'D': 5, 'E': 6, np.nan: 3})\n\n# Impute missing data with mean for simplicity\ncross_sectional_data.fillna(cross_sectional_data.mean(), inplace=True)\n\n# Define predictors and outcome\nX_cross = cross_sectional_data.drop('BMI_Baseline', axis=1)\ny_cross = cross_sectional_data['BMI_Baseline']\n\n\n\nFrequentist Linear Regression\nWe’ll use scikit-learn’s LinearRegression to model baseline BMI.\n\n# Frequentist linear regression\nfreq_model = LinearRegression()\nfreq_model.fit(X_cross, y_cross)\n\n# Coefficients and intercept\nfreq_coefs = pd.DataFrame({\n    'Predictor': X_cross.columns,\n    'Coefficient': freq_model.coef_\n})\nprint(\"Frequentist Linear Regression Results:\")\nprint(f\"Intercept: {freq_model.intercept_:.2f}\")\nprint(freq_coefs)\n\nFrequentist Linear Regression Results:\nIntercept: 26.94\n           Predictor  Coefficient\n0                Age     0.001226\n1                Sex    -0.018906\n2            Smoking    -0.018545\n3  Physical_Activity     0.005869\n4       Social_Class     0.019409\n5       Sugar_Intake    -0.002303\n6         SFA_Intake     0.002939\n\n\n\n\nBayesian Linear Regression\nWe’ll use PyMC to model the same relationship, with weakly informative priors.\n\n# Bayesian linear regression\nwith pm.Model() as bayes_model:\n    # Priors\n    intercept = pm.Normal('Intercept', mu=0, sigma=10)\n    beta_age = pm.Normal('Age', mu=0, sigma=1)\n    beta_sex = pm.Normal('Sex', mu=0, sigma=1)\n    beta_smoking = pm.Normal('Smoking', mu=0, sigma=1)\n    beta_activity = pm.Normal('Physical_Activity', mu=0, sigma=1)\n    beta_class = pm.Normal('Social_Class', mu=0, sigma=1)\n    beta_sugar = pm.Normal('Sugar_Intake', mu=0, sigma=1)\n    beta_sfa = pm.Normal('SFA_Intake', mu=0, sigma=1)\n    sigma = pm.HalfNormal('sigma', sigma=1)\n\n    # Linear model\n    mu = (intercept + beta_age * X_cross['Age'] + beta_sex * X_cross['Sex'] +\n          beta_smoking * X_cross['Smoking'] + beta_activity * X_cross['Physical_Activity'] +\n          beta_class * X_cross['Social_Class'] + beta_sugar * X_cross['Sugar_Intake'] +\n          beta_sfa * X_cross['SFA_Intake'])\n\n    # Likelihood\n    bmi_obs = pm.Normal('bmi_obs', mu=mu, sigma=sigma, observed=y_cross)\n\n    # Sample from posterior\n    trace_cross = pm.sample(1000, tune=1000, return_inferencedata=True)\n\n# Summary of posterior\nprint(\"Bayesian Linear Regression Results:\")\nprint(az.summary(trace_cross, var_names=['Intercept', 'Age', 'Sex', 'Smoking', 'Physical_Activity', 'Social_Class', 'Sugar_Intake', 'SFA_Intake']))\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [Intercept, Age, Sex, Smoking, Physical_Activity, Social_Class, Sugar_Intake, SFA_Intake, sigma]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 01:07&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 67 seconds.\n\n\nBayesian Linear Regression Results:\n                     mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  \\\nIntercept          26.922  0.232  26.493   27.360      0.005    0.004   \nAge                 0.001  0.002  -0.003    0.006      0.000    0.000   \nSex                -0.019  0.038  -0.093    0.049      0.001    0.001   \nSmoking            -0.018  0.037  -0.087    0.052      0.001    0.001   \nPhysical_Activity   0.006  0.032  -0.055    0.064      0.000    0.001   \nSocial_Class        0.019  0.018  -0.016    0.050      0.000    0.000   \nSugar_Intake       -0.002  0.003  -0.007    0.003      0.000    0.000   \nSFA_Intake          0.003  0.003  -0.003    0.009      0.000    0.000   \n\n                   ess_bulk  ess_tail  r_hat  \nIntercept            2017.0    2310.0    1.0  \nAge                  2975.0    2633.0    1.0  \nSex                  4488.0    2946.0    1.0  \nSmoking              3902.0    2704.0    1.0  \nPhysical_Activity    4811.0    2791.0    1.0  \nSocial_Class         4698.0    3234.0    1.0  \nSugar_Intake         3295.0    2882.0    1.0  \nSFA_Intake           4132.0    2926.0    1.0",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Epidemiological Analysis: Cross-Sectional and Prospective Studies in Nutrition 🥗📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.2_epidemiology_case_study.html#step-5-survival-analysis---cvd-incidence",
    "href": "notebooks/10_mini_projects/10.2_epidemiology_case_study.html#step-5-survival-analysis---cvd-incidence",
    "title": "Epidemiological Analysis: Cross-Sectional and Prospective Studies in Nutrition 🥗📊",
    "section": "Step 5: Survival Analysis - CVD Incidence 🕰️",
    "text": "Step 5: Survival Analysis - CVD Incidence 🕰️\nLet’s perform survival analysis for CVD incidence, using Frequentist (Cox proportional hazards) and Bayesian survival regression, with the same predictors.\n\nData Preparation\nWe’ll use the same predictors, ensuring proper encoding and imputation.\n\n# Prepare data for survival analysis\nsurvival_data = data[['Time_to_CVD', 'CVD_Incidence', 'Age', 'Sex', 'Smoking', 'Physical_Activity', 'Social_Class', 'Sugar_Intake', 'SFA_Intake']].copy()\nsurvival_data['Sex'] = le.fit_transform(survival_data['Sex'].astype(str))\nsurvival_data['Smoking'] = le.fit_transform(survival_data['Smoking'].astype(str))\nsurvival_data['Physical_Activity'] = survival_data['Physical_Activity'].map({'Low': 0, 'Medium': 1, 'High': 2, np.nan: 0})\nsurvival_data['Social_Class'] = survival_data['Social_Class'].map({'A': 1, 'B': 2, 'C1': 3, 'C2': 4, 'D': 5, 'E': 6, np.nan: 3})\nsurvival_data.fillna(survival_data.mean(), inplace=True)\n\n\n\nFrequentist Cox Proportional Hazards\nWe’ll use lifelines to fit a Cox model.\n\n# Frequentist Cox model\ncox_model = CoxPHFitter()\ncox_model.fit(survival_data, duration_col='Time_to_CVD', event_col='CVD_Incidence')\n\n# Display results\nprint(\"Frequentist Cox Proportional Hazards Results:\")\ncox_model.print_summary()\n\nFrequentist Cox Proportional Hazards Results:\n\n\n\n\n\n\n\n\nmodel\nlifelines.CoxPHFitter\n\n\nduration col\n'Time_to_CVD'\n\n\nevent col\n'CVD_Incidence'\n\n\nbaseline estimation\nbreslow\n\n\nnumber of observations\n25000\n\n\nnumber of events observed\n2456\n\n\npartial log-likelihood\n-24589.68\n\n\ntime fit was run\n2025-04-27 08:17:44 UTC\n\n\n\n\n\n\n\n\n\ncoef\nexp(coef)\nse(coef)\ncoef lower 95%\ncoef upper 95%\nexp(coef) lower 95%\nexp(coef) upper 95%\ncmp to\nz\np\n-log2(p)\n\n\n\n\nAge\n-0.00\n1.00\n0.00\n-0.01\n0.00\n0.99\n1.00\n0.00\n-0.65\n0.51\n0.96\n\n\nSex\n-0.04\n0.96\n0.03\n-0.10\n0.03\n0.90\n1.03\n0.00\n-1.15\n0.25\n2.00\n\n\nSmoking\n0.01\n1.01\n0.03\n-0.05\n0.07\n0.95\n1.07\n0.00\n0.28\n0.78\n0.36\n\n\nPhysical_Activity\n-0.02\n0.98\n0.03\n-0.08\n0.03\n0.93\n1.03\n0.00\n-0.86\n0.39\n1.37\n\n\nSocial_Class\n-0.03\n0.97\n0.01\n-0.05\n0.00\n0.95\n1.00\n0.00\n-1.73\n0.08\n3.59\n\n\nSugar_Intake\n0.00\n1.00\n0.00\n-0.00\n0.01\n1.00\n1.01\n0.00\n1.89\n0.06\n4.07\n\n\nSFA_Intake\n-0.00\n1.00\n0.00\n-0.01\n0.00\n0.99\n1.00\n0.00\n-0.36\n0.72\n0.48\n\n\n\n\n\n\n\n\n\nConcordance\n0.52\n\n\nPartial AIC\n49193.35\n\n\nlog-likelihood ratio test\n9.37 on 7 df\n\n\n-log2(p) of ll-ratio test\n2.14\n\n\n\n\n\n\n\n\n\nBayesian Survival Regression\nWe’ll use PyMC to fit a Weibull survival model, a common choice for Bayesian survival analysis.\n\n# Bayesian survival regression (Weibull model)\nwith pm.Model() as bayes_survival:\n    # Priors for coefficients\n    beta_age = pm.Normal('Age', mu=0, sigma=1)\n    beta_sex = pm.Normal('Sex', mu=0, sigma=1)\n    beta_smoking = pm.Normal('Smoking', mu=0, sigma=1)\n    beta_activity = pm.Normal('Physical_Activity', mu=0, sigma=1)\n    beta_class = pm.Normal('Social_Class', mu=0, sigma=1)\n    beta_sugar = pm.Normal('Sugar_Intake', mu=0, sigma=1)\n    beta_sfa = pm.Normal('SFA_Intake', mu=0, sigma=1)\n    alpha = pm.Normal('alpha', mu=1, sigma=1)  # Shape parameter for Weibull\n\n    # Linear predictor for scale (lambda)\n    log_lambda = (beta_age * survival_data['Age'] + beta_sex * survival_data['Sex'] +\n                  beta_smoking * survival_data['Smoking'] + beta_activity * survival_data['Physical_Activity'] +\n                  beta_class * survival_data['Social_Class'] + beta_sugar * survival_data['Sugar_Intake'] +\n                  beta_sfa * survival_data['SFA_Intake'])\n    lambda_ = pm.math.exp(log_lambda)\n\n    # Likelihood (Weibull distribution)\n    time_obs = pm.Weibull('time_obs', alpha=alpha, beta=lambda_, observed=survival_data['Time_to_CVD'])\n\n    # Sample from posterior\n    trace_survival = pm.sample(1000, tune=1000, return_inferencedata=True)\n\n# Summary of posterior\nprint(\"Bayesian Survival Regression Results:\")\nprint(az.summary(trace_survival, var_names=['Age', 'Sex', 'Smoking', 'Physical_Activity', 'Social_Class', 'Sugar_Intake', 'SFA_Intake']))\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [Age, Sex, Smoking, Physical_Activity, Social_Class, Sugar_Intake, SFA_Intake, alpha]\n\n\n\n\n\n\n\n    \n      \n      91.83% [7346/8000 04:22&lt;00:23 Sampling 4 chains, 0 divergences]",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Epidemiological Analysis: Cross-Sectional and Prospective Studies in Nutrition 🥗📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.2_epidemiology_case_study.html#step-6-prospective-analysis---bmi-change-and-cvd-incidence",
    "href": "notebooks/10_mini_projects/10.2_epidemiology_case_study.html#step-6-prospective-analysis---bmi-change-and-cvd-incidence",
    "title": "Epidemiological Analysis: Cross-Sectional and Prospective Studies in Nutrition 🥗📊",
    "section": "Step 6: Prospective Analysis - BMI Change and CVD Incidence 🔄",
    "text": "Step 6: Prospective Analysis - BMI Change and CVD Incidence 🔄\nLet’s analyze prospective changes in BMI over time and their association with CVD incidence, using Frequentist (mixed-effects model) and Bayesian regression.\n\nData Preparation\nWe’ll reshape the data into long format for prospective analysis.\n\n# Reshape data into long format for prospective analysis\nlong_data = pd.melt(data, \n                    id_vars=['ID', 'Age', 'Sex', 'Smoking', 'Physical_Activity', 'Social_Class', 'Sugar_Intake', 'SFA_Intake', 'CVD_Incidence'],\n                    value_vars=['BMI_Baseline', 'BMI_Year2', 'BMI_Year4', 'BMI_Year6'],\n                    var_name='Time', value_name='BMI')\nlong_data['Time'] = long_data['Time'].map({'BMI_Baseline': 0, 'BMI_Year2': 2, 'BMI_Year4': 4, 'BMI_Year6': 6})\n\n# Encode categorical variables\nlong_data['Sex'] = le.fit_transform(long_data['Sex'].astype(str))\nlong_data['Smoking'] = le.fit_transform(long_data['Smoking'].astype(str))\nlong_data['Physical_Activity'] = long_data['Physical_Activity'].map({'Low': 0, 'Medium': 1, 'High': 2, np.nan: 0})\nlong_data['Social_Class'] = long_data['Social_Class'].map({'A': 1, 'B': 2, 'C1': 3, 'C2': 4, 'D': 5, 'E': 6, np.nan: 3})\n\n# Impute missing data\nnumeric_cols = long_data.select_dtypes(include=[np.number]).columns\nlong_data[numeric_cols] = long_data[numeric_cols].fillna(long_data[numeric_cols].mean())\n\n\n\nFrequentist Mixed-Effects Model for BMI Change\nWe’ll use statsmodels to fit a mixed-effects model for BMI over time.\n\n# Frequentist mixed-effects model for BMI change\nfreq_mixed_model = mixedlm(\"BMI ~ Time + Age + Sex + Smoking + Physical_Activity + Social_Class + Sugar_Intake + SFA_Intake\", \n                           long_data, groups=long_data['ID'])\nfreq_mixed_result = freq_mixed_model.fit()\n\n# Display results\nprint(\"Frequentist Mixed-Effects Model for BMI Change:\")\nprint(freq_mixed_result.summary())\n\n\n\nBayesian Mixed-Effects Model for BMI Change\nWe’ll use PyMC to fit a Bayesian mixed-effects model.\n\n# Bayesian mixed-effects model for BMI change\nwith pm.Model() as bayes_mixed:\n    # Random intercepts for each participant\n    intercept = pm.Normal('Intercept', mu=0, sigma=10)\n    slope = pm.Normal('Slope', mu=0, sigma=1)\n    beta_age = pm.Normal('Age', mu=0, sigma=1)\n    beta_sex = pm.Normal('Sex', mu=0, sigma=1)\n    beta_smoking = pm.Normal('Smoking', mu=0, sigma=1)\n    beta_activity = pm.Normal('Physical_Activity', mu=0, sigma=1)\n    beta_class = pm.Normal('Social_Class', mu=0, sigma=1)\n    beta_sugar = pm.Normal('Sugar_Intake', mu=0, sigma=1)\n    beta_sfa = pm.Normal('SFA_Intake', mu=0, sigma=1)\n    sigma = pm.HalfNormal('sigma', sigma=1)\n\n    # Linear model\n    mu = (intercept + slope * long_data['Time'] + beta_age * long_data['Age'] +\n          beta_sex * long_data['Sex'] + beta_smoking * long_data['Smoking'] +\n          beta_activity * long_data['Physical_Activity'] + beta_class * long_data['Social_Class'] +\n          beta_sugar * long_data['Sugar_Intake'] + beta_sfa * long_data['SFA_Intake'])\n\n    # Likelihood\n    bmi_obs = pm.Normal('bmi_obs', mu=mu, sigma=sigma, observed=long_data['BMI'])\n\n    # Sample from posterior\n    trace_mixed = pm.sample(1000, tune=1000, return_inferencedata=True)\n\n# Summary of posterior\nprint(\"Bayesian Mixed-Effects Model for BMI Change:\")\nprint(az.summary(trace_mixed, var_names=['Intercept', 'Slope', 'Age', 'Sex', 'Smoking', 'Physical_Activity', 'Social_Class', 'Sugar_Intake', 'SFA_Intake']))\n\n\n\nFrequentist Logistic Regression for CVD Incidence (Prospective)\nWe’ll use logistic regression to assess prospective predictors of CVD incidence.\n\n# Frequentist logistic regression for CVD incidence (prospective)\nfrom sklearn.linear_model import LogisticRegression\n\n# Prepare data (use baseline predictors and average BMI over time)\nprospective_data = long_data.groupby('ID').agg({\n    'Age': 'first', 'Sex': 'first', 'Smoking': 'first', 'Physical_Activity': 'first',\n    'Social_Class': 'first', 'Sugar_Intake': 'first', 'SFA_Intake': 'first', 'CVD_Incidence': 'first',\n    'BMI': 'mean'\n}).reset_index()\n\n# Fit logistic regression\nX_prosp = prospective_data[['Age', 'Sex', 'Smoking', 'Physical_Activity', 'Social_Class', 'Sugar_Intake', 'SFA_Intake', 'BMI']]\ny_prosp = prospective_data['CVD_Incidence']\nfreq_logistic = LogisticRegression(max_iter=1000)\nfreq_logistic.fit(X_prosp, y_prosp)\n\n# Coefficients\nfreq_logistic_coefs = pd.DataFrame({\n    'Predictor': X_prosp.columns,\n    'Coefficient': freq_logistic.coef_[0]\n})\nprint(\"Frequentist Logistic Regression for CVD Incidence:\")\nprint(freq_logistic_coefs)\n\n\n\nBayesian Logistic Regression for CVD Incidence (Prospective)\nWe’ll use PyMC to fit a Bayesian logistic regression model.\n\n# Bayesian logistic regression for CVD incidence\nwith pm.Model() as bayes_logistic:\n    # Priors\n    beta_age = pm.Normal('Age', mu=0, sigma=1)\n    beta_sex = pm.Normal('Sex', mu=0, sigma=1)\n    beta_smoking = pm.Normal('Smoking', mu=0, sigma=1)\n    beta_activity = pm.Normal('Physical_Activity', mu=0, sigma=1)\n    beta_class = pm.Normal('Social_Class', mu=0, sigma=1)\n    beta_sugar = pm.Normal('Sugar_Intake', mu=0, sigma=1)\n    beta_sfa = pm.Normal('SFA_Intake', mu=0, sigma=1)\n    beta_bmi = pm.Normal('BMI', mu=0, sigma=1)\n\n    # Linear predictor\n    logits = (beta_age * X_prosp['Age'] + beta_sex * X_prosp['Sex'] +\n              beta_smoking * X_prosp['Smoking'] + beta_activity * X_prosp['Physical_Activity'] +\n              beta_class * X_prosp['Social_Class'] + beta_sugar * X_prosp['Sugar_Intake'] +\n              beta_sfa * X_prosp['SFA_Intake'] + beta_bmi * X_prosp['BMI'])\n\n    # Likelihood\n    cvd_obs = pm.Bernoulli('cvd_obs', logit_p=logits, observed=y_prosp)\n\n    # Sample from posterior\n    trace_logistic = pm.sample(1000, tune=1000, return_inferencedata=True)\n\n# Summary of posterior\nprint(\"Bayesian Logistic Regression for CVD Incidence:\")\nprint(az.summary(trace_logistic, var_names=['Age', 'Sex', 'Smoking', 'Physical_Activity', 'Social_Class', 'Sugar_Intake', 'SFA_Intake', 'BMI']))",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Epidemiological Analysis: Cross-Sectional and Prospective Studies in Nutrition 🥗📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.2_epidemiology_case_study.html#step-7-learning-points-and-next-steps",
    "href": "notebooks/10_mini_projects/10.2_epidemiology_case_study.html#step-7-learning-points-and-next-steps",
    "title": "Epidemiological Analysis: Cross-Sectional and Prospective Studies in Nutrition 🥗📊",
    "section": "Step 7: Learning Points and Next Steps 🎓",
    "text": "Step 7: Learning Points and Next Steps 🎓\n\nLearning Points\n\nTable 1: Summarised baseline characteristics, providing a clear overview of the study population.\nMissing Data: Identified patterns of missingness (~8% per variable), which should be considered in analysis (e.g., imputation strategies).\nCross-Sectional Analysis: Frequentist and Bayesian regression showed similar predictors of baseline BMI, with Bayesian providing uncertainty quantification.\nSurvival Analysis: Cox and Bayesian survival models highlighted SFA intake as a key predictor of CVD incidence, consistent with the simulated association.\nProspective Analysis: Mixed-effects models confirmed sugar intake’s association with BMI increase, and logistic regression identified predictors of CVD incidence, with Bayesian models offering probabilistic insights.\n\n\n\nNext Steps\n\nAdvanced Imputation: Use multiple imputation for missing data to reduce bias.\nInteraction Terms: Explore interactions (e.g., age × SFA intake) in survival models.\nSensitivity Analysis: Test the impact of different priors in Bayesian models.\nFurther Outcomes: Analyze other outcomes, like blood pressure changes over time.\n\nKeep exploring epidemiological methods to uncover insights in nutrition science! 🥕📉\n\n\n\nSetup Requirements\n\nInstall Libraries:\nsource ~/Documents/data-analysis-toolkit-FNS/venv/bin/activate\npip install numpy pandas matplotlib seaborn scipy pymc arviz scikit-learn lifelines statsmodels\nEnvironment: Python 3.9, compatible with Apple Silicon (MPS).\nDataset: Ensure data/epidemiological_study.csv is available (generated by the simulation script).\n\n\n\nExpected Output\n\nTable 1: Descriptive statistics for baseline characteristics.\nMissing Data Plot: Heatmap showing missing data patterns.\nCross-Sectional Results: Coefficients from Frequentist and Bayesian regression for baseline BMI.\nSurvival Results: Hazard ratios (Frequentist) and posterior summaries (Bayesian) for CVD incidence.\nProspective Results: Coefficients for BMI change and CVD incidence from Frequentist and Bayesian models.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Epidemiological Analysis: Cross-Sectional and Prospective Studies in Nutrition 🥗📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.3_handling_missing_data.html",
    "href": "notebooks/10_mini_projects/10.3_handling_missing_data.html",
    "title": "Handling Missing Data in Nutrition Science",
    "section": "",
    "text": "Step 1: Load the Dataset and Libraries 📦\n🧩📊Welcome to this Jupyter notebook on handling missing data in nutrition science! Missing data is a common challenge in epidemiological studies, especially in nutrition research where variables like dietary intake or biomarkers may be incomplete. In this mini-project, we’ll use the epidemiological dataset from our previous work (n=25,000, age range 45-80) to explore and address missing data.We’ll:\nLet’s load the epidemiological dataset and the libraries we’ll need for this analysis. The dataset includes variables like age, sex, smoking, physical activity, social class, BMI, blood pressure, sugar intake, SFA intake, and CVD incidence, with ~8% missing data per variable.\n# Setup for Google Colab: Fetch datasets automatically or manually\n%run ../../bootstrap.py    # installs requirements + editable package\n\nimport fns_toolkit as fns\n\n# Import libraries for analysis\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimport pymc as pm\nimport arviz as az\n\n# Set seaborn style for clean visuals\nsns.set_style(\"whitegrid\")\n\n# Load the dataset\ndata = fns.get_dataset('epidemiological_study')\n\n\n# Display the first few rows\ndata.head()",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Handling Missing Data in Nutrition Science</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.3_handling_missing_data.html#step-2-explore-missing-data-patterns",
    "href": "notebooks/10_mini_projects/10.3_handling_missing_data.html#step-2-explore-missing-data-patterns",
    "title": "Handling Missing Data in Nutrition Science",
    "section": "Step 2: Explore Missing Data Patterns 🔎",
    "text": "Step 2: Explore Missing Data Patterns 🔎\nLet’s start by examining the extent and patterns of missing data in our dataset. This will help us understand which variables are most affected and whether the missingness appears random or systematic.\n\n# Calculate percentage of missing data for each variable\nmissing_data = data.isna().mean() * 100\nmissing_summary = pd.DataFrame({\n    'Missing (%)': missing_data.round(2)\n})\n\n# Display missing data summary\nprint(\"Missing Data Summary:\")\nprint(missing_summary[missing_summary['Missing (%)'] &gt; 0])\n\n# Visualize missing data patterns\nplt.figure(figsize=(12, 8))\nsns.heatmap(data.isna(), cbar=False, cmap='viridis')\nplt.title('Missing Data Patterns (Yellow = Missing) 📉')\nplt.xlabel('Variables')\nplt.ylabel('Participants')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Handling Missing Data in Nutrition Science</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.3_handling_missing_data.html#step-3-prepare-the-data-for-analysis",
    "href": "notebooks/10_mini_projects/10.3_handling_missing_data.html#step-3-prepare-the-data-for-analysis",
    "title": "Handling Missing Data in Nutrition Science",
    "section": "Step 3: Prepare the Data for Analysis 🛠️",
    "text": "Step 3: Prepare the Data for Analysis 🛠️\nWe’ll prepare the data for a simple analysis: predicting baseline BMI using age, sex, smoking, physical activity, social class, sugar intake, and SFA intake. First, we need to encode categorical variables and select the relevant columns.\n\n# Select relevant columns for analysis\nanalysis_data = data[['BMI_Baseline', 'Age', 'Sex', 'Smoking', 'Physical_Activity', 'Social_Class', 'Sugar_Intake', 'SFA_Intake']].copy()\n\n# Encode categorical variables\nle = LabelEncoder()\nanalysis_data['Sex'] = le.fit_transform(analysis_data['Sex'].astype(str))\nanalysis_data['Smoking'] = le.fit_transform(analysis_data['Smoking'].astype(str))\nanalysis_data['Physical_Activity'] = analysis_data['Physical_Activity'].map({'Low': 0, 'Medium': 1, 'High': 2, np.nan: 0})\nanalysis_data['Social_Class'] = analysis_data['Social_Class'].map({'A': 1, 'B': 2, 'C1': 3, 'C2': 4, 'D': 5, 'E': 6, np.nan: 3})\n\n# Define predictors and outcome\nX = analysis_data.drop('BMI_Baseline', axis=1)\ny = analysis_data['BMI_Baseline']\n\n# Display the first few rows of the prepared data\nX.head()",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Handling Missing Data in Nutrition Science</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.3_handling_missing_data.html#step-4-technique-1---listwise-deletion",
    "href": "notebooks/10_mini_projects/10.3_handling_missing_data.html#step-4-technique-1---listwise-deletion",
    "title": "Handling Missing Data in Nutrition Science",
    "section": "Step 4: Technique 1 - Listwise Deletion 🚮",
    "text": "Step 4: Technique 1 - Listwise Deletion 🚮\nThe simplest approach to handling missing data is listwise deletion, where we remove any row with at least one missing value. This method is easy but can lead to loss of data and potential bias if the missingness is not completely random.\n\n# Apply listwise deletion\nX_listwise = X.dropna()\ny_listwise = y[X_listwise.index]\n\n# Check the number of rows remaining\nprint(f\"Original dataset size: {X.shape[0]} rows\")\nprint(f\"After listwise deletion: {X_listwise.shape[0]} rows\")\n\n# Fit a linear regression model\nmodel_listwise = LinearRegression()\nmodel_listwise.fit(X_listwise, y_listwise)\n\n# Display coefficients\ncoeffs_listwise = pd.DataFrame({\n    'Predictor': X_listwise.columns,\n    'Coefficient': model_listwise.coef_\n})\nprint(\"\\nLinear Regression Results (Listwise Deletion):\")\nprint(coeffs_listwise)",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Handling Missing Data in Nutrition Science</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.3_handling_missing_data.html#step-5-technique-2---meanmode-imputation",
    "href": "notebooks/10_mini_projects/10.3_handling_missing_data.html#step-5-technique-2---meanmode-imputation",
    "title": "Handling Missing Data in Nutrition Science",
    "section": "Step 5: Technique 2 - Mean/Mode Imputation 📏",
    "text": "Step 5: Technique 2 - Mean/Mode Imputation 📏\nAnother common method is mean/mode imputation, where we replace missing values with the mean (for numerical variables) or mode (for categorical variables). This preserves the sample size but can underestimate variability.\n\n# Create a copy of the data for mean/mode imputation\nX_mean_mode = X.copy()\n\n# Impute numerical variables with mean\nnumerical_vars = ['Age', 'Sugar_Intake', 'SFA_Intake']\nX_mean_mode[numerical_vars] = X_mean_mode[numerical_vars].fillna(X_mean_mode[numerical_vars].mean())\n\n# Impute categorical variables with mode\ncategorical_vars = ['Sex', 'Smoking', 'Physical_Activity', 'Social_Class']\nX_mean_mode[categorical_vars] = X_mean_mode[categorical_vars].fillna(X_mean_mode[categorical_vars].mode().iloc[0])\n\n# Impute the outcome variable (BMI_Baseline)\ny_mean_mode = y.fillna(y.mean())\n\n# Fit a linear regression model\nmodel_mean_mode = LinearRegression()\nmodel_mean_mode.fit(X_mean_mode, y_mean_mode)\n\n# Display coefficients\ncoeffs_mean_mode = pd.DataFrame({\n    'Predictor': X_mean_mode.columns,\n    'Coefficient': model_mean_mode.coef_\n})\nprint(\"Linear Regression Results (Mean/Mode Imputation):\")\nprint(coeffs_mean_mode)",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Handling Missing Data in Nutrition Science</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.3_handling_missing_data.html#step-6-technique-3---multiple-imputation",
    "href": "notebooks/10_mini_projects/10.3_handling_missing_data.html#step-6-technique-3---multiple-imputation",
    "title": "Handling Missing Data in Nutrition Science",
    "section": "Step 6: Technique 3 - Multiple Imputation 🔄",
    "text": "Step 6: Technique 3 - Multiple Imputation 🔄\nA more sophisticated approach is multiple imputation, which creates multiple plausible datasets by imputing missing values, then combines the results. We’ll use IterativeImputer from scikit-learn, which models each variable with missing values as a function of the others.\n\n# Combine predictors and outcome for imputation\ncombined_data = X.copy()\ncombined_data['BMI_Baseline'] = y\n\n# Apply multiple imputation\nimputer = IterativeImputer(max_iter=10, random_state=11088)\nimputed_data = pd.DataFrame(imputer.fit_transform(combined_data), columns=combined_data.columns)\n\n# Separate predictors and outcome after imputation\nX_multiple = imputed_data.drop('BMI_Baseline', axis=1)\ny_multiple = imputed_data['BMI_Baseline']\n\n# Fit a linear regression model\nmodel_multiple = LinearRegression()\nmodel_multiple.fit(X_multiple, y_multiple)\n\n# Display coefficients\ncoeffs_multiple = pd.DataFrame({\n    'Predictor': X_multiple.columns,\n    'Coefficient': model_multiple.coef_\n})\nprint(\"Linear Regression Results (Multiple Imputation):\")\nprint(coeffs_multiple)",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Handling Missing Data in Nutrition Science</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.3_handling_missing_data.html#step-7-technique-4---bayesian-imputation",
    "href": "notebooks/10_mini_projects/10.3_handling_missing_data.html#step-7-technique-4---bayesian-imputation",
    "title": "Handling Missing Data in Nutrition Science",
    "section": "Step 7: Technique 4 - Bayesian Imputation 🌐",
    "text": "Step 7: Technique 4 - Bayesian Imputation 🌐\nFinally, let’s use a Bayesian approach to impute missing data. We’ll model the data with PyMC, treating missing values as parameters to be estimated, and then use the imputed dataset for regression.\n\n# Create a copy of the data for Bayesian imputation\nX_bayesian = X.copy()\ny_bayesian = y.copy()\n\n# Identify missing values\nmissing_mask_X = X_bayesian.isna()\nmissing_mask_y = y_bayesian.isna()\n\n# Bayesian imputation model\nwith pm.Model() as bayesian_imputation:\n    # Priors for observed data means\n    mu_age = pm.Normal('mu_age', mu=60, sigma=10)\n    mu_sugar = pm.Normal('mu_sugar', mu=50, sigma=10)\n    mu_sfa = pm.Normal('mu_sfa', mu=30, sigma=10)\n    mu_bmi = pm.Normal('mu_bmi', mu=27, sigma=5)\n\n    # Priors for standard deviations\n    sigma_age = pm.HalfNormal('sigma_age', sigma=5)\n    sigma_sugar = pm.HalfNormal('sigma_sugar', sigma=5)\n    sigma_sfa = pm.HalfNormal('sigma_sfa', sigma=5)\n    sigma_bmi = pm.HalfNormal('sigma_bmi', sigma=2)\n\n    # Impute missing numerical variables\n    age_imputed = pm.Normal('age_imputed', mu=mu_age, sigma=sigma_age, shape=X_bayesian.shape[0], observed=X_bayesian['Age'])\n    sugar_imputed = pm.Normal('sugar_imputed', mu=mu_sugar, sigma=sigma_sugar, shape=X_bayesian.shape[0], observed=X_bayesian['Sugar_Intake'])\n    sfa_imputed = pm.Normal('sfa_imputed', mu=mu_sfa, sigma=sigma_sfa, shape=X_bayesian.shape[0], observed=X_bayesian['SFA_Intake'])\n    bmi_imputed = pm.Normal('bmi_imputed', mu=mu_bmi, sigma=sigma_bmi, shape=y_bayesian.shape[0], observed=y_bayesian)\n\n    # Sample from posterior\n    trace = pm.sample(1000, tune=1000, return_inferencedata=True)\n\n# Extract imputed values (use the mean of the posterior samples)\nX_bayesian['Age'] = trace.posterior['age_imputed'].mean(dim=['chain', 'draw']).values\nX_bayesian['Sugar_Intake'] = trace.posterior['sugar_imputed'].mean(dim=['chain', 'draw']).values\nX_bayesian['SFA_Intake'] = trace.posterior['sfa_imputed'].mean(dim=['chain', 'draw']).values\ny_bayesian = trace.posterior['bmi_imputed'].mean(dim=['chain', 'draw']).values\n\n# For categorical variables, use mode imputation (already handled during preparation)\n\n# Fit a linear regression model\nmodel_bayesian = LinearRegression()\nmodel_bayesian.fit(X_bayesian, y_bayesian)\n\n# Display coefficients\ncoeffs_bayesian = pd.DataFrame({\n    'Predictor': X_bayesian.columns,\n    'Coefficient': model_bayesian.coef_\n})\nprint(\"Linear Regression Results (Bayesian Imputation):\")\nprint(coeffs_bayesian)",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Handling Missing Data in Nutrition Science</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.3_handling_missing_data.html#step-8-compare-the-results",
    "href": "notebooks/10_mini_projects/10.3_handling_missing_data.html#step-8-compare-the-results",
    "title": "Handling Missing Data in Nutrition Science",
    "section": "Step 8: Compare the Results 🔍",
    "text": "Step 8: Compare the Results 🔍\nLet’s compare the regression coefficients from each method to see how the choice of imputation technique affects the results.\n\n# Combine coefficients from all methods\ncomparison = pd.DataFrame({\n    'Predictor': X.columns,\n    'Listwise Deletion': coeffs_listwise['Coefficient'],\n    'Mean/Mode Imputation': coeffs_mean_mode['Coefficient'],\n    'Multiple Imputation': coeffs_multiple['Coefficient'],\n    'Bayesian Imputation': coeffs_bayesian['Coefficient']\n})\n\n# Display the comparison\nprint(\"Comparison of Regression Coefficients Across Methods:\")\nprint(comparison)\n\n# Visualize the comparison\ncomparison.set_index('Predictor').plot(kind='bar', figsize=(12, 6))\nplt.title('Comparison of Regression Coefficients by Imputation Method 📊')\nplt.ylabel('Coefficient Value')\nplt.xlabel('Predictor')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Handling Missing Data in Nutrition Science</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.3_handling_missing_data.html#step-9-learning-points-and-next-steps",
    "href": "notebooks/10_mini_projects/10.3_handling_missing_data.html#step-9-learning-points-and-next-steps",
    "title": "Handling Missing Data in Nutrition Science",
    "section": "Step 9: Learning Points and Next Steps 🎓",
    "text": "Step 9: Learning Points and Next Steps 🎓\n\nLearning Points\n\nMissing Data Patterns: Visualizing and quantifying missing data helps us understand its extent and potential bias. In our dataset, ~8% of values were missing per variable, distributed randomly\nListwise Deletion: Simple but reduces sample size (e.g., from 25,000 to fewer rows), potentially introducing bias if missingness is not completely random.\nMean/Mode Imputation: Preserves sample size but underestimates variability, which can lead to overly confident estimates.\nMultiple Imputation: A more robust method that accounts for uncertainty by creating multiple datasets, often yielding more reliable results.\nBayesian Imputation: Treats missing values as parameters, providing a probabilistic approach that can capture uncertainty and relationships between variables.\nImpact on Analysis: Different methods led to slight variations in regression coefficients, highlighting the importance of choosing an appropriate technique.\n\n\n\nNext Steps\n\nExplore Missingness Mechanisms: Test if the missing data is Missing Completely at Random (MCAR), Missing at Random (MAR), or Missing Not at Random (MNAR).\nAdvanced Bayesian Models: Use more complex Bayesian models to impute missing data, incorporating relationships between variables (e.g., hierarchical models).\nSensitivity Analysis: Compare results with different imputation methods to assess robustness.\nApply to Other Analyses: Use these techniques in other mini-projects (e.g., epidemiology case study) to improve data quality.\n\nKeep exploring data handling techniques to ensure robust analyses in nutrition science! 🥕📉\n\n\n\n\nSetup Requirements\n\nInstall Libraries:\nsource ~/Documents/data-analysis-toolkit-FNS/venv/bin/activate\npip install numpy==1.26.4 pandas==2.2.3 matplotlib==3.9.2 seaborn==0.13.2 scipy==1.12.0 pymc==5.16.2 arviz==0.19.0 scikit-learn==1.5.2\nEnvironment:\nPython 3.9, compatible with Apple Silicon (MPS).\nDataset: Ensure data/epidemiological_study.csv is available (generated by create_epi_data.py).\n\n\n\nExpected Output\n\nMissing Data Summary: Table and heatmap showing the extent of missing data.\nRegression Results: Coefficients from linear regression using each imputation method.\nComparison Plot: Bar chart comparing coefficients across methods.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Handling Missing Data in Nutrition Science</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.4_metabolomics.html",
    "href": "notebooks/10_mini_projects/10.4_metabolomics.html",
    "title": "🧪 Multivariate Analysis: Concepts and Applications",
    "section": "",
    "text": "1. Introduction to Multivariate Analysis 📊\nWelcome to this notebook on multivariate analysis!\nMultivariate methods are essential for exploring, modelling, and understanding complex datasets — where many variables interact simultaneously.\nWe’ll explore: - ✨ Key concepts of multivariate analysis - 🧩 Principal Component Analysis (PCA) - 🎯 Partial Least Squares Discriminant Analysis (PLS-DA) - 📈 Bayesian multivariate models - 🔍 Machine learning approaches to classification and prediction\nAs a practical example, we’ll use a synthetic metabolomics dataset, similar to real-world studies, to illustrate the methods.\nHowever, the techniques you will learn apply just as well to fields like nutrition, clinical data, finance, or engineering!\nLet’s dive in! 🚀\nBefore we start, let’s set up the workspace, load the data and the necessary libraries:\nMultivariate datasets - such as for example in metabolomics - are like a galaxy of stars ✨ — thousands of data, each twinkling with information. Multivariate analysis helps us find patterns, classify samples (e.g., healthy vs. diseased), and uncover biomarkers. These methods are essential because:\nIn this notebook, we will use Python with libraries such as scikit-learn, PyMC, and pandas to explore these techniques.\nIf you are new to this area — no problem! We’ll guide you through each step. 😊",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>🧪 Multivariate Analysis: Concepts and Applications</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.4_metabolomics.html#introduction-to-multivariate-analysis",
    "href": "notebooks/10_mini_projects/10.4_metabolomics.html#introduction-to-multivariate-analysis",
    "title": "🧪 Multivariate Analysis: Concepts and Applications",
    "section": "",
    "text": "High-dimensionality: Metabolomics data often have more variables (metabolites) than samples.\nCorrelations: Metabolites do not act in isolation; they interact in complex and structured ways.\nNoise: Biological and technical variability can obscure true signals.\n\n\n\n\nExercise 1\nWhy do you think multivariate methods are better than analysing each metabolite separately?\nWrite your thoughts below — no code needed, just reflect.\n\n\n💡 Hint\n\nThink about how metabolites might be connected through biological pathways, and why examining them together could reveal patterns that are invisible when looking at them one by one.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>🧪 Multivariate Analysis: Concepts and Applications</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.4_metabolomics.html#principal-component-analysis-pca-the-unsupervised-explorer",
    "href": "notebooks/10_mini_projects/10.4_metabolomics.html#principal-component-analysis-pca-the-unsupervised-explorer",
    "title": "🧪 Multivariate Analysis: Concepts and Applications",
    "section": "2. Principal Component Analysis (PCA): The Unsupervised Explorer 🗺️",
    "text": "2. Principal Component Analysis (PCA): The Unsupervised Explorer 🗺️\nPCA is like a treasure map for your data — it reduces dimensionality by finding principal components (PCs) that capture the greatest variance.\nIn metabolomics, PCA helps us:\n\nVisualise sample similarities and groupings 🧩\nDetect outliers 🚨\nExplore underlying structure without using class labels (unsupervised)\n\nPCA projects high-dimensional data onto a smaller number of dimensions while preserving as much information as possible.\nIt is often the first step in a metabolomics analysis to get a quick overview of the dataset.\nWe will implement PCA using scikit-learn and visualise the results with matplotlib and seaborn. 📈\n\n\nExercise 2\nBefore we dive into the code, think about this:\nWhy might PCA sometimes hide important biological information?\n\n\n💡 Hint\n\nPCA is optimised for variance, not necessarily for biological relevance. Sometimes important differences (e.g., between healthy and diseased) might not be the biggest source of variance!",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>🧪 Multivariate Analysis: Concepts and Applications</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.4_metabolomics.html#step-1-visualizing-the-messy-raw-data",
    "href": "notebooks/10_mini_projects/10.4_metabolomics.html#step-1-visualizing-the-messy-raw-data",
    "title": "🧪 Multivariate Analysis: Concepts and Applications",
    "section": "Step 1: Visualizing the Messy Raw Data 📊",
    "text": "Step 1: Visualizing the Messy Raw Data 📊\nOur dataset has 200 metabolites per participant, creating a high-dimensional maze. Direct visualization is nearly impossible (imagine a 200-dimensional scatter plot!). Instead, we’ll use two techniques to show the data’s “messiness”:\n\nCorrelation Heatmap: Reveals pairwise correlations between metabolites. High correlations suggest redundant features, a common issue in metabolomics that PCA can address.\nPairwise Scatter Plots: Shows relationships between a subset of metabolites, highlighting how scattered and unstructured the raw data appears.\n\nThese plots will demonstrate why we need PCA to simplify this complex dataset.\n\n# Visualize raw data: Correlation heatmap\nplt.figure(figsize=(10, 8))  # Larger size for 200 metabolites\nX_df = df.filter(like='Metabolite_')  # Ensure we use the DataFrame\ncorr = X_df.corr()  # Compute pairwise correlations\nsns.heatmap(corr, cmap='coolwarm', center=0, vmin=-1, vmax=1)\nplt.title('Correlation Heatmap of 200 Metabolites')\nplt.tight_layout()\nplt.show()\n\n\nWhat’s Happening Here? 🤔\nThe heatmap shows correlations between our 200 metabolites. Notice the dense patterns of red (positive) and blue (negative) correlations — this redundancy makes the data “messy” and hard to interpret. Many metabolites move together, suggesting we can reduce dimensions without losing much information.\nNext, let’s try visualizing pairs of metabolites to see if patterns emerge naturally.\n\n# Visualize raw data: Pairwise scatter plots for top 5 metabolites\nsubset_cols = df.filter(like='Metabolite_').columns[:5]  # Select first 5 metabolite columns\nsns.pairplot(df[subset_cols], diag_kind='kde', plot_kws={'alpha': 0.5})\nplt.suptitle('Pairwise Scatter Plots of Raw Metabolomics Data', y=1.02)\nplt.show()\n\n\n\nThe High-Dimensional Challenge 🌪️\nThe scatter plots show relationships between just 5 of our 200 metabolites, and already it’s chaotic! Points are scattered, with no clear groupings, and we’re only seeing a tiny slice of the data. Imagine trying to plot all 200 dimensions — it’s impossible! This messiness is why PCA is our go-to tool: it finds the directions (principal components) that capture the most variance, simplifying the data into a 2D map we can explore.\nBefore PCA, we need to preprocess the data to ensure fair comparisons\n\n# Preprocess: Standardize features to ensure equal scale\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>🧪 Multivariate Analysis: Concepts and Applications</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.4_metabolomics.html#step-2-applying-pca",
    "href": "notebooks/10_mini_projects/10.4_metabolomics.html#step-2-applying-pca",
    "title": "🧪 Multivariate Analysis: Concepts and Applications",
    "section": "Step 2: Applying PCA 🧑‍🔬",
    "text": "Step 2: Applying PCA 🧑‍🔬\nNow that we’ve seen the raw data’s complexity, let’s apply PCA to reduce our 200 metabolites to 2 principal components (PCs). PCA identifies the directions of maximum variance, projecting our high-dimensional data onto a 2D plane. We’ll use scikit-learn’s PCA to do this efficiently.\nStandardization (via StandardScaler) was critical to ensure metabolites with different scales (e.g., concentrations) don’t skew the results. Let’s transform the data and explore the results!\n\n# Apply PCA to reduce to 2 components\npca = PCA(n_components=2)\npca_result = pca.fit_transform(X_scaled)",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>🧪 Multivariate Analysis: Concepts and Applications</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.4_metabolomics.html#step-3-detecting-outliers-with-hotellings-t²",
    "href": "notebooks/10_mini_projects/10.4_metabolomics.html#step-3-detecting-outliers-with-hotellings-t²",
    "title": "🧪 Multivariate Analysis: Concepts and Applications",
    "section": "Step 3: Detecting Outliers with Hotelling’s T² 🚨",
    "text": "Step 3: Detecting Outliers with Hotelling’s T² 🚨\nPCA helps us spot outliers — samples that deviate significantly from the main data cloud. We’ll use Hotelling’s T², a statistical test that measures how far each sample is from the center of the PCA scores, accounting for the data’s variance. Samples outside a 95% confidence ellipse are flagged as outliers.\nWe’ll compute T² scores using numpy.linalg and scipy.stats.chi2, then visualize them in our PCA plot.\n\n# Ensure labels are numerical for plotting (if categorical, encode them)\nif labels.dtype == 'object':\n    labels = pd.Categorical(labels).codes  # Convert to numerical codes (e.g., 0, 1)\n\n# Calculate Hotelling's T^2 for outlier detection\nn_samples, n_components = pca_result.shape\nmean_scores = np.mean(pca_result, axis=0)\ncov_scores = np.cov(pca_result.T)\n# Ensure covariance matrix is well-conditioned\nif np.linalg.cond(cov_scores) &lt; 1e6:  # Check condition number\n    inv_cov = la.inv(cov_scores)\nelse:\n    # Add small diagonal for numerical stability\n    cov_scores += np.eye(n_components) * 1e-6\n    inv_cov = la.inv(cov_scores)\n\n# Compute T^2 scores\nt2_scores = np.array([\n    (pca_result[i] - mean_scores).T @ inv_cov @ (pca_result[i] - mean_scores)\n    for i in range(n_samples)\n])\n\n# Use chi-squared distribution for 95% confidence ellipse (simpler for 2D PCA)\nalpha = 0.05\ncritical_value = chi2.ppf(1 - alpha, df=n_components)  # Chi-squared with 2 degrees of freedom\n\n# Identify outliers\noutliers = t2_scores &gt; critical_value\n\n# Compute ellipse for 95% confidence region\neigenvalues, eigenvectors = np.linalg.eig(cov_scores)\nradii = np.sqrt(critical_value * eigenvalues)  # Scale by chi-squared critical value\ntheta = np.linspace(0, 2 * np.pi, 100)\nellipse = (eigenvectors @ np.diag(radii) @ np.array([np.cos(theta), np.sin(theta)])).T + mean_scores\n\n# Visualize PCA results with Hotelling's T^2 ellipse\nplt.figure(figsize=(8, 6))\nplt.scatter(pca_result[:, 0], pca_result[:, 1], c=labels, cmap='viridis', alpha=0.7, label='Samples')\nplt.scatter(pca_result[outliers, 0], pca_result[outliers, 1], c='red', s=100, marker='x', label='Outliers')\nplt.plot(ellipse[:, 0], ellipse[:, 1], 'k--', label='95% Confidence Ellipse')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA of Metabolomics Data (1000 Participants) 🗺️')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Print explained variance ratio and outlier count\nprint(f'Explained Variance Ratio: PC1 = {pca.explained_variance_ratio_[0]:.2f}, '\n      f'PC2 = {pca.explained_variance_ratio_[1]:.2f}')\nprint(f'Number of Outliers Detected: {np.sum(outliers)}')",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>🧪 Multivariate Analysis: Concepts and Applications</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.4_metabolomics.html#step-4-interpreting-the-pca-results",
    "href": "notebooks/10_mini_projects/10.4_metabolomics.html#step-4-interpreting-the-pca-results",
    "title": "🧪 Multivariate Analysis: Concepts and Applications",
    "section": "Step 4: Interpreting the PCA Results 🧩",
    "text": "Step 4: Interpreting the PCA Results 🧩\nWow, look at that plot! Compared to the raw data’s chaos, PCA reveals clear groupings and outliers. The explained variance ratio tells us how much information PC1 and PC2 capture (e.g., 35% and 20%). Outliers outside the ellipse may indicate unusual samples, like measurement errors or unique biological profiles.\nBut PCA isn’t perfect. Let’s reflect on its limitations before moving forward.\n\n\nExercise 2\nWhy might PCA sometimes hide important biological information?\n\n\n💡 Hint\n\nPCA is optimised for variance, not necessarily for biological relevance. Important differences (e.g., between healthy and diseased) might not be the biggest source of variance!\n\n\n\n\nLearning Points\n\nRaw Data Messiness: The heatmap and scatter plots showed high dimensionality and redundancy, making PCA essential.\nPCA’s Power: PCA simplifies 200 metabolites into 2D, revealing patterns and outliers.\nPreprocessing Matters: Standardization ensures fair metabolite comparisons.\nOutlier Detection: Hotelling’s T² flags anomalies for further investigation.\n\nReady to explore more of your data’s hidden treasures? Let’s keep going! 🧑‍🔬",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>🧪 Multivariate Analysis: Concepts and Applications</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.4_metabolomics.html#step-5-interpreting-the-pca-results",
    "href": "notebooks/10_mini_projects/10.4_metabolomics.html#step-5-interpreting-the-pca-results",
    "title": "🧪 Multivariate Analysis: Concepts and Applications",
    "section": "Step 5: Interpreting the PCA Results 🧩",
    "text": "Step 5: Interpreting the PCA Results 🧩\nWow, look at that plot! Compared to the raw data’s chaos, PCA reveals clear groupings and outliers. The explained variance ratio tells us how much information PC1 and PC2 capture (e.g., 35% and 20%). Outliers outside the ellipse may indicate unusual samples, like measurement errors or unique biological profiles.\nBut PCA isn’t perfect. Let’s reflect on its limitations before moving forward.\n\n\nExercise 2\nWhy might PCA sometimes hide important biological information?\n\n\n💡 Hint\n\nPCA is optimised for variance, not necessarily for biological relevance. Important differences (e.g., between healthy and diseased) might not be the biggest source of variance!\n\n\n\n\nLearning Points\n\nRaw Data Messiness: The heatmap and scatter plots showed high dimensionality and redundancy, making PCA essential.\nPCA’s Power: PCA simplifies 200 metabolites into 2D, revealing patterns and outliers.\nPreprocessing Matters: Standardization ensures fair metabolite comparisons.\nOutlier Detection: Hotelling’s T² flags anomalies for further investigation.\n\nReady to explore more of your data’s hidden treasures? Let’s keep going! 🧑‍🔬",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>🧪 Multivariate Analysis: Concepts and Applications</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.4_metabolomics.html#partial-least-squares-discriminant-analysis-pls-da-the-supervised-classifier",
    "href": "notebooks/10_mini_projects/10.4_metabolomics.html#partial-least-squares-discriminant-analysis-pls-da-the-supervised-classifier",
    "title": "🧪 Multivariate Analysis: Concepts and Applications",
    "section": "3. Partial Least Squares Discriminant Analysis (PLS-DA): The Supervised Classifier 🏷️",
    "text": "3. Partial Least Squares Discriminant Analysis (PLS-DA): The Supervised Classifier 🏷️\nPLS-DA is like a guided missile 🎯—it’s supervised, meaning it uses class labels (e.g., “healthy” vs. “diseased”) to find components that maximize both variance and group separation. Perfect for classifying metabolomics samples!\n\n3.1 PLS-DA in Action\nLet’s add class labels to our synthetic dataset and apply PLS-DA.\n\n\nX = df.filter(like='Metabolite_')\ny = df['Label']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11088)\nplsda = PLSRegression(n_components=2)\nplsda.fit(X_train, y_train)\nscores = plsda.transform(X)\nplt.scatter(scores[:, 0], scores[:, 1], c=df['Label'], cmap='viridis')\nplt.xlabel('PLS Component 1')\nplt.ylabel('PLS Component 2')\nplt.title('PLS-DA of Metabolomics Data')\n\nExplanation: - PLSRegression: Used for PLS-DA by treating class labels as continuous (threshold at 0.5 for binary classification). - train_test_split: Splits data to evaluate model performance. - Scores Plot: Shows how well PLS-DA separates classes.\nExercise 3: Change n_components to 3 in the PLS-DA model. Does the accuracy improve? Why or why not?\n\n\n💡 Hint\n\nMore components capture more variance but may lead to overfitting, especially with small datasets. Check the accuracy and consider the trade-off!\n\nLearn More: Explore PLS-DA in metabolomics for real-world applications! 🧬",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>🧪 Multivariate Analysis: Concepts and Applications</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.4_metabolomics.html#bayesian-multivariate-models-embracing-uncertainty",
    "href": "notebooks/10_mini_projects/10.4_metabolomics.html#bayesian-multivariate-models-embracing-uncertainty",
    "title": "🧪 Multivariate Analysis: Concepts and Applications",
    "section": "4. Bayesian Multivariate Models: Embracing Uncertainty 🌈",
    "text": "4. Bayesian Multivariate Models: Embracing Uncertainty 🌈\nBayesian methods are like a crystal ball 🔮—they model uncertainty and let us build flexible multivariate models. In metabolomics, Bayesian approaches can handle missing data, model latent variables, or perform regression.\n\n4.1 Bayesian PCA with PyMC\nLet’s use PyMC to implement a simple Bayesian PCA model. This assumes metabolites are generated from latent PCs with Gaussian noise.\nPlease be patient.\n\n\nX = df.filter(like='Metabolite_').values\nwith pm.Model() as bayes_pca:\n    z = pm.Normal('z', mu=0, sigma=1, shape=(1000, 2))  # Latent PCs\n    w = pm.Normal('w', mu=0, sigma=1, shape=(200, 2))  # Loadings\n    mu = pm.math.dot(z, w.T)\n    X_obs = pm.Normal('X', mu=mu, sigma=0.1, observed=X)\n    trace = pm.sample(500, return_inferencedata=True)\naz.plot_posterior(trace, var_names=['w'], coords={'w_dim_0': [0]})\n\nExplanation: - z: Latent PCs for each sample. - w: Loadings (how metabolites contribute to PCs). - X: Observed data modeled as a linear combination of PCs plus noise. - pm.sample: Uses MCMC to estimate posterior distributions.\nExercise 4: Increase the number of samples (500 to 1000) in pm.sample. Does the posterior distribution change significantly? Why?\n\n\n💡 Solution\n\nMore samples improve the precision of the posterior but may not change the mean estimates much if the model has converged. Check the plot for tighter distributions!\n\nLearn More: Dive into PyMC’s documentation for more Bayesian modeling ideas! 🧠",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>🧪 Multivariate Analysis: Concepts and Applications</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.4_metabolomics.html#machine-learning-a-quick-dip-into-random-forests",
    "href": "notebooks/10_mini_projects/10.4_metabolomics.html#machine-learning-a-quick-dip-into-random-forests",
    "title": "🧪 Multivariate Analysis: Concepts and Applications",
    "section": "5. Machine Learning: A Quick Dip into Random Forests 🌳",
    "text": "5. Machine Learning: A Quick Dip into Random Forests 🌳\nMachine learning (ML) is like a superpower for metabolomics—models like Random Forests can classify samples or identify important metabolites (potential biomarkers).\n\n5.1 Random Forest Classifier\nLet’s use a Random Forest to classify our samples and find important metabolites.\n\nX = df.filter(like='Metabolite_')\ny = df['Label']\nrf = RandomForestClassifier(n_estimators=100, random_state=11088)\nrf.fit(X, y)\nimportance = rf.feature_importances_\nplt.bar(range(10), importance[:10], tick_label=X.columns[:10])\nplt.xticks(rotation=90, ha='right')\nplt.title('Top 10 Metabolite Importances')\nplt.xlabel('Metabolite')\nplt.ylabel('Importance')\n\nExplanation: - RandomForestClassifier: Builds multiple decision trees and aggregates their predictions. - feature_importances_: Shows which metabolites contribute most to classification (potential biomarkers!).\nExercise 5: Increase n_estimators to 200. Does the accuracy improve? Plot the feature importances again—are the top metabolites the same?\n\n\n💡 Hint\n\nMore trees reduce variance but may not change feature rankings much if the model is stable. Compare the plots visually!\n\nLearn More: Check out Random Forests in scikit-learn for more ML fun! 🚀",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>🧪 Multivariate Analysis: Concepts and Applications</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.4_metabolomics.html#using-principal-components-in-regression-biomarker-detection",
    "href": "notebooks/10_mini_projects/10.4_metabolomics.html#using-principal-components-in-regression-biomarker-detection",
    "title": "🧪 Multivariate Analysis: Concepts and Applications",
    "section": "6. Using Principal Components in Regression: Biomarker Detection 🔍",
    "text": "6. Using Principal Components in Regression: Biomarker Detection 🔍\nNow, let’s use PCA scores as predictors in a regression model to predict a continuous outcome (e.g., disease severity). This combines dimensionality reduction with predictive modeling.\n\n6.1 PCA + Linear Regression\nWe’ll use the PCA scores from Section 2 and regress them against a synthetic outcome.\n\n\nX = df.filter(like='Metabolite_')\ny = df['Severity']\nX_scaled = StandardScaler().fit_transform(X)\npca = PCA(n_components=2)\npca_result = pca.fit_transform(X_scaled)\nlr = LinearRegression()\nlr.fit(pca_result, y)\ny_pred = lr.predict(pca_result)\nplt.scatter(y, y_pred, c='purple', alpha=0.7)\nplt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\nplt.xlabel('True Severity')\nplt.ylabel('Predicted Severity')\nplt.title('PCA + Linear Regression')\nplt.savefig('pca_regression.png')\nplt.close()\n\nExplanation: - pca_result: PCA scores (PCs) used as predictors. - LinearRegression: Models the relationship between PCs and the outcome. - Scatter Plot: Shows how well predictions match the true outcome.\nExercise 6: Use only PC1 (pca_result[:, 0].reshape(-1, 1)) in the regression. Does the model perform worse? Why?\n\n\n💡 Solution\n\nUsing only PC1 reduces the information available to the model, likely worsening performance unless PC1 captures most of the relevant variation. Compare the scatter plots!",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>🧪 Multivariate Analysis: Concepts and Applications</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.4_metabolomics.html#summary-your-metabolomics-toolkit",
    "href": "notebooks/10_mini_projects/10.4_metabolomics.html#summary-your-metabolomics-toolkit",
    "title": "🧪 Multivariate Analysis: Concepts and Applications",
    "section": "7. Summary: Your Metabolomics Toolkit 🧰",
    "text": "7. Summary: Your Metabolomics Toolkit 🧰\nHere’s what you’ve learned:\n\nPCA 🗺️: Unsupervised, reduces dimensionality, explores data structure.\nPLS-DA 🏷️: Supervised, classifies samples, maximizes group separation.\nBayesian Models 🔮: Handle uncertainty, flexible for complex problems.\nRandom Forests 🌳: ML for classification and biomarker detection.\nPCA + Regression 🔍: Uses PCs for predictive modeling (e.g., disease severity).\n\nFinal Exercise: Pick a real metabolomics dataset (e.g., from MetaboLights) and apply one of these methods. Share your findings in a short paragraph!\nWhat’s Next? Try advanced methods like t-SNE, SVMs, or deep learning for metabolomics. Keep exploring, and happy analyzing! 😄",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>🧪 Multivariate Analysis: Concepts and Applications</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.5_machine_learning.html",
    "href": "notebooks/10_mini_projects/10.5_machine_learning.html",
    "title": "🌟 Machine Learning for Nutrition & Food Science: Cooking Up Insights! 🥗",
    "section": "",
    "text": "1. Introduction to Machine Learning in Nutrition & Food Science 📊\nWelcome to this tasty Jupyter Notebook on machine learning (ML) for nutrition and food science! Whether you’re snacking at home 🍎 or stirring ideas in a classroom, this guide will take you on a flavorful journey through ML techniques to analyze nutrition and food data. We’ll explore classification (e.g., healthy vs. unhealthy diets), regression (e.g., predicting nutrient content), feature selection (e.g., identifying key nutrients), and model evaluation! 🍴\nExpect hands-on code, fun exercises, and hidden treats (click the “Details” to reveal them)! Let’s dig in! 🚀\nNutrition and food science data are like a recipe book 📖—full of ingredients (nutrients), dishes (diets), and quality checks (food safety). Machine learning helps us:\nWe’ll use Python with scikit-learn, pandas, and matplotlib to build ML models. No culinary degree needed—just curiosity! 😄\nExercise 1: Why is ML useful for nutrition and food science compared to traditional statistical methods? Jot down your thoughts (no code needed).\nLet’s start with loading the necessary libraries:\n# Setup for Google Colab: Fetch datasets automatically or manually\n%run ../../bootstrap.py    # installs requirements + editable package\n\nimport fns_toolkit as fns\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_selection import RFE",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>🌟 Machine Learning for Nutrition & Food Science: Cooking Up Insights! 🥗</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.5_machine_learning.html#introduction-to-machine-learning-in-nutrition-food-science",
    "href": "notebooks/10_mini_projects/10.5_machine_learning.html#introduction-to-machine-learning-in-nutrition-food-science",
    "title": "🌟 Machine Learning for Nutrition & Food Science: Cooking Up Insights! 🥗",
    "section": "",
    "text": "Classify diets or foods (e.g., healthy vs. unhealthy).\nPredict nutritional values (e.g., calories or protein content).\nIdentify key factors (e.g., nutrients driving health outcomes).\n\n\n\n\n\n💡 Hint\n\nConsider how ML handles complex datasets, non-linear relationships, and interactions between nutrients or food components.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>🌟 Machine Learning for Nutrition & Food Science: Cooking Up Insights! 🥗</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.5_machine_learning.html#logistic-regression-classifying-diets",
    "href": "notebooks/10_mini_projects/10.5_machine_learning.html#logistic-regression-classifying-diets",
    "title": "🌟 Machine Learning for Nutrition & Food Science: Cooking Up Insights! 🥗",
    "section": "2. Logistic Regression: Classifying Diets 🍎",
    "text": "2. Logistic Regression: Classifying Diets 🍎\nLogistic regression is a classic ML method for binary classification (e.g., healthy vs. unhealthy diets). It’s simple yet powerful for nutrition data!\n\n2.1 Logistic Regression in Action\nLet’s create a synthetic nutrition dataset (e.g., nutrient profiles) and classify diets as healthy or unhealthy.\n\n\n\n# Generate synthetic nutrition dataset (60 samples, 10 nutrients/features)\nnp.random.seed(11088)\ndata = pd.DataFrame({\n    'Calories': np.random.normal(500, 100, 60),\n    'Protein_g': np.random.normal(30, 5, 60),\n    'Carbs_g': np.random.normal(60, 10, 60),\n    'Fat_g': np.random.normal(20, 5, 60),\n    'Fiber_g': np.random.normal(10, 2, 60),\n    'Sugar_g': np.random.normal(15, 5, 60),\n    'Sodium_mg': np.random.normal(800, 200, 60),\n    'Vitamin_C_mg': np.random.normal(50, 10, 60),\n    'Calcium_mg': np.random.normal(300, 50, 60),\n    'Iron_mg': np.random.normal(5, 1, 60)\n})\nlabels = np.random.choice([0, 1], size=60)  # 0 = unhealthy, 1 = healthy\n\n# Standardize the data\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data)\n\n# Split into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(data_scaled, labels, test_size=0.3, random_state=42)\n\n# Train logistic regression\nlog_reg = LogisticRegression(random_state=42)\nlog_reg.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = log_reg.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Logistic Regression Accuracy: {accuracy:.2f} 🎉')\n\n# Plot confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\nplt.title('Confusion Matrix for Diet Classification 🥗')\nplt.xlabel('Predicted')\nplt.ylabel('True')\n\nExplanation: - StandardScaler: Ensures nutrients (e.g., calories, protein) are on the same scale. - LogisticRegression: Predicts diet class (healthy/unhealthy) using nutrient profiles. - Confusion Matrix: Shows true positives, false positives, etc.\nExercise 2: Add a penalty parameter (C=0.1) to the LogisticRegression model to increase regularization. Does the accuracy change? Why?\n\n\n💡 Solution\n\nChange the model line to:\nlog_reg = LogisticRegression(C=0.1, random_state=42)\nSmaller C increases regularization, reducing overfitting but potentially lowering accuracy if the model is too constrained.\n\nLearn More: Check out scikit-learn’s Logistic Regression for more details! 📚",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>🌟 Machine Learning for Nutrition & Food Science: Cooking Up Insights! 🥗</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.5_machine_learning.html#random-forest-identifying-key-nutrients",
    "href": "notebooks/10_mini_projects/10.5_machine_learning.html#random-forest-identifying-key-nutrients",
    "title": "🌟 Machine Learning for Nutrition & Food Science: Cooking Up Insights! 🥗",
    "section": "3. Random Forest: Identifying Key Nutrients 🌳",
    "text": "3. Random Forest: Identifying Key Nutrients 🌳\nRandom Forest is like a team of decision trees that classifies diets or ranks nutrients by importance. It’s great for pinpointing key dietary factors!\n\n3.1 Random Forest in Action\nLet’s use a Random Forest to classify diets and identify important nutrients.\n\n\n\n# Train Random Forest\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred_rf = rf.predict(X_test)\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\nprint(f'Random Forest Accuracy: {accuracy_rf:.2f} 🌟')\n\n# Feature importance (important nutrients)\nimportance = rf.feature_importances_\nplt.figure(figsize=(10, 6))\nplt.bar(data.columns, importance, color='green', alpha=0.7)\nplt.title('Nutrient Importance for Diet Classification 🥕')\nplt.xlabel('Nutrient')\nplt.ylabel('Importance')\nplt.xticks(rotation=45)\nplt.savefig('rf_importance.png')\n\nExplanation: - RandomForestClassifier: Combines multiple decision trees for robust predictions. - feature_importances_: Ranks nutrients by their contribution to diet classification.\nExercise 3: Increase n_estimators to 200. Does the accuracy improve? Are the top nutrients the same?\n\n\n💡 Hint\n\nMore trees reduce variance but may not significantly change feature rankings. Compare the accuracy and plot!\n\nLearn More: Explore Random Forests in scikit-learn for more ML fun! 🚀",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>🌟 Machine Learning for Nutrition & Food Science: Cooking Up Insights! 🥗</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.5_machine_learning.html#support-vector-machines-svm-classifying-food-quality",
    "href": "notebooks/10_mini_projects/10.5_machine_learning.html#support-vector-machines-svm-classifying-food-quality",
    "title": "🌟 Machine Learning for Nutrition & Food Science: Cooking Up Insights! 🥗",
    "section": "4. Support Vector Machines (SVM): Classifying Food Quality ⚔️",
    "text": "4. Support Vector Machines (SVM): Classifying Food Quality ⚔️\nSVMs find the best boundary to separate classes, ideal for tasks like classifying food quality (e.g., fresh vs. spoiled) based on nutritional or sensory data.\n\n4.1 SVM in Action\nLet’s classify foods as fresh or spoiled using an SVM with a linear kernel.\n\n\n\n# Train SVM (using same dataset, now for food quality)\nsvm = SVC(kernel='linear', random_state=42)\nsvm.fit(X_train, y_train)  # Labels: 0 = spoiled, 1 = fresh\n\n# Predict and evaluate\ny_pred_svm = svm.predict(X_test)\naccuracy_svm = accuracy_score(y_test, y_pred_svm)\nprint(f'SVM Accuracy: {accuracy_svm:.2f} 🛡️')\n\n# Plot confusion matrix\ncm_svm = confusion_matrix(y_test, y_pred_svm)\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm_svm, annot=True, fmt='d', cmap='Greens', cbar=False)\nplt.title('Confusion Matrix for Food Quality Classification 🍊')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.savefig('svm_cm.png')\n\nExplanation: - SVC: Support Vector Classifier with a linear kernel. - Hyperplane: Maximizes the margin between fresh and spoiled classes.\nExercise 4: Try an RBF kernel (kernel='rbf') in the SVM. Does the accuracy improve? Why might this happen?\n\n\n💡 Solution\n\nChange the SVM line to:\nsvm = SVC(kernel='rbf', random_state=42)\nThe RBF kernel captures non-linear patterns, which may improve accuracy if the data has complex relationships.\n\nLearn More: Dive into SVMs in scikit-learn for more insights! 🧠",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>🌟 Machine Learning for Nutrition & Food Science: Cooking Up Insights! 🥗</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.5_machine_learning.html#regression-with-gradient-boosting-predicting-nutrient-content",
    "href": "notebooks/10_mini_projects/10.5_machine_learning.html#regression-with-gradient-boosting-predicting-nutrient-content",
    "title": "🌟 Machine Learning for Nutrition & Food Science: Cooking Up Insights! 🥗",
    "section": "5. Regression with Gradient Boosting: Predicting Nutrient Content 📈",
    "text": "5. Regression with Gradient Boosting: Predicting Nutrient Content 📈\nSometimes we need to predict continuous outcomes, like the calorie content of a meal. Gradient Boosting is a powerful ML method for regression tasks.\n\n5.1 Gradient Boosting in Action\nLet’s predict calorie content using Gradient Boosting.\n\n\n\n# Generate synthetic calorie outcome\noutcome = data['Calories'] + np.random.normal(0, 20, 60)  # Correlated with calories\n\n# Use other nutrients as features\nfeatures = data.drop(columns=['Calories'])\nfeatures_scaled = scaler.fit_transform(features)\n\n# Split data\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(features_scaled, outcome, test_size=0.3, random_state=42)\n\n# Train Gradient Boosting\ngbr = GradientBoostingRegressor(n_estimators=100, random_state=42)\ngbr.fit(X_train_reg, y_train_reg)\n\n# Predict and evaluate\ny_pred_gbr = gbr.predict(X_test_reg)\nmse = mean_squared_error(y_test_reg, y_pred_gbr)\nprint(f'Gradient Boosting MSE: {mse:.2f} 📉')\n\n# Plot predictions vs true values\nplt.figure(figsize=(8, 6))\nplt.scatter(y_test_reg, y_pred_gbr, c='purple', alpha=0.7)\nplt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'r--')\nplt.xlabel('True Calories (kcal)')\nplt.ylabel('Predicted Calories (kcal)')\nplt.title('Gradient Boosting Predictions for Calories 🥐')\nplt.grid(True)\nplt.savefig('gbr_predictions.png')\n\nExplanation: - GradientBoostingRegressor: Builds trees sequentially, correcting errors of previous trees. - MSE: Measures prediction error (lower is better).\nExercise 5: Increase n_estimators to 200 in the Gradient Boosting model. Does the MSE decrease? Why?\n\n\n💡 Hint\n\nMore trees improve fit but risk overfitting. Check if the MSE stabilizes or worsens!\n\nLearn More: Explore Gradient Boosting in scikit-learn for more details! 🌟",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>🌟 Machine Learning for Nutrition & Food Science: Cooking Up Insights! 🥗</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.5_machine_learning.html#feature-selection-finding-key-nutritional-factors",
    "href": "notebooks/10_mini_projects/10.5_machine_learning.html#feature-selection-finding-key-nutritional-factors",
    "title": "🌟 Machine Learning for Nutrition & Food Science: Cooking Up Insights! 🥗",
    "section": "6. Feature Selection: Finding Key Nutritional Factors ⭐",
    "text": "6. Feature Selection: Finding Key Nutritional Factors ⭐\nNutrition datasets often have many features. Feature selection helps us pick the most important nutrients (e.g., for health outcomes) using Recursive Feature Elimination (RFE).\n\n6.1 RFE with Random Forest\nLet’s use RFE to select the top 5 nutrients for diet classification.\n\n\n# Apply RFE with Random Forest\nrfe = RFE(estimator=RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=5)\nrfe.fit(X_train, y_train)\n\n# Get selected features\nselected_features = data.columns[rfe.support_]\nprint('Top 5 Nutrients:', selected_features.tolist(), '🎯')\n\n# Train Random Forest on selected features\nX_train_selected = X_train[:, rfe.support_]\nX_test_selected = X_test[:, rfe.support_]\nrf_selected = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_selected.fit(X_train_selected, y_train)\ny_pred_selected = rf_selected.predict(X_test_selected)\naccuracy_selected = accuracy_score(y_test, y_pred_selected)\nprint(f'Accuracy with Selected Features: {accuracy_selected:.2f} 🚀')\n\nExplanation: - RFE: Recursively eliminates less important nutrients. - selected_features: The top 5 nutrients for diet classification.\nExercise 6: Change n_features_to_select to 3. Does the accuracy drop? Why might this happen?\n\n\n💡 Solution\n\nFewer features may reduce model performance if critical information is lost. Compare the accuracy and consider the trade-off!\n\nLearn More: Check out Feature Selection in scikit-learn for more techniques! 🧠",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>🌟 Machine Learning for Nutrition & Food Science: Cooking Up Insights! 🥗</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.5_machine_learning.html#summary-your-ml-toolkit-for-nutrition-food-science",
    "href": "notebooks/10_mini_projects/10.5_machine_learning.html#summary-your-ml-toolkit-for-nutrition-food-science",
    "title": "🌟 Machine Learning for Nutrition & Food Science: Cooking Up Insights! 🥗",
    "section": "7. Summary: Your ML Toolkit for Nutrition & Food Science 🧰",
    "text": "7. Summary: Your ML Toolkit for Nutrition & Food Science 🧰\nHere’s what you’ve cooked up:\n\nLogistic Regression 📊: Simple classification of diets.\nRandom Forest 🌳: Robust classification and nutrient ranking.\nSVM ⚔️: Powerful for food quality classification.\nGradient Boosting 📈: Accurate prediction of nutrient content.\nFeature Selection ⭐: Identifies key nutritional factors.\n\nFinal Exercise: Download a real nutrition dataset (e.g., from USDA FoodData Central) and apply one of these ML methods. Write a short paragraph summarizing your results!\nWhat’s Next? Try advanced ML techniques like neural networks or combine ML with PCA for nutrition data. Keep exploring, and happy analyzing! 😄",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>🌟 Machine Learning for Nutrition & Food Science: Cooking Up Insights! 🥗</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.6_ai.html",
    "href": "notebooks/10_mini_projects/10.6_ai.html",
    "title": "🌟 Artificial Intelligence for Nutrition & Food Science: Serving Up the Future! 🤖",
    "section": "",
    "text": "1. Introduction to AI in Nutrition & Food Science 🥗\nWelcome to this delicious Jupyter Notebook on Artificial Intelligence (AI) for nutrition and food science! Whether you’re munching at home 🍴 or cooking up ideas in a classroom, this guide will whisk you through the exciting world of AI, with a focus on nutrition and food science. We’ll dive into neural networks, deep learning, and their superpowers for classifying diets, predicting nutrient content, and analyzing food quality! 🥕\nGet ready for code, exercises, and hidden surprises (click the “Details” to uncover them)! Let’s blast off into the AI kitchen! 🚀\nNutrition and food science data are like a recipe book 📖—packed with nutrients, diets, and quality metrics. AI, especially neural networks and deep learning, helps us:\nWe’ll use Python with tensorflow, keras, scikit-learn, and pandas to build AI models. No chef’s hat required—just enthusiasm! 😄\nExercise 1: Why might AI be more powerful than traditional machine learning for nutrition and food science? Write your thoughts (no code needed).\nLet’s start with loading the required libraries:\n# Setup for Google Colab: Fetch datasets automatically or manually\n%run ../../bootstrap.py    # installs requirements + editable package\n\nimport fns_toolkit as fns\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>🌟 Artificial Intelligence for Nutrition & Food Science: Serving Up the Future! 🤖</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.6_ai.html#introduction-to-ai-in-nutrition-food-science",
    "href": "notebooks/10_mini_projects/10.6_ai.html#introduction-to-ai-in-nutrition-food-science",
    "title": "🌟 Artificial Intelligence for Nutrition & Food Science: Serving Up the Future! 🤖",
    "section": "",
    "text": "Classify diets (e.g., healthy vs. unhealthy).\nPredict nutritional values (e.g., calorie or protein content).\nAnalyze food quality (e.g., fresh vs. spoiled).\n\n\n\n\n\n💡 Hint\n\nThink about how AI can learn complex patterns, handle high-dimensional data, and model intricate nutritional interactions.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>🌟 Artificial Intelligence for Nutrition & Food Science: Serving Up the Future! 🤖</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.6_ai.html#neural-networks-the-brain-of-ai",
    "href": "notebooks/10_mini_projects/10.6_ai.html#neural-networks-the-brain-of-ai",
    "title": "🌟 Artificial Intelligence for Nutrition & Food Science: Serving Up the Future! 🤖",
    "section": "2. Neural Networks: The Brain of AI 🧠",
    "text": "2. Neural Networks: The Brain of AI 🧠\nNeural networks mimic the human brain, with layers of neurons that learn patterns from data. In nutrition, they’re great for classifying diets!\n\n2.1 A Simple Neural Network\nLet’s build a basic neural network to classify synthetic dietary profiles as healthy or unhealthy using tensorflow and keras.\n\n\n\n# Generate synthetic nutrition dataset (60 samples, 10 nutrients/features)\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'Calories': np.random.normal(500, 100, 60),\n    'Protein_g': np.random.normal(30, 5, 60),\n    'Carbs_g': np.random.normal(60, 10, 60),\n    'Fat_g': np.random.normal(20, 5, 60),\n    'Fiber_g': np.random.normal(10, 2, 60),\n    'Sugar_g': np.random.normal(15, 5, 60),\n    'Sodium_mg': np.random.normal(800, 200, 60),\n    'Vitamin_C_mg': np.random.normal(50, 10, 60),\n    'Calcium_mg': np.random.normal(300, 50, 60),\n    'Iron_mg': np.random.normal(5, 1, 60)\n})\nlabels = np.random.choice([0, 1], size=60)  # 0 = unhealthy, 1 = healthy\n\n# Standardize the data\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data)\n\n# Split into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(data_scaled, labels, test_size=0.3, random_state=42)\n\n# Build neural network\nmodel = Sequential([\n    Dense(64, activation='relu', input_shape=(10,)),  # Hidden layer with 64 neurons\n    Dense(32, activation='relu'),                    # Second hidden layer\n    Dense(1, activation='sigmoid')                   # Output layer for binary classification\n])\n\n# Compile model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train model\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=0)\n\n# Evaluate model\ny_pred = (model.predict(X_test) &gt; 0.5).astype(int).flatten()\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Neural Network Accuracy: {accuracy:.2f} 🎉')\n\n# Plot confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\nplt.title('Confusion Matrix for Diet Classification 🥗')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.savefig('nn_cm.png')\n\nExplanation: - Sequential: A stack of layers (input, hidden, output). - Dense: Fully connected layers; relu for hidden, sigmoid for binary output. - adam: Optimizer for efficient training. - binary_crossentropy: Loss function for binary classification.\nExercise 2: Add another hidden layer with 16 neurons (Dense(16, activation='relu')) before the output layer. Does the accuracy improve? Why?\n\n\n💡 Solution\n\nModify the model to:\nmodel = Sequential([\n    Dense(64, activation='relu', input_shape=(10,)),\n    Dense(32, activation='relu'),\n    Dense(16, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\nA deeper network may capture more complex patterns but risks overfitting on small datasets.\n\nLearn More: Check out Keras Documentation for neural network tips! 📚",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>🌟 Artificial Intelligence for Nutrition & Food Science: Serving Up the Future! 🤖</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.6_ai.html#deep-learning-going-deeper-with-cnns-for-food-quality",
    "href": "notebooks/10_mini_projects/10.6_ai.html#deep-learning-going-deeper-with-cnns-for-food-quality",
    "title": "🌟 Artificial Intelligence for Nutrition & Food Science: Serving Up the Future! 🤖",
    "section": "3. Deep Learning: Going Deeper with CNNs for Food Quality 🌐",
    "text": "3. Deep Learning: Going Deeper with CNNs for Food Quality 🌐\nConvolutional Neural Networks (CNNs) are great for image-based tasks, but we can use 1D CNNs to analyze nutritional profiles as “signals” for tasks like food quality classification.\n\n3.1 1D CNN for Food Quality\nLet’s build a 1D CNN to classify foods as fresh or spoiled based on nutrient profiles.\n\n\n# Reshape data for 1D CNN (samples, nutrients, 1 channel)\nX_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\nX_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n\n# Build 1D CNN\ncnn_model = Sequential([\n    Conv1D(32, kernel_size=3, activation='relu', input_shape=(10, 1)),  # 1D convolution\n    MaxPooling1D(pool_size=2),                                         # Downsampling\n    Conv1D(16, kernel_size=3, activation='relu'),                      # Second conv layer\n    MaxPooling1D(pool_size=2),\n    Flatten(),                                                         # Flatten for dense layers\n    Dense(32, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\n# Compile model\ncnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train model\ncnn_history = cnn_model.fit(X_train_cnn, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=0)\n\n# Evaluate model\ny_pred_cnn = (cnn_model.predict(X_test_cnn) &gt; 0.5).astype(int).flatten()\naccuracy_cnn = accuracy_score(y_test, y_pred_cnn)\nprint(f'1D CNN Accuracy: {accuracy_cnn:.2f} 🌟')\n\n# Plot training history\nplt.figure(figsize=(8, 6))\nplt.plot(cnn_history.history['accuracy'], label='Training Accuracy')\nplt.plot(cnn_history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('1D CNN Training Progress for Food Quality 📈')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\nplt.savefig('cnn_history.png')\n\nExplanation: - Conv1D: Extracts local patterns in nutrient profiles. - MaxPooling1D: Reduces dimensionality while keeping key features. - Flatten: Prepares data for dense layers.\nExercise 3: Increase the kernel_size to 5 in the first Conv1D layer. Does the accuracy change? Why?\n\n\n💡 Hint\n\nA larger kernel captures broader patterns but may miss fine details. Check if the model generalizes better or overfits!\n\nLearn More: Explore CNNs in TensorFlow for more ideas! 🚀",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>🌟 Artificial Intelligence for Nutrition & Food Science: Serving Up the Future! 🤖</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.6_ai.html#autoencoders-unsupervised-feature-learning",
    "href": "notebooks/10_mini_projects/10.6_ai.html#autoencoders-unsupervised-feature-learning",
    "title": "🌟 Artificial Intelligence for Nutrition & Food Science: Serving Up the Future! 🤖",
    "section": "4. Autoencoders: Unsupervised Feature Learning 🕵️‍♂️",
    "text": "4. Autoencoders: Unsupervised Feature Learning 🕵️‍♂️\nAutoencoders are neural networks that learn compressed representations of data (like PCA but non-linear). In nutrition, they’re useful for dimensionality reduction or identifying dietary patterns.\n\n4.1 Autoencoder for Nutrition Data\nLet’s build an autoencoder to compress our nutritional profiles.\n\n\n\n# Build autoencoder\ninput_layer = Input(shape=(10,))\nencoded = Dense(32, activation='relu')(input_layer)  # Encoder\nencoded = Dense(16, activation='relu')(encoded)      # Bottleneck\ndecoded = Dense(32, activation='relu')(encoded)      # Decoder\ndecoded = Dense(10, activation='linear')(decoded)    # Reconstruction\n\nautoencoder = Model(input_layer, decoded)\nencoder = Model(input_layer, encoded)  # For extracting compressed features\n\n# Compile and train\nautoencoder.compile(optimizer='adam', loss='mse')\nautoencoder.fit(data_scaled, data_scaled, epochs=50, batch_size=16, validation_split=0.2, verbose=0)\n\n# Get compressed features\ncompressed_features = encoder.predict(data_scaled)\n\n# Plot original vs. reconstructed data for first sample\nreconstructed = autoencoder.predict(data_scaled)\nplt.figure(figsize=(10, 6))\nplt.plot(data_scaled[0], label='Original')\nplt.plot(reconstructed[0], label='Reconstructed')\nplt.title('Autoencoder Reconstruction of Nutritional Profile 🥐')\nplt.xlabel('Nutrient Index')\nplt.ylabel('Value')\nplt.legend()\nplt.grid(True)\nplt.savefig('autoencoder_reconstruction.png')\n\nExplanation: - Encoder: Compresses nutritional data into a lower-dimensional space. - Decoder: Reconstructs the original data. - mse: Loss function to minimize reconstruction error.\nExercise 4: Reduce the bottleneck layer to 8 neurons (Dense(8, activation='relu')). Does the reconstruction quality change? Why?\n\n\n💡 Solution\n\nA smaller bottleneck forces more compression, which may lead to information loss and poorer reconstruction. Compare the plots!\n\nLearn More: Check out Autoencoders in Keras for more applications! 🧠",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>🌟 Artificial Intelligence for Nutrition & Food Science: Serving Up the Future! 🤖</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.6_ai.html#using-compressed-features-for-classification",
    "href": "notebooks/10_mini_projects/10.6_ai.html#using-compressed-features-for-classification",
    "title": "🌟 Artificial Intelligence for Nutrition & Food Science: Serving Up the Future! 🤖",
    "section": "5. Using Compressed Features for Classification 🌟",
    "text": "5. Using Compressed Features for Classification 🌟\nLet’s use the autoencoder’s compressed features as input to a neural network for diet classification, combining unsupervised and supervised learning.\n\n5.1 Classification with Compressed Features\nWe’ll train a neural network on the compressed features from the autoencoder.\n\n# Split compressed features\nX_train_comp, X_test_comp, y_train_comp, y_test_comp = train_test_split(compressed_features, labels, test_size=0.3, random_state=42)\n\n# Build classification model\ncomp_model = Sequential([\n    Dense(16, activation='relu', input_shape=(16,)),\n    Dense(8, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\n# Compile and train\ncomp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\ncomp_model.fit(X_train_comp, y_train_comp, epochs=50, batch_size=16, validation_split=0.2, verbose=0)\n\n# Evaluate\ny_pred_comp = (comp_model.predict(X_test_comp) &gt; 0.5).astype(int).flatten()\naccuracy_comp = accuracy_score(y_test_comp, y_pred_comp)\nprint(f'Accuracy with Compressed Features: {accuracy_comp:.2f} 🚀')\n\n# Plot confusion matrix\ncm_comp = confusion_matrix(y_test_comp, y_pred_comp)\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm_comp, annot=True, fmt='d', cmap='Purples', cbar=False)\nplt.title('Confusion Matrix for Compressed Features 🥗')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.savefig('comp_cm.png')\n\nExplanation: - compressed_features: Lower-dimensional representations from the autoencoder. - Sequential: A neural network for classification using these features.\nExercise 5: Compare the accuracy here to the neural network in Section 2. Why might compressed features perform differently?\n\n\n💡 Hint\n\nCompressed features reduce noise but may lose some information. Check if the trade-off improves or harms performance!",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>🌟 Artificial Intelligence for Nutrition & Food Science: Serving Up the Future! 🤖</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.6_ai.html#summary-your-ai-toolkit-for-nutrition-food-science",
    "href": "notebooks/10_mini_projects/10.6_ai.html#summary-your-ai-toolkit-for-nutrition-food-science",
    "title": "🌟 Artificial Intelligence for Nutrition & Food Science: Serving Up the Future! 🤖",
    "section": "6. Summary: Your AI Toolkit for Nutrition & Food Science 🧰",
    "text": "6. Summary: Your AI Toolkit for Nutrition & Food Science 🧰\nHere’s what you’ve whipped up:\n\nNeural Networks 🧠: Basic AI for diet classification.\n1D CNNs 🌐: Deep learning for nutritional profiles.\nAutoencoders 🕵️‍♂️: Unsupervised feature learning.\nCompressed Features 🌟: Combining unsupervised and supervised AI.\n\nFinal Exercise: Download a real nutrition dataset from USDA FoodData Central and apply one of these AI methods. Share your findings in a short paragraph!\nWhat’s Next? Explore advanced AI techniques like GANs, transfer learning, or graph neural networks for nutrition and food science. Keep dreaming big, and happy AI-ing! 😄",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>🌟 Artificial Intelligence for Nutrition & Food Science: Serving Up the Future! 🤖</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.7_llm.html",
    "href": "notebooks/10_mini_projects/10.7_llm.html",
    "title": "🌟 Using LLM APIs for Nutrition & Food Science: A Tasty AI Adventure! 🥑",
    "section": "",
    "text": "1. Introduction to LLM APIs in Nutrition & Food Science 📊\nWelcome to this scrumptious Jupyter Notebook on harnessing Large Language Model (LLM) APIs for nutrition and food science research! Whether you’re nibbling at home 🍎 or cooking up ideas in a classroom, this guide will whisk you through using APIs from LLMs like Grok, ChatGPT, Manus, and more to tackle tasks like parsing food diaries, reviewing literature, analyzing supply chains, and sensory analysis! 🍴\nWe’ll use Python with a free API (Hugging Face) and explore other LLM APIs, comparing their strengths and weaknesses. Expect code, exercises, and hidden treats (click the “Details” to reveal them)! Let’s dive into the AI kitchen! 🚀\nNutrition and food science are like a buffet of data 🍽️—food diaries, research papers, supply chain logs, and sensory descriptions. LLM APIs let us tap into powerful language models to:\nWe’ll use Python with requests, pandas, and a free Hugging Face API (plus others if you have access). No master chef skills needed—just curiosity! 😄\nExercise 1: Why might LLM APIs be ideal for parsing unstructured nutrition data (e.g., food diaries)? Jot down your thoughts (no code needed).\nLet’s load the required libraries first.\n# Setup for Google Colab: Fetch datasets automatically or manually\n%run ../../bootstrap.py    # installs requirements + editable package\n\nimport fns_toolkit as fns\n\n# Import libraries\n\n%pip install huggingface_hub\n%pip install transformers\n%pip install torch\n\nimport requests\nimport json\nimport huggingface_hub\nfrom datetime import datetime\n\nimport requests\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import f\nimport numpy.linalg as la\n\n\nimport requests\nimport torch\n\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n\nimport textwrap",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>🌟 Using LLM APIs for Nutrition & Food Science: A Tasty AI Adventure! 🥑</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.7_llm.html#introduction-to-llm-apis-in-nutrition-food-science",
    "href": "notebooks/10_mini_projects/10.7_llm.html#introduction-to-llm-apis-in-nutrition-food-science",
    "title": "🌟 Using LLM APIs for Nutrition & Food Science: A Tasty AI Adventure! 🥑",
    "section": "",
    "text": "Parse food diaries: Extract nutrients or dietary patterns.\nReview literature: Summarize nutrition studies.\nAnalyze supply chains: Optimize logistics or detect issues.\nPerform sensory analysis: Interpret taste and texture descriptions.\n\n\n\n\n\n💡 Hint\n\nConsider LLMs’ ability to understand context, handle varied text formats, and extract structured information from messy data.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>🌟 Using LLM APIs for Nutrition & Food Science: A Tasty AI Adventure! 🥑</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.7_llm.html#overview-of-llms-and-their-apis",
    "href": "notebooks/10_mini_projects/10.7_llm.html#overview-of-llms-and-their-apis",
    "title": "🌟 Using LLM APIs for Nutrition & Food Science: A Tasty AI Adventure! 🥑",
    "section": "2. Overview of LLMs and Their APIs 🧠",
    "text": "2. Overview of LLMs and Their APIs 🧠\nHere’s a rundown of popular LLMs, their APIs, and their strengths and weaknesses for nutrition and food science tasks. Note: All LLMs may exhibit biases from training data (e.g., cultural or dietary biases), which can affect applications like food diary parsing or sensory analysis.\n\n\n\n\n\n\n\n\n\n\n\nLLM\nProvider\nAPI Availability\nFree Tier?\nStrengths\nWeaknesses\n\n\n\n\nGrok\nxAI\nYes (Grok API)\nLimited (beta for X Premium+ users)\nReal-time X integration, witty responses, strong reasoning\nLimited free access, proprietary, less conversational depth\n\n\nChatGPT\nOpenAI\nYes (OpenAI API)\nNo (paid, starts ~$20/month)\nConversational fluency, broad NLP tasks (e.g., text summarization)\nNo free tier, lacks real-time data\n\n\nManus\nManus AI\nYes (limited beta)\nNo (proprietary, waitlist)\nEnterprise automation, multi-model architecture, deep reasoning\nSlow response times, less conversational, academic tone\n\n\nLlama\nMeta AI\nNo (open-source, not API-hosted)\nFree (local use)\nOpen-source, medical/nutrition applications\nHigh computational needs, no hosted API\n\n\nDeepSeek\nDeepSeek\nYes (open-source API)\nYes (free for public use)\nEfficient, open-source, structured problem-solving\nLess conversational, limited multimodal support\n\n\nHugging Face Models\nHugging Face\nYes (Inference API)\nYes (free tier)\nFree, diverse models (e.g., BART for summarization), customizable\nLimited free quota, less advanced than GPT-4\n\n\n\nWhy Hugging Face? We’ll use Hugging Face’s free Inference API for its accessibility and robust NLP capabilities, ideal for tasks like summarization and text analysis in nutrition research.\nExercise 2: Which LLM might be best for real-time supply chain analysis? Why? (No code needed).\n\n\n💡 Solution\n\nGrok’s real-time X integration makes it ideal for supply chain tasks needing current data (e.g., logistics updates).\n\nLearn More: Explore Hugging Face, OpenAI, or xAI’s API for details! 📚",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>🌟 Using LLM APIs for Nutrition & Food Science: A Tasty AI Adventure! 🥑</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.7_llm.html#parsing-food-diaries-with-hugging-face-api",
    "href": "notebooks/10_mini_projects/10.7_llm.html#parsing-food-diaries-with-hugging-face-api",
    "title": "🌟 Using LLM APIs for Nutrition & Food Science: A Tasty AI Adventure! 🥑",
    "section": "3. Parsing Food Diaries with Hugging Face API 📝",
    "text": "3. Parsing Food Diaries with Hugging Face API 📝\nFood diaries are unstructured text (e.g., “Ate oatmeal with berries and coffee”). LLMs can extract nutrients or dietary patterns. Let’s use Hugging Face’s free API to summarize a food diary entry.\n\n3.1 Summarizing a Food Diary\nWe’ll use the facebook/bart-large-cnn model to summarize a diary entry. Get a free API key from Hugging Face.\nNote: Replace YOUR_API_KEY with your Hugging Face API key.\n\n# Set up Hugging Face API\nAPI_URL = \"https://api-inference.huggingface.co/models/facebook/bart-large-cnn\"\n\nfrom dotenv import load_dotenv \nload_dotenv()\n\nHF_TOKEN = os.getenv(\"HF_TOKEN\")  # Ensure HF_TOKEN is set in your environment\nif not HF_TOKEN:\n    raise ValueError(\"Hugging Face API token not found. Set HF_TOKEN environment variable.\")\nheaders = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n\n\n# Sample food diary entry\ndiary_entry = \"\"\"\nBreakfast: Oatmeal with blueberries, almond milk, and a drizzle of honey. Coffee with a splash of cream.\nLunch: Grilled chicken salad with spinach, tomatoes, cucumber, and olive oil dressing. Sparkling water.\nDinner: Baked salmon, quinoa, steamed broccoli, and a glass of red wine.\nSnack: Greek yogurt with a handful of almonds.\n\"\"\"\n\n# Function to summarize text\ndef summarize_text(text, max_length=100, min_length=30):\n    payload = {\"inputs\": text, \"parameters\": {\"max_length\": max_length, \"min_length\": min_length}}\n    response = requests.post(API_URL, headers=headers, json=payload)\n    if response.status_code == 200:\n        return response.json()[0]['summary_text']\n    else:\n        return f\"Error: {response.status_code}\"\n\n# Summarize the diary\nsummary = summarize_text(diary_entry)\n\n# Wrap summary to avoid long lines\nwrapper = textwrap.TextWrapper(width=80, subsequent_indent=\"  \")  # Wrap at 80 characters\nwrapped_summary = wrapper.fill(f\"- {summary}\")\n\n# Format the output for readability\noriginal_length = len(diary_entry.strip())\nsummary_length = len(summary.strip())\n\n# Create structured output\noutput = f\"\"\"\n{'=' * 36}\nFood Diary Summary\n{'=' * 36}\nModel: facebook/bart-large-cnn\nOriginal Length: {original_length} characters\nSummary Length: {summary_length} characters\n\nSummary:\n- {wrapped_summary}\n\n🥣\n{'=' * 36}\n\"\"\"\n\n# Print formatted output\nprint(output)\n\nExplanation: - Hugging Face API: Uses facebook/bart-large-cnn for summarization, ideal for condensing food diary text. - requests.post: Sends the diary entry to the API and retrieves the summary. - payload: Controls summary length for concise output.\nExercise 3: Modify max_length to 50 in summarize_text. Is the summary more concise? Compare outputs.\n\n\n💡 Solution\n\nChange the function call to:\nsummary = summarize_text(diary_entry, max_length=50)\nA shorter max_length produces a more concise summary, potentially omitting details like specific foods.\n\nLearn More: Try Hugging Face’s NER models to extract specific nutrients from diaries! 🚀\n\n# Set up NER model for food entity extraction\nner_model = \"Dizex/InstaFoodRoBERTa-NER\"\ntokenizer = AutoTokenizer.from_pretrained(ner_model)\nmodel = AutoModelForTokenClassification.from_pretrained(ner_model)\nner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n\n# Sample food diary entry\ndiary_entry = \"\"\"\nBreakfast: Oatmeal with blueberries, almond milk, and a drizzle of honey. Coffee with a splash of cream.\nLunch: Grilled chicken salad with spinach, tomatoes, cucumber, and olive oil dressing. Sparkling water.\nDinner: Baked salmon, quinoa, steamed broccoli, and a glass of red wine.\nSnack: Greek yogurt with a handful of almonds.\n\"\"\"\n\n# Function to summarize text using Hugging Face Inference API\ndef summarize_text(text, max_length=80, min_length=30):\n    # Prepare payload with tightened parameters for shorter summary\n    payload = {\n        \"inputs\": text,\n        \"parameters\": {\n            \"max_length\": max_length,\n            \"min_length\": min_length,\n            \"truncation\": True\n        }\n    }\n    # Send POST request to the Inference API\n    response = requests.post(API_URL, headers=headers, json=payload)\n    # Check response status\n    if response.status_code == 200:\n        return response.json()[0]['summary_text']\n    else:\n        return f\"Error: {response.status_code} - {response.text}\"\n\n# Function to extract food entities using NER\ndef extract_foods(text):\n    # Normalize text to avoid spacing issues\n    text = \" \".join(text.split())\n    # Run NER pipeline and extract FOOD entities\n    ner_results = ner_pipeline(text)\n    # Filter valid food entities (exclude single letters or fragments)\n    foods = [result['word'] for result in ner_results if result['entity_group'] == 'FOOD' and len(result['word']) &gt; 1]\n    # Post-process to merge common multi-word foods\n    merged_foods = []\n    i = 0\n    while i &lt; len(foods):\n        # Check for known multi-word foods\n        if i &lt; len(foods) - 1 and foods[i].lower() + \" \" + foods[i+1].lower() in nutrient_map:\n            merged_foods.append(foods[i] + \" \" + foods[i+1])\n            i += 2\n        else:\n            merged_foods.append(foods[i])\n            i += 1\n    return merged_foods\n\n# Extended dictionary to map foods to key nutrients (case-insensitive)\nnutrient_map = {\n    \"blueberries\": \"Antioxidants, Vitamin C, Fiber\",\n    \"almonds\": \"Vitamin E, Magnesium, Healthy Fats\",\n    \"salmon\": \"Omega-3 Fatty Acids, Vitamin D, Protein\",\n    \"spinach\": \"Iron, Vitamin K, Folate\",\n    \"olive oil\": \"Monounsaturated Fats, Vitamin E, Antioxidants\",\n    \"broccoli\": \"Vitamin C, Vitamin K, Fiber\",\n    \"quinoa\": \"Protein, Magnesium, Fiber\",\n    \"greek yogurt\": \"Protein, Probiotics, Calcium\",\n    \"oatmeal\": \"Fiber, Iron, Magnesium\",\n    \"chicken\": \"Protein, Vitamin B6, Niacin\",\n    \"tomatoes\": \"Vitamin C, Lycopene, Potassium\",\n    \"cucumber\": \"Hydration, Vitamin K, Antioxidants\",\n    \"honey\": \"Antioxidants, Natural Sugars\",\n    \"cream\": \"Calcium, Vitamin A, Saturated Fats\",\n    \"red wine\": \"Resveratrol, Antioxidants\",\n    \"almond milk\": \"Calcium, Vitamin E, Low Calories\",\n    \"coffee\": \"Caffeine, Antioxidants\",\n    \"sparkling water\": \"Hydration, No Calories\",\n    \"chicken salad\": \"Protein, Vitamins A and C\",\n    \"olive oil dressing\": \"Monounsaturated Fats, Vitamin E\",\n}\n\n# Summarize the diary\nsummary = summarize_text(diary_entry)\n\n# Extract food entities\nfoods = extract_foods(diary_entry)\n\n# Map foods to nutrients (case-insensitive)\nnutrients = {food: nutrient_map.get(food.lower(), \"Unknown nutrients\") for food in foods}\n\n# Pre-compute food/nutrient list with wrapped lines\nwrapper = textwrap.TextWrapper(width=80, subsequent_indent=\"  \")  # Wrap at 80 characters\nfood_nutrient_lines = [wrapper.fill(f\"- {food}: {nutrients[food]}\") for food in sorted(nutrients)]\nfood_nutrient_text = \"\\n\".join(food_nutrient_lines)\n\n# Format the output for readability\ncurrent_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\noriginal_length = len(diary_entry.strip())\nsummary_length = len(summary.strip())\n\n# Wrap summary to avoid long lines\nwrapped_summary = wrapper.fill(f\"- {summary}\")\n\noutput = f\"\"\"\n{'=' * 40}\nFood Diary Analysis\n{'=' * 40}\nSummarization Model: facebook/bart-large-cnn\nNER Model: Dizex/InstaFoodRoBERTa-NER\nTimestamp: {current_time}\nOriginal Length: {original_length} characters\nSummary Length: {summary_length} characters\nFood Count: {len(foods)} items\n\nSummary:\n{wrapped_summary}\n\nExtracted Foods and Nutrients:\n{food_nutrient_text}\n\n🥣\n{'=' * 40}\n\"\"\"\n\n# Print formatted output\nprint(output)",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>🌟 Using LLM APIs for Nutrition & Food Science: A Tasty AI Adventure! 🥑</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.7_llm.html#literature-review-with-hugging-face-api",
    "href": "notebooks/10_mini_projects/10.7_llm.html#literature-review-with-hugging-face-api",
    "title": "🌟 Using LLM APIs for Nutrition & Food Science: A Tasty AI Adventure! 🥑",
    "section": "4. Literature Review with Hugging Face API 📚",
    "text": "4. Literature Review with Hugging Face API 📚\nLLM APIs can summarize nutrition research papers for literature reviews. Let’s use the same Hugging Face API to summarize a study abstract.\n\n4.1 Summarizing a Nutrition Study\nWe’ll summarize a sample abstract to extract key findings.\n\n# Sample nutrition study abstract\nabstract = \"\"\"\nThe Mediterranean diet, characterized by high intake of fruits, vegetables, whole grains, and olive oil, has been associated with reduced cardiovascular risk and improved cognitive function. This study examined the impact of adherence to the Mediterranean diet on heart disease outcomes in 500 participants over five years. Results showed a 30% reduction in cardiovascular events among high-adherence groups compared to low-adherence groups. Challenges include cultural barriers and cost of fresh produce.\n\"\"\"\n\n# Summarize the abstract\nlit_summary = summarize_text(abstract)\nprint(f'Literature Summary: {lit_summary} 📝')\n\n\nExplanation: - summarize_text: Reuses the Hugging Face API to condense the abstract into key points. - Output: Highlights main findings (e.g., cardiovascular benefits) for quick review.\nExercise 4: Change min_length to 50 in the summarize_text call for the abstract. Does it include more details? Why?\n\n\n💡 Solution\n\nUpdate the call to:\nlit_summary = summarize_text(abstract, max_length=100, min_length=50)\nA higher min_length ensures a longer summary, capturing more details like study size or challenges.\n\nLearn More: For advanced literature reviews, try OpenAI’s API for deeper contextual analysis (paid).",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>🌟 Using LLM APIs for Nutrition & Food Science: A Tasty AI Adventure! 🥑</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.7_llm.html#supply-chain-analysis-with-a-mock-llm-api",
    "href": "notebooks/10_mini_projects/10.7_llm.html#supply-chain-analysis-with-a-mock-llm-api",
    "title": "🌟 Using LLM APIs for Nutrition & Food Science: A Tasty AI Adventure! 🥑",
    "section": "5. Supply Chain Analysis with a Mock LLM API 🚚",
    "text": "5. Supply Chain Analysis with a Mock LLM API 🚚\nLLM APIs can analyze supply chain logs (e.g., delivery reports) to detect issues or optimize logistics. Since real APIs like OpenAI or xAI are paid, we’ll use a mock API to simulate extracting insights from a supply chain log.\n\n5.1 Mock Supply Chain Analysis\nWe’ll simulate an LLM extracting key issues from a supply chain log.\n\n# Mock function to simulate an LLM API for supply chain analysis\ndef mock_supply_chain_analysis(log_text):\n    # Simulate LLM extracting issues (in reality, use Grok or ChatGPT API)\n    import random\n    issues = ['Delay in delivery', 'Contamination risk', 'Inventory shortage', 'Transport cost overrun']\n    confidence = random.uniform(0.7, 0.95)\n    detected_issue = random.choice(issues)\n    return {'issue': detected_issue, 'confidence': confidence}\n\n# Sample supply chain log\nsupply_log = \"\"\"\nDelivery of fresh produce delayed by 2 days due to truck breakdown. Spinach showed signs of wilting upon arrival. Inventory levels for tomatoes are critically low. Fuel costs for transport increased by 15% this month.\n\"\"\"\n\n# Analyze the log\nresult = mock_supply_chain_analysis(supply_log)\nprint(f'Supply Chain Issue: {result[\"issue\"]} (Confidence: {result[\"confidence\"]:.2f}) 🚛')\n\n# Save result to file\nwith open('supply_chain_result.json', 'w') as f:\n    json.dump(result, f)\n\nExplanation: - mock_supply_chain_analysis: Simulates an LLM identifying issues (e.g., delays, contamination) from a log. - Real Alternative: Grok’s API with X integration could provide real-time supply chain insights.\nExercise 5: Add a new issue (e.g., ‘Supplier miscommunication’) to the issues list in mock_supply_chain_analysis. Test it and check the output.\n\n\n💡 Solution\n\nUpdate the issues list:\nissues = ['Delay in delivery', 'Contamination risk', 'Inventory shortage', 'Transport cost overrun', 'Supplier miscommunication']\nThe function now includes the new issue in its random selection.\n\nLearn More: For real supply chain analysis, explore xAI’s API for Grok’s real-time capabilities (paid).",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>🌟 Using LLM APIs for Nutrition & Food Science: A Tasty AI Adventure! 🥑</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.7_llm.html#sensory-analysis-with-hugging-face-api",
    "href": "notebooks/10_mini_projects/10.7_llm.html#sensory-analysis-with-hugging-face-api",
    "title": "🌟 Using LLM APIs for Nutrition & Food Science: A Tasty AI Adventure! 🥑",
    "section": "6. Sensory Analysis with Hugging Face API 🍫",
    "text": "6. Sensory Analysis with Hugging Face API 🍫\nSensory analysis involves interpreting taste, texture, or aroma descriptions (e.g., “creamy, nutty chocolate”). LLMs can classify or summarize sensory data. Let’s use Hugging Face to summarize a sensory description.\n\n6.1 Summarizing Sensory Data\nWe’ll summarize a sensory description of a food product.\n\n# Sample sensory description\nsensory_text = \"\"\"\nThe dark chocolate bar has a rich, velvety texture with a deep cocoa flavor. It offers subtle hints of roasted nuts and a slight fruity tang. The finish is smooth with a mild bitterness that lingers pleasantly.\n\"\"\"\n\n# Summarize the sensory description\nsensory_summary = summarize_text(sensory_text)\nprint(f'Sensory Summary: {sensory_summary} 🍫')\n\n# Save summary to file\nwith open('sensory_summary.txt', 'w') as f:\n    f.write(sensory_summary)\n\nExplanation: - summarize_text: Uses Hugging Face to condense sensory text into key descriptors (e.g., “rich, nutty”). - Application: Useful for product development or consumer feedback analysis.\nExercise 6: Try a different sensory description (e.g., for a cheese or beverage). Does the summary capture the main flavors? Why or why not?\n\n\n💡 Solution\n\nCreate a new sensory_text (e.g., “The cheddar is sharp, creamy, with a nutty aftertaste”). The summary should capture key descriptors if the input is clear, but vague inputs may lead to less precise summaries.\n\nLearn More: For advanced sensory analysis, try ChatGPT’s API for sentiment or emotion detection (paid).",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>🌟 Using LLM APIs for Nutrition & Food Science: A Tasty AI Adventure! 🥑</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.7_llm.html#combining-applications-a-nutrition-food-science-pipeline",
    "href": "notebooks/10_mini_projects/10.7_llm.html#combining-applications-a-nutrition-food-science-pipeline",
    "title": "🌟 Using LLM APIs for Nutrition & Food Science: A Tasty AI Adventure! 🥑",
    "section": "7. Combining Applications: A Nutrition & Food Science Pipeline 🛠️",
    "text": "7. Combining Applications: A Nutrition & Food Science Pipeline 🛠️\nLet’s combine our LLM API calls into a pipeline to process a food diary, summarize a study, analyze a supply chain, and evaluate sensory data.\n\n7.1 Building the Pipeline\nWe’ll create a function to run all tasks and save results.\n\n# Pipeline function\ndef nutrition_food_science_pipeline(diary, abstract, supply_log, sensory_text):\n    results = {}\n    \n    # Step 1: Summarize food diary\n    results['diary_summary'] = summarize_text(diary)\n    \n    # Step 2: Summarize literature\n    results['literature_summary'] = summarize_text(abstract)\n    \n    # Step 3: Analyze supply chain\n    results['supply_chain_issue'] = mock_supply_chain_analysis(supply_log)\n    \n    # Step 4: Summarize sensory data\n    results['sensory_summary'] = summarize_text(sensory_text)\n    \n    return results\n\n# Run pipeline\npipeline_results = nutrition_food_science_pipeline(diary_entry, abstract, supply_log, sensory_text)\n\n# Print results\nprint('Pipeline Results:')\nprint(f'Food Diary Summary: {pipeline_results[\"diary_summary\"]} 🥣')\nprint(f'Literature Summary: {pipeline_results[\"literature_summary\"]} 📝')\nprint(f'Supply Chain Issue: {pipeline_results[\"supply_chain_issue\"]} 🚛')\nprint(f'Sensory Summary: {pipeline_results[\"sensory_summary\"]} 🍫')\n\n# Save pipeline results\nwith open('pipeline_results.json', 'w') as f:\n    json.dump(pipeline_results, f)\n\nExplanation: - nutrition_food_science_pipeline: Combines diary parsing, literature summarization, supply chain analysis, and sensory summarization. - results: A dictionary storing outputs from each step.\nExercise 7: Add a step to the pipeline to count the number of meals in the diary_summary (hint: split by punctuation). Print the count in the results.\n\n\n💡 Solution\n\nAdd to the pipeline:\nresults['meal_count'] = len(results['diary_summary'].split('.')) - 1  # Subtract 1 for trailing period\nThen print:\nprint(f'Meal Count: {pipeline_results[\"meal_count\"]}')\nThis counts sentences in the summary, approximating meal mentions.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>🌟 Using LLM APIs for Nutrition & Food Science: A Tasty AI Adventure! 🥑</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.7_llm.html#summary-your-llm-api-toolkit-for-nutrition-food-science",
    "href": "notebooks/10_mini_projects/10.7_llm.html#summary-your-llm-api-toolkit-for-nutrition-food-science",
    "title": "🌟 Using LLM APIs for Nutrition & Food Science: A Tasty AI Adventure! 🥑",
    "section": "8. Summary: Your LLM API Toolkit for Nutrition & Food Science 🧰",
    "text": "8. Summary: Your LLM API Toolkit for Nutrition & Food Science 🧰\nHere’s what you’ve mastered:\n\nFood Diary Parsing 🥣: Summarize dietary logs with Hugging Face.\nLiterature Reviews 📚: Condense research papers for quick insights.\nSupply Chain Analysis 🚚: Detect issues in logistics (mock API).\nSensory Analysis 🍫: Summarize taste and texture descriptions.\nPipelines 🛠️: Combine LLM tasks for efficient workflows.\n\nFinal Exercise: Find a real nutrition dataset or paper (e.g., from USDA FoodData Central) and apply one of these LLM API approaches. Share your findings in a short paragraph!\nWhat’s Next? Experiment with paid APIs like Grok or ChatGPT for advanced tasks, or explore open-source models like DeepSeek for cost-free options. Keep tasting the future of AI in food science! 😄",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>🌟 Using LLM APIs for Nutrition & Food Science: A Tasty AI Adventure! 🥑</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.8_Bayesian_vs_Frequentist.html",
    "href": "notebooks/10_mini_projects/10.8_Bayesian_vs_Frequentist.html",
    "title": "Frequentist vs. Bayesian Methods in Nutrition and Food Science 🥗📊",
    "section": "",
    "text": "Step 1: Understanding Frequentist Methods 📏\nWelcome to this Jupyter notebook on statistical methods in nutrition and food science! Whether you’re analysing dietary patterns, conducting clinical trials on supplements, or exploring metabolomics data, choosing the right statistical approach is crucial. Two main paradigms dominate: Frequentist and Bayesian methods. Each has unique strengths and limitations, especially in the context of nutrition research, where data can be noisy, sample sizes vary, and prior knowledge (e.g., from past studies) is often valuable.\nIn this notebook, we’ll: - Define Frequentist and Bayesian methods and their core principles 🧩 - Compare their strengths and limitations in nutrition and food science contexts - Apply both methods to a clinical trial example, comparing their outputs 📈\nLet’s dive in and explore how these methods can help us uncover insights in nutrition and food science!\nFrequentist statistics is the traditional approach used in many scientific studies, including nutrition and food science. It relies on the idea of long-run frequencies—how often an event would occur if an experiment were repeated infinitely under identical conditions.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Frequentist vs. Bayesian Methods in Nutrition and Food Science 🥗📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.8_Bayesian_vs_Frequentist.html#step-1-understanding-frequentist-methods",
    "href": "notebooks/10_mini_projects/10.8_Bayesian_vs_Frequentist.html#step-1-understanding-frequentist-methods",
    "title": "Frequentist vs. Bayesian Methods in Nutrition and Food Science 🥗📊",
    "section": "",
    "text": "Key Concepts\n\nP-values: Measure the probability of observing data (or more extreme) under the null hypothesis. Common in clinical trials (e.g., testing if a dietary intervention reduces cholesterol).\nConfidence Intervals: Provide a range of plausible values for a parameter (e.g., mean weight loss) with a specified confidence level (e.g., 95%).\nHypothesis Testing: Tests null vs. alternative hypotheses (e.g., “Does a low-carb diet affect blood sugar levels?”).\n\nFrequentist methods are widely used in nutrition research, such as t-tests for comparing group means (e.g., fibre intake in two diets) or ANOVA for multi-group comparisons (e.g., nutrient levels across diets).",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Frequentist vs. Bayesian Methods in Nutrition and Food Science 🥗📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.8_Bayesian_vs_Frequentist.html#step-2-understanding-bayesian-methods",
    "href": "notebooks/10_mini_projects/10.8_Bayesian_vs_Frequentist.html#step-2-understanding-bayesian-methods",
    "title": "Frequentist vs. Bayesian Methods in Nutrition and Food Science 🥗📊",
    "section": "Step 2: Understanding Bayesian Methods 🌐",
    "text": "Step 2: Understanding Bayesian Methods 🌐\nBayesian statistics offers a different perspective, treating probabilities as degrees of belief that can be updated with new data. It’s particularly powerful in nutrition and food science, where prior knowledge (e.g., from previous studies) can be incorporated.\n\nKey Concepts\n\nPrior Distribution: Encodes existing knowledge or assumptions (e.g., expected effect size of a vitamin supplement based on past trials).\nLikelihood: Describes how likely the observed data is under different parameter values.\nPosterior Distribution: Combines the prior and likelihood to update beliefs after seeing data, giving a full distribution of parameter estimates.\nCredible Intervals: Provide a range where the parameter lies with a specified probability (e.g., 95%), directly interpretable as probability statements.\n\nBayesian methods are gaining traction in nutrition research, especially for clinical trials with small sample sizes or when integrating prior studies (e.g., modelling the effect of omega-3 on heart health).",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Frequentist vs. Bayesian Methods in Nutrition and Food Science 🥗📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.8_Bayesian_vs_Frequentist.html#step-3-comparing-strengths-and-limitations",
    "href": "notebooks/10_mini_projects/10.8_Bayesian_vs_Frequentist.html#step-3-comparing-strengths-and-limitations",
    "title": "Frequentist vs. Bayesian Methods in Nutrition and Food Science 🥗📊",
    "section": "Step 3: Comparing Strengths and Limitations ⚖️",
    "text": "Step 3: Comparing Strengths and Limitations ⚖️\nLet’s compare Frequentist and Bayesian methods, focusing on their application in nutrition and food science, including clinical trials, dietary studies, and metabolomics.\n\n\n\n\n\n\n\n\nAspect\nFrequentist Methods\nBayesian Methods\n\n\n\n\nInterpretability\nP-values and confidence intervals can be hard to interpret (e.g., a 95% CI does not mean a 95% chance the true value lies within it).\nPosterior distributions and credible intervals are intuitive (e.g., a 95% credible interval directly means a 95% probability).\n\n\nPrior Knowledge\nDoes not incorporate prior knowledge; relies solely on current data.\nIncorporates prior knowledge via priors, useful in nutrition (e.g., using past studies on calcium intake).\n\n\nSmall Sample Sizes\nLess reliable with small samples, common in clinical trials (e.g., pilot studies on dietary interventions).\nHandles small samples better by leveraging priors, improving estimates (e.g., effect of a new diet on blood pressure).\n\n\nFlexibility\nLess flexible for complex models or hierarchical data (e.g., nested dietary studies).\nHighly flexible for complex models, such as hierarchical models in nutrition (e.g., varying effects across populations).\n\n\nComputational Cost\nGenerally faster, using analytical solutions (e.g., t-tests in dietary comparisons).\nComputationally intensive, requiring MCMC sampling (e.g., Bayesian models for clinical trial outcomes).\n\n\nUncertainty\nProvides point estimates and intervals but no full distribution (e.g., mean nutrient intake).\nProvides full posterior distributions, capturing uncertainty comprehensively (e.g., distribution of supplement effects).\n\n\nRisk of Misuse\nP-value misuse (e.g., p-hacking in nutrition studies) can lead to false positives.\nRequires careful prior specification; poor priors can bias results (e.g., overly optimistic priors in clinical trials).\n\n\n\n\nNarrative Summary\n\nFrequentist Strengths: Ideal for straightforward hypothesis testing in large-scale clinical trials (e.g., testing if a supplement reduces cholesterol). They’re computationally efficient and widely accepted in scientific publishing.\nFrequentist Limitations: Struggle with small sample sizes or complex models, and don’t incorporate prior knowledge, which is often available in nutrition research (e.g., prior studies on dietary fat intake).\nBayesian Strengths: Excellent for integrating prior knowledge and handling small or complex datasets, common in nutrition (e.g., pilot trials, metabolomics). They provide intuitive uncertainty quantification via posteriors.\nBayesian Limitations: Computationally demanding and sensitive to prior choices, which can be subjective (e.g., choosing priors for a dietary intervention’s effect).\n\nIn nutrition and food science, the choice depends on your study’s goals. Frequentist methods suit large, well-controlled trials, while Bayesian methods shine in exploratory studies or when prior data is available.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Frequentist vs. Bayesian Methods in Nutrition and Food Science 🥗📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.8_Bayesian_vs_Frequentist.html#step-4-practical-example-clinical-trial-on-a-nutritional-supplement",
    "href": "notebooks/10_mini_projects/10.8_Bayesian_vs_Frequentist.html#step-4-practical-example-clinical-trial-on-a-nutritional-supplement",
    "title": "Frequentist vs. Bayesian Methods in Nutrition and Food Science 🥗📊",
    "section": "Step 4: Practical Example: Clinical Trial on a Nutritional Supplement 🧪",
    "text": "Step 4: Practical Example: Clinical Trial on a Nutritional Supplement 🧪\nLet’s apply both methods to a simulated clinical trial in nutrition science. Suppose we’re testing a new supplement claimed to reduce fasting blood glucose levels in 50 participants over 8 weeks. We have:\n\nControl Group: 25 participants receiving a placebo.\nTreatment Group: 25 participants receiving the supplement.\nOutcome: Change in fasting blood glucose (mmol/L) after 8 weeks.\nPrior Knowledge: Past studies suggest the supplement reduces glucose by ~0.5 mmol/L on average, with a standard deviation of 0.2.\n\nWe’ll: 1. Use a Frequentist t-test to compare the groups. 2. Use a Bayesian model to estimate the treatment effect, incorporating prior knowledge. 3. Compare the results to highlight differences in interpretation.\n\nSimulate the Data\nFirst, let’s simulate the data and visualize it.\n\n# Import libraries for simulation and analysis\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import ttest_ind\nimport pymc as pm\nimport arviz as az\n\n# Set random seed for reproducibility\nnp.random.seed(11088)\n\n# Simulate clinical trial data\nn_per_group = 25  # 25 participants per group\n# Control group: Mean change = 0, SD = 0.3\ncontrol = np.random.normal(loc=0, scale=0.3, size=n_per_group)\n# Treatment group: Mean change = -0.4 (reduction), SD = 0.3\ntreatment = np.random.normal(loc=-0.4, scale=0.3, size=n_per_group)\n\n# Combine into a DataFrame\ndata = pd.DataFrame({\n    'Group': ['Control'] * n_per_group + ['Treatment'] * n_per_group,\n    'Glucose_Change': np.concatenate([control, treatment])\n})\n\n# Visualize the data\nplt.figure(figsize=(8, 6))\nsns.boxplot(x='Group', y='Glucose_Change', data=data)\nplt.title('Change in Fasting Blood Glucose by Group 📊')\nplt.ylabel('Glucose Change (mmol/L)')\nplt.xlabel('Group')\nplt.grid(True)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Frequentist vs. Bayesian Methods in Nutrition and Food Science 🥗📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.8_Bayesian_vs_Frequentist.html#step-5-frequentist-analysis-t-test",
    "href": "notebooks/10_mini_projects/10.8_Bayesian_vs_Frequentist.html#step-5-frequentist-analysis-t-test",
    "title": "Frequentist vs. Bayesian Methods in Nutrition and Food Science 🥗📊",
    "section": "Step 5: Frequentist Analysis: T-Test 📉",
    "text": "Step 5: Frequentist Analysis: T-Test 📉\nLet’s perform a two-sample t-test to compare the mean glucose change between the control and treatment groups. The null hypothesis (H₀) is that there’s no difference between the groups, while the alternative (H₁) is that the supplement reduces glucose levels.\nWe’ll calculate the p-value and a 95% confidence interval for the difference in means.\n\n# Perform two-sample t-test\nt_stat, p_value = ttest_ind(control, treatment, equal_var=True)\n\n# Calculate means and standard error for confidence interval\nmean_diff = np.mean(treatment) - np.mean(control)\nse_diff = np.sqrt((np.var(control, ddof=1) / n_per_group) + (np.var(treatment, ddof=1) / n_per_group))\nci_95 = 1.96 * se_diff  # 95% CI: 1.96 * SE\nci_lower = mean_diff - ci_95\nci_upper = mean_diff + ci_95\n\n# Print results\nprint(f\"Frequentist T-Test Results:\")\nprint(f\"T-statistic: {t_stat:.2f}\")\nprint(f\"P-value: {p_value:.4f}\")\nprint(f\"Mean Difference (Treatment - Control): {mean_diff:.2f} mmol/L\")\nprint(f\"95% Confidence Interval: [{ci_lower:.2f}, {ci_upper:.2f}] mmol/L\")",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Frequentist vs. Bayesian Methods in Nutrition and Food Science 🥗📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.8_Bayesian_vs_Frequentist.html#step-6-bayesian-analysis-modelling-the-treatment-effect",
    "href": "notebooks/10_mini_projects/10.8_Bayesian_vs_Frequentist.html#step-6-bayesian-analysis-modelling-the-treatment-effect",
    "title": "Frequentist vs. Bayesian Methods in Nutrition and Food Science 🥗📊",
    "section": "Step 6: Bayesian Analysis: Modelling the Treatment Effect 🌐",
    "text": "Step 6: Bayesian Analysis: Modelling the Treatment Effect 🌐\nNow, let’s use a Bayesian model to estimate the treatment effect, incorporating prior knowledge from past studies (mean reduction of 0.5 mmol/L, SD 0.2). We’ll model the glucose change in each group as normal distributions with unknown means, using PyMC to sample from the posterior distribution of the treatment effect.\nWe’ll compare the posterior distribution of the difference in means to the Frequentist results.\n\n# Define Bayesian model\nwith pm.Model() as model:\n    # Priors for group means based on prior knowledge\n    mu_control = pm.Normal('mu_control', mu=0, sigma=0.5)  # Control group mean\n    mu_treatment = pm.Normal('mu_treatment', mu=-0.5, sigma=0.2)  # Treatment group mean, informed by prior\n    # Common standard deviation\n    sigma = pm.HalfNormal('sigma', sigma=0.5)\n    # Likelihoods\n    control_obs = pm.Normal('control_obs', mu=mu_control, sigma=sigma, observed=control)\n    treatment_obs = pm.Normal('treatment_obs', mu=mu_treatment, sigma=sigma, observed=treatment)\n    # Derived quantity: difference in means\n    diff_means = pm.Deterministic('diff_means', mu_treatment - mu_control)\n    # Sample from posterior\n    trace = pm.sample(1000, tune=1000, return_inferencedata=True)\n\n# Plot posterior of the difference in means\naz.plot_posterior(trace, var_names=['diff_means'])\nplt.title('Posterior of Treatment Effect (Difference in Means) 📈')\nplt.xlabel('Difference in Means (mmol/L)')\nplt.tight_layout()\nplt.show()\n\n# Summary of posterior\nsummary = az.summary(trace, var_names=['diff_means'])\nprint(\"Bayesian Posterior Summary:\")\nprint(summary)",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Frequentist vs. Bayesian Methods in Nutrition and Food Science 🥗📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.8_Bayesian_vs_Frequentist.html#step-7-comparing-the-results",
    "href": "notebooks/10_mini_projects/10.8_Bayesian_vs_Frequentist.html#step-7-comparing-the-results",
    "title": "Frequentist vs. Bayesian Methods in Nutrition and Food Science 🥗📊",
    "section": "Step 7: Comparing the Results 🔍",
    "text": "Step 7: Comparing the Results 🔍\n\nFrequentist Results\nThe t-test gave us a p-value (e.g., 0.002), suggesting a statistically significant difference if p &lt; 0.05. The 95% confidence interval (e.g., [-0.60, -0.20]) indicates the plausible range for the true difference in means, but it’s not a probability statement—95% of such intervals would contain the true value if the experiment were repeated.\n\n\nBayesian Results\nThe posterior distribution of the difference in means directly shows the probability distribution of the treatment effect. The 94% highest density interval (HDI, similar to a credible interval) might be [-0.58, -0.22], meaning there’s a 94% probability the true difference lies in this range. The posterior mean (e.g., -0.40) aligns with the Frequentist point estimate but provides a full distribution of uncertainty.\n\n\nKey Differences\n\nInterpretation: The Bayesian HDI is directly interpretable as a probability, while the Frequentist CI is not. This makes Bayesian results more intuitive for nutrition researchers (e.g., assessing supplement efficacy).\nPrior Knowledge: The Bayesian model used prior knowledge (mean reduction of 0.5), which can stabilise estimates in small trials, unlike the Frequentist approach.\nUncertainty: Bayesian methods provide a full posterior distribution, offering richer insights (e.g., probability of a clinically meaningful reduction &gt; 0.3 mmol/L).\n\nIn nutrition and food science, Bayesian methods can be particularly useful when sample sizes are small (e.g., pilot trials) or when integrating prior studies, while Frequentist methods are efficient for large, well-controlled trials.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Frequentist vs. Bayesian Methods in Nutrition and Food Science 🥗📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.8_Bayesian_vs_Frequentist.html#step-8-learning-points-and-next-steps",
    "href": "notebooks/10_mini_projects/10.8_Bayesian_vs_Frequentist.html#step-8-learning-points-and-next-steps",
    "title": "Frequentist vs. Bayesian Methods in Nutrition and Food Science 🥗📊",
    "section": "Step 8: Learning Points and Next Steps 🎓",
    "text": "Step 8: Learning Points and Next Steps 🎓\n\nLearning Points\n\nFrequentist Methods: Efficient and widely accepted, but they don’t incorporate prior knowledge and can be less reliable with small samples, common in nutrition pilot studies.\nBayesian Methods: Intuitive and flexible, especially for integrating prior studies or handling complex data in nutrition (e.g., hierarchical models for dietary effects across populations).\nPractical Application: In our clinical trial example, Bayesian methods provided a probabilistic interpretation of the supplement’s effect, complementing the Frequentist p-value and CI.\nChoosing an Approach: Use Frequentist methods for large trials with clear hypotheses (e.g., testing a new diet’s effect on weight loss). Use Bayesian methods when prior knowledge is available or uncertainty quantification is key (e.g., estimating nutrient effects in a small cohort).\n\n\n\nNext Steps\n\nExplore More Examples: Apply these methods to other nutrition problems, like dietary intake analysis or metabolomics (e.g., PCA with Bayesian priors).\nCross-Validation: In supervised tasks (e.g., RandomForestClassifier), compare Frequentist and Bayesian performance metrics.\nHierarchical Models: Use Bayesian methods for hierarchical data in nutrition, such as varying effects of a diet across different age groups.\n\nKeep exploring statistical methods to unlock deeper insights in nutrition and food science! 🥕📉\n\n\n\nSetup Requirements\n\nInstall Libraries:\nsource ~/Documents/data-analysis-toolkit-FNS/venv/bin/activate\npip install numpy pandas matplotlib seaborn scipy pymc arviz\nEnvironment: Python 3.9, compatible with Apple Silicon (MPS).\n\n\n\nExpected Output\n\nBox Plot: A box plot comparing glucose change between control and treatment groups.\nFrequentist Results (Console):\nFrequentist T-Test Results:\nT-statistic: 5.12\nP-value: 0.0002\nMean Difference (Treatment - Control): -0.40 mmol/L\n95% Confidence Interval: [-0.60, -0.20] mmol/L\nBayesian Posterior Plot: A plot of the posterior distribution of the difference in means, with a 94% HDI.\nBayesian Summary (Console):\nBayesian Posterior Summary:\n            mean   sd  hdi_3%  hdi_97%  ...\ndiff_means -0.40 0.09  -0.58   -0.22    ...",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Frequentist vs. Bayesian Methods in Nutrition and Food Science 🥗📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.9_fortran.html",
    "href": "notebooks/10_mini_projects/10.9_fortran.html",
    "title": "Exploring Fortran in Nutrition Science 🧪📊",
    "section": "",
    "text": "How to Download and Run This Notebook 📥\nWelcome to this fun mini-project where we take a step back in time and explore Fortran, a programming language that has played a significant role in scientific computing since the 1950s! Fortran (short for Formula Translation) was widely used in early nutrition and epidemiological research for simulations, statistical modeling, and numerical computations. In this notebook, we’ll use Fortran to calculate a simple nutritional index based on dietary intakes, bringing a bit of historical flair to our modern data science toolkit.\nWe’ll: - Learn about Fortran’s role in scientific computing history 🕰️ - Calculate a nutritional index using sugar and SFA intake data 🍬🥛 - Run Fortran code externally and display the results in this notebook 📈\nLet’s have some fun and see what Fortran can do in nutrition science!\nTo make this notebook easy to use, here’s how you can download and run it, along with the Fortran code:\nLet’s get started with the analysis!",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Exploring Fortran in Nutrition Science 🧪📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.9_fortran.html#how-to-download-and-run-this-notebook",
    "href": "notebooks/10_mini_projects/10.9_fortran.html#how-to-download-and-run-this-notebook",
    "title": "Exploring Fortran in Nutrition Science 🧪📊",
    "section": "",
    "text": "Download the Notebook:\n\nIf you’re viewing this on a platform like GitHub or a rendered HTML page, you can download the raw .ipynb file by clicking the ‘Raw’ button or ‘Download’ link (if available).\nTo save manually: Copy the JSON content, paste it into a text editor, and save the file with a .ipynb extension (e.g., 10.9_fortran_nutrition.ipynb).\n\nDownload the Fortran Code:\n\nIn Step 4, you’ll find the Fortran code (nutrition_index.f90) embedded in a Markdown cell. Copy the code block, paste it into a text editor, and save it as nutrition_index.f90 in the same directory as this notebook.\n\nRun the Notebook:\n\nOpen Jupyter Notebook in your environment:\nsource ~/Documents/data-analysis-toolkit-FNS/venv/bin/activate\njupyter notebook\nOpen 10.9_fortran_nutrition.ipynb and follow the steps to run the cells.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Exploring Fortran in Nutrition Science 🧪📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.9_fortran.html#step-1-a-brief-history-of-fortran-in-scientific-computing",
    "href": "notebooks/10_mini_projects/10.9_fortran.html#step-1-a-brief-history-of-fortran-in-scientific-computing",
    "title": "Exploring Fortran in Nutrition Science 🧪📊",
    "section": "Step 1: A Brief History of Fortran in Scientific Computing 🕰️",
    "text": "Step 1: A Brief History of Fortran in Scientific Computing 🕰️\nFortran was developed by IBM in the 1950s and became the go-to language for scientific and engineering applications due to its efficiency in numerical computations. In nutrition science, Fortran was used for early computational models, such as simulating metabolic processes, analysing dietary surveys, and performing statistical analyses like ANOVA or regression.\nWhile Python has largely replaced Fortran in modern data science, Fortran is still used in high-performance computing (e.g., climate modelling, computational chemistry) because of its speed and ability to handle complex mathematical operations. Today, we’ll use Fortran to calculate a simple nutritional index, giving us a taste of its capabilities!",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Exploring Fortran in Nutrition Science 🧪📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.9_fortran.html#step-2-define-the-problem---calculating-a-nutritional-index",
    "href": "notebooks/10_mini_projects/10.9_fortran.html#step-2-define-the-problem---calculating-a-nutritional-index",
    "title": "Exploring Fortran in Nutrition Science 🧪📊",
    "section": "Step 2: Define the Problem - Calculating a Nutritional Index 🍬🥛",
    "text": "Step 2: Define the Problem - Calculating a Nutritional Index 🍬🥛\nLet’s define a simple task: calculating a nutritional index based on sugar and SFA (saturated fatty acid) intake. We’ll use a small dataset of 5 participants with their daily sugar and SFA intakes (in grams). The index will be a weighted score:\n\nNutritional Index = (Sugar Intake × 0.4) + (SFA Intake × 0.6)\nLower scores indicate healthier diets (less sugar and SFA).\n\nWe’ll write a Fortran program to read the intake data, compute the index for each participant, and save the results to a file. Then, we’ll load the results into this notebook for visualisation.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Exploring Fortran in Nutrition Science 🧪📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.9_fortran.html#step-3-create-the-dataset",
    "href": "notebooks/10_mini_projects/10.9_fortran.html#step-3-create-the-dataset",
    "title": "Exploring Fortran in Nutrition Science 🧪📊",
    "section": "Step 3: Create the Dataset 📋",
    "text": "Step 3: Create the Dataset 📋\nFirst, let’s create a small dataset of sugar and SFA intakes for 5 participants and save it to a file (intakes.txt) that our Fortran program can read.\n\nimport pandas as pd\n\n# Create a small dataset\ndata = pd.DataFrame({\n    'Participant': ['P1', 'P2', 'P3', 'P4', 'P5'],\n    'Sugar_Intake': [40, 55, 30, 60, 25],  # grams/day\n    'SFA_Intake': [20, 35, 15, 40, 10]     # grams/day\n})\n\n# Save the data to a text file (space-separated)\ndata[['Sugar_Intake', 'SFA_Intake']].to_csv('intakes.txt', sep=' ', index=False, header=False)\n\n# Display the dataset\nprint(\"Dataset of Intakes:\")\nprint(data)\n\nDataset of Intakes:\n  Participant  Sugar_Intake  SFA_Intake\n0          P1            40          20\n1          P2            55          35\n2          P3            30          15\n3          P4            60          40\n4          P5            25          10",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Exploring Fortran in Nutrition Science 🧪📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.9_fortran.html#step-4-write-the-fortran-program",
    "href": "notebooks/10_mini_projects/10.9_fortran.html#step-4-write-the-fortran-program",
    "title": "Exploring Fortran in Nutrition Science 🧪📊",
    "section": "Step 4: Write the Fortran Program 🖥️",
    "text": "Step 4: Write the Fortran Program 🖥️\nBelow is the Fortran program (nutrition_index.f90) that reads the intake data, calculates the nutritional index for each participant, and saves the results to a file (index_scores.txt).\nCopy the code below into a file named nutrition_index.f90 in the same directory as this notebook:\nprogram nutrition_index\n    implicit none\n    integer, parameter :: n = 5  ! Number of participants\n    real :: sugar(n), sfa(n), index(n)\n    integer :: i\n\n    ! Open the input file\n    open(unit=10, file='intakes.txt', status='old')\n\n    ! Read sugar and SFA intakes\n    do i = 1, n\n        read(10, *) sugar(i), sfa(i)\n    end do\n    close(10)\n\n    ! Calculate the nutritional index\n    do i = 1, n\n        index(i) = (sugar(i) * 0.4) + (sfa(i) * 0.6)\n    end do\n\n    ! Save the results to a file\n    open(unit=20, file='index_scores.txt', status='replace')\n    do i = 1, n\n        write(20, *) index(i)\n    end do\n    close(20)\n\n    print *, 'Nutritional index scores saved to index_scores.txt'\nend program nutrition_index",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Exploring Fortran in Nutrition Science 🧪📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.9_fortran.html#step-5-compile-and-run-the-fortran-program",
    "href": "notebooks/10_mini_projects/10.9_fortran.html#step-5-compile-and-run-the-fortran-program",
    "title": "Exploring Fortran in Nutrition Science 🧪📊",
    "section": "Step 5: Compile and Run the Fortran Program ⚙️",
    "text": "Step 5: Compile and Run the Fortran Program ⚙️\nTo run the Fortran program, you’ll need a Fortran compiler like gfortran. If you don’t have gfortran installed, you can install it on macOS using Homebrew:\nbrew install gcc\nThen, compile and run the program:\ngfortran -o nutrition_index nutrition_index.f90\n./nutrition_index\nThis will read intakes.txt, calculate the nutritional index scores, and save them to index_scores.txt. Let’s run this command in the notebook using Python’s subprocess module.\n\nimport subprocess\n\n# Compile the Fortran program\ntry:\n    subprocess.run(['gfortran', '-o', 'nutrition_index', 'nutrition_index.f90'], check=True)\n    print(\"Fortran program compiled successfully.\")\nexcept subprocess.CalledProcessError:\n    print(\"Error: Failed to compile the Fortran program. Ensure gfortran is installed and nutrition_index.f90 exists.\")\n\n# Run the Fortran program\ntry:\n    subprocess.run(['./nutrition_index'], check=True)\n    print(\"Fortran program executed successfully.\")\nexcept subprocess.CalledProcessError:\n    print(\"Error: Failed to run the Fortran program. Ensure it compiled correctly.\")",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Exploring Fortran in Nutrition Science 🧪📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.9_fortran.html#step-6-load-and-visualise-the-results",
    "href": "notebooks/10_mini_projects/10.9_fortran.html#step-6-load-and-visualise-the-results",
    "title": "Exploring Fortran in Nutrition Science 🧪📊",
    "section": "Step 6: Load and Visualise the Results 📈",
    "text": "Step 6: Load and Visualise the Results 📈\nThe Fortran program saved the nutritional index scores to index_scores.txt. Let’s load the results into the notebook and visualise them alongside the original intake data.\n\n# Load the index scores\nindex_scores = pd.read_csv('index_scores.txt', header=None, names=['Nutritional_Index'])\n\n# Combine with the original data\ndata['Nutritional_Index'] = index_scores\n\n# Display the results\nprint(\"Results with Nutritional Index:\")\nprint(data)\n\n# Visualise the nutritional index\nplt.figure(figsize=(8, 6))\nsns.barplot(x='Nutritional_Index', y='Participant', data=data)\nplt.title('Nutritional Index Scores by Participant 📊')\nplt.xlabel('Nutritional Index (Lower = Healthier)')\nplt.ylabel('Participant')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Exploring Fortran in Nutrition Science 🧪📊</span>"
    ]
  },
  {
    "objectID": "notebooks/10_mini_projects/10.9_fortran.html#step-7-learning-points-and-next-steps",
    "href": "notebooks/10_mini_projects/10.9_fortran.html#step-7-learning-points-and-next-steps",
    "title": "Exploring Fortran in Nutrition Science 🧪📊",
    "section": "Step 7: Learning Points and Next Steps 🎓",
    "text": "Step 7: Learning Points and Next Steps 🎓\n\nLearning Points\n\nFortran’s Legacy: Fortran has a rich history in scientific computing, particularly in nutrition and epidemiology, where it was used for early computational models. It’s still valued for its performance in numerical tasks.\nNumerical Computation: We used Fortran to perform a simple calculation (nutritional index), showcasing its ability to handle numerical data efficiently.\nIntegration with Modern Tools: By running Fortran externally and loading the results into Python, we combined the strengths of both languages—Fortran’s computational speed and Python’s visualisation capabilities.\nFun Exploration: Exploring Fortran adds a unique and enjoyable element to our mini-projects, bridging the past and present of scientific computing.\n\n\n\nNext Steps\n\nMore Complex Fortran Models: Try using Fortran for more advanced nutrition science tasks, such as simulating dietary intake over time or solving differential equations for metabolic models.\nFortran in Jupyter: Explore the fortran-magic package to run Fortran code directly in Jupyter Notebook cells:\npip install fortran-magic\nThen load the extension in a notebook cell:\n%load_ext fortranmagic\nCompare with Python: Rewrite the nutritional index calculation in Python and compare the results and performance with Fortran.\nHistorical Context: Research historical nutrition studies that used Fortran to add more context to your project.\n\nKeep having fun exploring computational methods in nutrition science! 🥕📉\n\n\n\nSetup Requirements\n\nInstall Libraries:\nsource ~/Documents/data-analysis-toolkit-FNS/venv/bin/activate\npip install pandas==2.2.3 matplotlib==3.9.2 seaborn==0.13.2\nInstall a Fortran Compiler:\n\nOn macOS, install gfortran via Homebrew:\nbrew install gcc\nOn Linux, install gfortran:\nsudo apt-get install gfortran\nOn Windows, install gfortran via MinGW or WSL.\n\nEnvironment: Python 3.9, compatible with Apple Silicon (MPS).\nFortran File: Ensure nutrition_index.f90 is saved in the same directory as this notebook.\n\n\n\nExpected Output\n\nDataset Table: A small table showing sugar and SFA intakes for 5 participants.\nFortran Output: Nutritional index scores saved to index_scores.txt and displayed in the notebook.\nBar Plot: A visualisation of the nutritional index scores by participant.",
    "crumbs": [
      "10 Mini Projects",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Exploring Fortran in Nutrition Science 🧪📊</span>"
    ]
  }
]