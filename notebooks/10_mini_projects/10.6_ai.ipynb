{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "title: \"10.6_ai\"\n",
    "format:\n",
    "  html: default\n",
    "toc: false\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 10.6 • Artificial Intelligence: Search, Uncertainty, Learning, and Decisions\n",
    "\n",
    "“AI” is a toolbox:\n",
    "- **Symbolic** (reasoning with rules),\n",
    "- **Search** (A*, heuristics),\n",
    "- **Probabilistic** (Bayes nets, HMMs),\n",
    "- **Decision-making** (MDPs, RL),\n",
    "- **Learning** (from data — see 10.5).\n",
    "\n",
    "We’ll **implement**: BFS vs A*, admissible heuristics, value iteration for an MDP, and a tiny Bayesian network inference example."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np, heapq\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(11088)"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1) Problem-solving as search\n",
    "**State space** + **actions** + **goal test** + **costs** → path. We compare BFS and A* on a grid with obstacles.\n",
    "\n",
    "**Heuristic admissibility**: `h(n) ≤ true_cost(n→goal)` ⇒ A* finds an optimal path."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def bfs(grid, start, goal):\n",
    "    H,W = grid.shape; from collections import deque\n",
    "    Q=deque([start]); came={start:None}; vis={start}\n",
    "    while Q:\n",
    "        r,c=Q.popleft()\n",
    "        if (r,c)==goal: break\n",
    "        for dr,dc in [(1,0),(-1,0),(0,1),(0,-1)]:\n",
    "            nr,nc=r+dr,c+dc\n",
    "            if 0<=nr<H and 0<=nc<W and grid[nr,nc]==0 and (nr,nc) not in vis:\n",
    "                came[(nr,nc)]=(r,c); vis.add((nr,nc)); Q.append((nr,nc))\n",
    "    path=[]; cur=goal\n",
    "    while cur is not None: path.append(cur); cur=came.get(cur)\n",
    "    return path[::-1]\n",
    "\n",
    "def astar(grid, start, goal):\n",
    "    H,W = grid.shape\n",
    "    def h(a,b): return abs(a[0]-b[0])+abs(a[1]-b[1])  # Manhattan: admissible on 4-neighbour grid\n",
    "    openq=[(h(start,goal),0,start,None)]\n",
    "    came={}; g={start:0}; vis=set()\n",
    "    while openq:\n",
    "        f, cost, cur, par = heapq.heappop(openq)\n",
    "        if cur in vis: continue\n",
    "        vis.add(cur); came[cur]=par\n",
    "        if cur==goal: break\n",
    "        for d in [(1,0),(-1,0),(0,1),(0,-1)]:\n",
    "            nr,nc=cur[0]+d[0], cur[1]+d[1]\n",
    "            if 0<=nr<H and 0<=nc<W and grid[nr,nc]==0:\n",
    "                ng=cost+1\n",
    "                if ng < g.get((nr,nc), 1e9):\n",
    "                    g[(nr,nc)]=ng\n",
    "                    heapq.heappush(openq,(ng+h((nr,nc),goal), ng, (nr,nc), cur))\n",
    "    path=[]; cur=goal\n",
    "    while cur is not None: path.append(cur); cur=came.get(cur)\n",
    "    return path[::-1]\n",
    "\n",
    "grid = np.zeros((25,25), int)\n",
    "grid[8:18,12]=1; grid[13,12]=0  # wall with door\n",
    "start, goal = (0,0), (24,24)\n",
    "p_bfs = bfs(grid,start,goal)\n",
    "p_astar = astar(grid,start,goal)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(grid, cmap='gray_r')\n",
    "rb,cb=zip(*p_bfs); ra,ca=zip(*p_astar)\n",
    "plt.plot(cb,rb,'b--',label='BFS')\n",
    "plt.plot(ca,ra,'r-',label='A*')\n",
    "plt.legend(); plt.title('BFS vs A*'); plt.show()"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Observation**: BFS ignores distance to goal; A* uses `h` to expand promising nodes first → fewer expansions and often shorter runtime.\n",
    "\n",
    "**Exercise 1**: Replace Manhattan with a heuristic that overestimates by +3. Does A* remain optimal? Why not? (Because it violates admissibility.)"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2) Decisions under uncertainty: Markov Decision Processes (MDPs)\n",
    "An MDP is \\( (S,A,P,R,\\gamma) \\). We compute an **optimal policy** with **value iteration**.\n",
    "\n",
    "\\[ V_{k+1}(s) = \\max_a \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V_k(s')] \\]\n",
    "\n",
    "Environment: 3×3 grid, slipping 10% sideways. Reward +1 at goal, −0.04 per step."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "S=[(r,c) for r in range(3) for c in range(3)]\n",
    "A=[(1,0),(-1,0),(0,1),(0,-1)]\n",
    "goal=(2,2); gamma=0.95\n",
    "def step(s,a,slip=0.1):\n",
    "    r,c=s; dr,dc=a\n",
    "    main=(r+dr,c+dc); slip_alt=(r+dc,c+dr)  # rough side slip\n",
    "    nxt=[]\n",
    "    for i, (nr,nc) in enumerate([main, slip_alt]):\n",
    "        if 0<=nr<3 and 0<=nc<3:\n",
    "            p=1-slip if i==0 else slip\n",
    "            nxt.append(((nr,nc), p))\n",
    "        else:\n",
    "            p=1-slip if i==0 else slip\n",
    "            nxt.append(((r,c), p))\n",
    "    return nxt\n",
    "R=lambda s: 1.0 if s==goal else -0.04\n",
    "\n",
    "V={s:0.0 for s in S}\n",
    "for _ in range(200):\n",
    "    V = {s: (0 if s==goal else max(sum(p*(R(s2)+gamma*V[s2]) for s2,p in step(s,a)) for a in A)) for s in S}\n",
    "\n",
    "Pi={}\n",
    "for s in S:\n",
    "    if s==goal: Pi[s]=(0,0); continue\n",
    "    Pi[s]=max(A, key=lambda a: sum(p*(R(s2)+gamma*V[s2]) for s2,p in step(s,a)))\n",
    "np.array([[Pi[(r,c)] for c in range(3)] for r in range(3)])"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Exercise 2**: Increase slip to 0.3. Does the optimal policy become more cautious (e.g., hugging walls)? Discuss risk-sensitive modifications.\n",
    "\n",
    "### Policy iteration vs. value iteration\n",
    "- Policy iteration alternates **policy evaluation** and **policy improvement**; often fewer iterations.\n",
    "- Value iteration performs a Bellman backup to optimality; simpler to code."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3) Tiny Bayesian network: probabilistic reasoning\n",
    "Variables: `Smoking (S) → Cough (C)` and `Smoking (S) → CVD (D)`. Given observed `Cough=True`, infer `P(S)` and `P(D)`.\n",
    "This shows **belief propagation** in the simplest form."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "P_S = {1:0.3, 0:0.7}\n",
    "P_C_given_S = {1:{1:0.6, 0:0.4}, 0:{1:0.2, 0:0.8}}\n",
    "P_D_given_S = {1:{1:0.15,0:0.85}, 0:{1:0.05,0:0.95}}\n",
    "\n",
    "# Observe C=1, compute posterior P(S|C=1) ∝ P(C=1|S)P(S)\n",
    "unnorm = {s: P_C_given_S[s][1]*P_S[s] for s in [0,1]}\n",
    "Z = sum(unnorm.values())\n",
    "post_S = {s:unnorm[s]/Z for s in [0,1]}\n",
    "\n",
    "# Predictive for D given C=1: sum_s P(D|S=s)P(S=s|C=1)\n",
    "P_D1 = sum(P_D_given_S[s][1]*post_S[s] for s in [0,1])\n",
    "post_S, P_D1"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Why this matters**: many ‘AI’ tasks reduce to **structured uncertainty** (Bayes nets, HMMs). ML fits parameters; probabilistic AI lets you **reason** with them.\n",
    "\n",
    "## 4) Where ML (10.5) and RL meet\n",
    "- If transitions/rewards unknown, **RL** learns by interaction.\n",
    "- If states partially observed, **POMDPs** (belief over states) — not covered here, but conceptual upgrade.\n",
    "\n",
    "## Exercises\n",
    "1. Change the A* heuristic to Euclidean distance. Still admissible? (Yes.) Compare node expansions (instrument your code).\n",
    "2. Add a pit state with −1 reward; recompute policy. How does risk trade-off appear?\n",
    "3. Extend Bayes net: add `Activity (A)` that reduces `D` independently. Compute `P(D|C=1, A=1)`.\n",
    "\n",
    "## Takeaways\n",
    "- **Search** provides goal-directed behaviour without learning.\n",
    "- **Probabilistic inference** manages uncertainty coherently.\n",
    "- **Decision-making** optimises action with explicit trade-offs.\n",
    "- **Learning** plugs in when models aren’t given; evaluation remains king."
   ],
   "metadata": {
    "tags": []
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}